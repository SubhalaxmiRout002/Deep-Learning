{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Improving Your Digit Recognition Model From Assignment 1**\n",
        "\n",
        "For this problem, you will attempt to improve your Digit Recognition Model from Assignment1- problem2. More specifically, you will add the following components:\n",
        "\n",
        "Modify your create_nn method to use Mini-batch gradient descent (5pt).\n",
        "\n",
        "1.   Your create_nn method should accept an additional parameter (batch_size). Then instead of updating parameters per entire data, you will create random batches of (batch_size) samples from training data, run forward and backward passes, and update parameters per each batch. Then to get the train_loss for each epoch, you should keep track and average train_losses over batches.\n",
        "\n",
        "2.   Retrain the model using mini-batch gradient descent you implemented above and interpret the learning curves. Answer the following question:\n",
        "\n",
        "\n",
        "*   How do training and validation losses compare to the model without minibatch gradient descent?\n"
      ],
      "metadata": {
        "id": "B4QsibQC_La4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale, OneHotEncoder\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lZCKJdvm1jMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW0gIv3j_A-j"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Parameter Initialization\n",
        "def initialize_parameters(nx, nh1, nh2, ny):\n",
        "    tf.random.set_seed(1)  # Set random seed for reproducibility\n",
        "    W1 = tf.Variable(tf.random.normal(shape=(nh1, nx), stddev=0.01), name=\"W1\")\n",
        "    b1 = tf.Variable(tf.zeros(shape=(nh1, 1), name=\"b1\"))\n",
        "    W2 = tf.Variable(tf.random.normal(shape=(nh2, nh1), stddev=0.01), name=\"W2\")\n",
        "    b2 = tf.Variable(tf.zeros(shape=(nh2, 1), name=\"b2\"))\n",
        "    W3 = tf.Variable(tf.random.normal(shape=(ny, nh2), stddev=0.01), name=\"W3\")\n",
        "    b3 = tf.Variable(tf.zeros(shape=(ny, 1), name=\"b3\"))\n",
        "\n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
        "    return parameters\n",
        "\n",
        "#Forward Pass:\n",
        "\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "\n",
        "    Z1 = tf.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.matmul(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.matmul(parameters[\"W3\"], A2) + parameters[\"b3\"]\n",
        "\n",
        "    # Apply softmax activation for multi-class classification\n",
        "    Yhat = tf.nn.softmax(Z3, axis=0)\n",
        "\n",
        "    return Yhat\n",
        "\n",
        "#Loss calculations\n",
        "\n",
        "def compute_loss(Y, Yhat):\n",
        "    # Cross-entropy loss\n",
        "    loss = -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(Yhat + 1e-10), axis=0))\n",
        "    return loss\n",
        "\n",
        "#Backward Pass\n",
        "\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters.values())\n",
        "    return gradients\n",
        "\n",
        "#Gradient Descent To update the parameters\n",
        "\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "    for param, grad in zip(parameters.values(), gradients):\n",
        "        param.assign_sub(learning_rate * grad)\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Neural Network Model\n",
        "\n",
        "def create_nn_model_mb(train_X, train_Y, val_X, val_Y, num_iterations, learning_rate, nh1, nh2, batch_size):\n",
        "    nx, m = train_X.shape\n",
        "    ny = train_Y.shape[0]  # Number of output neurons for multi-class classification\n",
        "\n",
        "    parameters = initialize_parameters(nx, nh1, nh2, ny)\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "# Adjusted calculation for number of batches\n",
        "    num_batches = (m + batch_size - 1) // batch_size\n",
        "    #print(\"num_batches\", num_batches)\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        epoch_train_loss = 0  # Initialize epoch training loss\n",
        "        train_loss = []   # Initialize train_loss\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = min(start + batch_size, m)\n",
        "            X_batch = train_X[:, start:end]\n",
        "            Y_batch = train_Y[:, start:end]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                train_Yhat = forward_pass(parameters, tf.convert_to_tensor(X_batch, dtype=tf.float32))\n",
        "                train_loss = compute_loss(tf.convert_to_tensor(Y_batch, dtype=tf.float32), train_Yhat)\n",
        "\n",
        "            gradients = backward_pass(parameters, train_loss, tape)\n",
        "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "            epoch_train_loss = epoch_train_loss + train_loss.numpy()  # Accumulate loss for the current batch\n",
        "            #print(\"Batch ######\", batch)\n",
        "            #print(epoch_train_loss)\n",
        "\n",
        "        epoch_train_loss = epoch_train_loss/num_batches\n",
        "        train_losses.append(epoch_train_loss)  # Append epoch loss to train_losses\n",
        "        #print(\"epoch_train_loss : \", epoch_train_loss)\n",
        "        #epoch_train_loss = sum(train_loss) / len(train_loss)\n",
        "        #print(num_iterations)\n",
        "        #print(train_losses)\n",
        "\n",
        "        # Calculate validation loss after each epoch\n",
        "        val_Yhat = forward_pass(parameters, tf.convert_to_tensor(val_X, dtype=tf.float32))\n",
        "        val_loss = compute_loss(tf.convert_to_tensor(val_Y, dtype=tf.float32), val_Yhat)\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        print(\"epoch {}: train_loss:{} val_loss{}\".format(i, epoch_train_loss, val_loss.numpy()))\n",
        "\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return parameters, history"
      ],
      "metadata": {
        "id": "phkQL7eHBHIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"German_digits.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract features (X) and labels (Y)\n",
        "X = df.iloc[:, :-1].values / 255.0  # Scale pixel values to the range [0, 1]\n",
        "Y = df.iloc[:, -1].values\n",
        "\n",
        "# Split the dataset into train and test\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=12)\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 10  # Number of digits (0-9)\n",
        "train_Y = pd.get_dummies(train_Y).values\n",
        "test_Y = pd.get_dummies(test_Y).values\n",
        "\n",
        "# Transpose the datasets\n",
        "train_X = train_X.T\n",
        "train_Y = train_Y.T\n",
        "test_X = test_X.T\n",
        "test_Y = test_Y.T\n",
        "\n",
        "print(\"train_X\", train_X.shape)\n",
        "print(\"train_Y\", train_Y.shape)\n",
        "print(\"test_X\", test_X.shape)\n",
        "print(\"test_Y\", test_Y.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtwadW5qBciM",
        "outputId": "23645f19-cec9-4dae-c3e0-9a8c7b4111cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X (1600, 3540)\n",
            "train_Y (10, 3540)\n",
            "test_X (1600, 886)\n",
            "test_Y (10, 886)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the shape of train and test data sets. Below we will see the training loss and test loss for the mini batch gradient descent model. As mentioned in the instructions added batch size as a parameter."
      ],
      "metadata": {
        "id": "hj9LVGXkbVyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 100\n",
        "nh1 = 128\n",
        "nh2 = 64\n",
        "batch_size = 32\n",
        "\n",
        "# Train the model\n",
        "parameters_mb, history_mb = create_nn_model_mb(train_X, train_Y, test_X, test_Y, num_iterations, learning_rate, nh1, nh2, batch_size)\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.plot(history_mb['train_loss'], label='Training Loss')\n",
        "plt.plot(history_mb['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zc5fVLzP1eAh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15f0b99e-d316-4d79-b585-fdfdb698527b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: train_loss:2.3027105116629385 val_loss2.3025336265563965\n",
            "epoch 1: train_loss:2.3025650763296865 val_loss2.302422523498535\n",
            "epoch 2: train_loss:2.3024352455998325 val_loss2.302321434020996\n",
            "epoch 3: train_loss:2.3023142041386784 val_loss2.3022234439849854\n",
            "epoch 4: train_loss:2.3021967668791077 val_loss2.302125930786133\n",
            "epoch 5: train_loss:2.3020780086517334 val_loss2.3020222187042236\n",
            "epoch 6: train_loss:2.301952538189587 val_loss2.3019070625305176\n",
            "epoch 7: train_loss:2.3018144551698154 val_loss2.301774501800537\n",
            "epoch 8: train_loss:2.301656673620413 val_loss2.3016159534454346\n",
            "epoch 9: train_loss:2.301470539591334 val_loss2.301421880722046\n",
            "epoch 10: train_loss:2.3012440483849326 val_loss2.3011772632598877\n",
            "epoch 11: train_loss:2.300960641723495 val_loss2.300863027572632\n",
            "epoch 12: train_loss:2.300596439086639 val_loss2.3004491329193115\n",
            "epoch 13: train_loss:2.3001156239896208 val_loss2.2998886108398438\n",
            "epoch 14: train_loss:2.299460011559564 val_loss2.299102544784546\n",
            "epoch 15: train_loss:2.2985295519098505 val_loss2.2979536056518555\n",
            "epoch 16: train_loss:2.297148189029178 val_loss2.296194314956665\n",
            "epoch 17: train_loss:2.2949813808406794 val_loss2.293337821960449\n",
            "epoch 18: train_loss:2.291351249626091 val_loss2.288361072540283\n",
            "epoch 19: train_loss:2.284789310919272 val_loss2.2789950370788574\n",
            "epoch 20: train_loss:2.2720304072440207 val_loss2.2602856159210205\n",
            "epoch 21: train_loss:2.2467080644659094 val_loss2.2240753173828125\n",
            "epoch 22: train_loss:2.202879461082252 val_loss2.169325351715088\n",
            "epoch 23: train_loss:2.147677550444732 val_loss2.1111106872558594\n",
            "epoch 24: train_loss:2.090005249590487 val_loss2.0517184734344482\n",
            "epoch 25: train_loss:2.0203670703613006 val_loss1.9735718965530396\n",
            "epoch 26: train_loss:1.9173528847393688 val_loss1.8610228300094604\n",
            "epoch 27: train_loss:1.7861268992896553 val_loss1.7363253831863403\n",
            "epoch 28: train_loss:1.650875641419007 val_loss1.6067116260528564\n",
            "epoch 29: train_loss:1.5072543846594322 val_loss1.469599723815918\n",
            "epoch 30: train_loss:1.3683455377011686 val_loss1.354798436164856\n",
            "epoch 31: train_loss:1.260845757282532 val_loss1.2753878831863403\n",
            "epoch 32: train_loss:1.1861827357395276 val_loss1.2231134176254272\n",
            "epoch 33: train_loss:1.1338145185161281 val_loss1.1880817413330078\n",
            "epoch 34: train_loss:1.0947066744168599 val_loss1.162339448928833\n",
            "epoch 35: train_loss:1.0627014422201895 val_loss1.141040325164795\n",
            "epoch 36: train_loss:1.033895925358609 val_loss1.1210551261901855\n",
            "epoch 37: train_loss:1.0056752820272703 val_loss1.0998334884643555\n",
            "epoch 38: train_loss:0.9753262567090558 val_loss1.0748108625411987\n",
            "epoch 39: train_loss:0.9398393614872081 val_loss1.043124794960022\n",
            "epoch 40: train_loss:0.8969340576781882 val_loss1.0039854049682617\n",
            "epoch 41: train_loss:0.8482966621716818 val_loss0.9615124464035034\n",
            "epoch 42: train_loss:0.7999982753315488 val_loss0.9219174385070801\n",
            "epoch 43: train_loss:0.7573513700081421 val_loss0.888197124004364\n",
            "epoch 44: train_loss:0.7213063425308949 val_loss0.8598053455352783\n",
            "epoch 45: train_loss:0.6902914828545338 val_loss0.8354644179344177\n",
            "epoch 46: train_loss:0.6626979311307272 val_loss0.8134337663650513\n",
            "epoch 47: train_loss:0.6374029866209975 val_loss0.7936251163482666\n",
            "epoch 48: train_loss:0.6141316361792453 val_loss0.7755438089370728\n",
            "epoch 49: train_loss:0.5922984129136747 val_loss0.7587040066719055\n",
            "epoch 50: train_loss:0.5718569967660818 val_loss0.7427306175231934\n",
            "epoch 51: train_loss:0.5524933824131081 val_loss0.7272294163703918\n",
            "epoch 52: train_loss:0.5338815684254105 val_loss0.7126271724700928\n",
            "epoch 53: train_loss:0.5160762509247204 val_loss0.6988257169723511\n",
            "epoch 54: train_loss:0.49890458261644516 val_loss0.685470700263977\n",
            "epoch 55: train_loss:0.4823432437202952 val_loss0.6722893714904785\n",
            "epoch 56: train_loss:0.46632808151545824 val_loss0.6602189540863037\n",
            "epoch 57: train_loss:0.45081795939991065 val_loss0.6482046842575073\n",
            "epoch 58: train_loss:0.4357621507333206 val_loss0.6367297768592834\n",
            "epoch 59: train_loss:0.4211443320319459 val_loss0.6259951591491699\n",
            "epoch 60: train_loss:0.40705466297295717 val_loss0.6157118678092957\n",
            "epoch 61: train_loss:0.39344187805781494 val_loss0.6061150431632996\n",
            "epoch 62: train_loss:0.3803327883686031 val_loss0.596972644329071\n",
            "epoch 63: train_loss:0.36759567844706614 val_loss0.5883700847625732\n",
            "epoch 64: train_loss:0.3553030915759705 val_loss0.5801772475242615\n",
            "epoch 65: train_loss:0.343447435412321 val_loss0.572575032711029\n",
            "epoch 66: train_loss:0.33200878549266505 val_loss0.5655784010887146\n",
            "epoch 67: train_loss:0.3210393591104327 val_loss0.5589614510536194\n",
            "epoch 68: train_loss:0.3104119367293409 val_loss0.5524901747703552\n",
            "epoch 69: train_loss:0.30014550303285187 val_loss0.5466551184654236\n",
            "epoch 70: train_loss:0.29033643477134874 val_loss0.5409743189811707\n",
            "epoch 71: train_loss:0.2807319074332177 val_loss0.5360260605812073\n",
            "epoch 72: train_loss:0.2715592536034885 val_loss0.5315659046173096\n",
            "epoch 73: train_loss:0.26273956136392046 val_loss0.5270522236824036\n",
            "epoch 74: train_loss:0.25414818176278126 val_loss0.5230010151863098\n",
            "epoch 75: train_loss:0.24586509765536935 val_loss0.5194225311279297\n",
            "epoch 76: train_loss:0.23785333952925228 val_loss0.516147255897522\n",
            "epoch 77: train_loss:0.23016082253810521 val_loss0.513118326663971\n",
            "epoch 78: train_loss:0.22276821198898392 val_loss0.5100849270820618\n",
            "epoch 79: train_loss:0.21568949490382866 val_loss0.5075687170028687\n",
            "epoch 80: train_loss:0.20872935286916053 val_loss0.5055301785469055\n",
            "epoch 81: train_loss:0.20200652041816497 val_loss0.5030569434165955\n",
            "epoch 82: train_loss:0.19552151325183945 val_loss0.5010927319526672\n",
            "epoch 83: train_loss:0.1892121773179587 val_loss0.499128520488739\n",
            "epoch 84: train_loss:0.18300415837281458 val_loss0.4973312020301819\n",
            "epoch 85: train_loss:0.17709568263711156 val_loss0.4956261217594147\n",
            "epoch 86: train_loss:0.17132411447462734 val_loss0.4939180016517639\n",
            "epoch 87: train_loss:0.1656837770463647 val_loss0.49252983927726746\n",
            "epoch 88: train_loss:0.16035642528587635 val_loss0.49130919575691223\n",
            "epoch 89: train_loss:0.15505954428567542 val_loss0.490083247423172\n",
            "epoch 90: train_loss:0.15005955006088223 val_loss0.4889584183692932\n",
            "epoch 91: train_loss:0.14512436360389264 val_loss0.48809266090393066\n",
            "epoch 92: train_loss:0.14040269044873951 val_loss0.4875023663043976\n",
            "epoch 93: train_loss:0.13584563486815035 val_loss0.48665791749954224\n",
            "epoch 94: train_loss:0.13142411964567932 val_loss0.4862082004547119\n",
            "epoch 95: train_loss:0.12708693766244897 val_loss0.48557162284851074\n",
            "epoch 96: train_loss:0.12295651301607355 val_loss0.48535987734794617\n",
            "epoch 97: train_loss:0.11891339229302364 val_loss0.4855193495750427\n",
            "epoch 98: train_loss:0.11502618062402215 val_loss0.4852498173713684\n",
            "epoch 99: train_loss:0.11128892180686062 val_loss0.48578041791915894\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting and evaluating the NN model with mini batch gradient descent\n",
        "\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X)\n",
        "    predictions = np.argmax(Yhat, axis=0)\n",
        "    return predictions\n",
        "\n",
        "# Use the trained parameters to make predictions on the test set\n",
        "test_predictions_mb = predict(parameters_mb, tf.convert_to_tensor(test_X, dtype=tf.float32))\n",
        "\n",
        "# Compare predicted labels with true labels to calculate test accuracy\n",
        "correct_predictions_mb = np.sum(test_predictions_mb == np.argmax(test_Y, axis=0))\n",
        "total_samples = test_Y.shape[1]\n",
        "test_accuracy = correct_predictions_mb / total_samples * 100\n",
        "print(\"Test Accuracy: {:.1f}%\".format(test_accuracy))\n",
        "\n",
        "# Plot a few images in the test set  with their predicted labels\n",
        "num_images_to_plot = 5\n",
        "random_indices = np.random.choice(test_X.shape[1], num_images_to_plot, replace=False)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "for i, idx in enumerate(random_indices, 1):\n",
        "    plt.subplot(1, num_images_to_plot, i)\n",
        "    plt.imshow(test_X[:, idx].reshape(40, 40), cmap='gray')\n",
        "    plt.title(\"True Label: {}\\nPredicted Label: {}\".format(test_Y[:, idx].argmax(), test_predictions_mb[idx]))\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "j9dDMudeCBeG",
        "outputId": "f26b3d03-ad56-49b1-d513-6beaddf4d599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 86.0%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a model without mini batch parameter, which we did in our last assignment. This is the same model we will run to see how the learning curve is showing."
      ],
      "metadata": {
        "id": "i48qb9Njco2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Neural Network Model\n",
        "\n",
        "def create_nn_model(train_X, train_Y, val_X, val_Y, num_iterations, learning_rate, nh1, nh2):\n",
        "    nx, m = train_X.shape\n",
        "    ny = 10  # Number of output neurons for multi-class classification\n",
        "\n",
        "    parameters = initialize_parameters(nx, nh1, nh2, ny)\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            train_Yhat = forward_pass(parameters, tf.convert_to_tensor(train_X, dtype=tf.float32))\n",
        "            train_loss = compute_loss(tf.convert_to_tensor(train_Y, dtype=tf.float32), train_Yhat)\n",
        "\n",
        "        val_Yhat = forward_pass(parameters, tf.convert_to_tensor(val_X, dtype=tf.float32))\n",
        "        val_loss = compute_loss(tf.convert_to_tensor(val_Y, dtype=tf.float32), val_Yhat)\n",
        "\n",
        "        print(\"iteration {}: train_loss:{} val_loss{}\".format(i, train_loss.numpy(), val_loss.numpy()))\n",
        "\n",
        "        train_losses.append(train_loss.numpy())\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        gradients = backward_pass(parameters, train_loss, tape)\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return parameters, history\n",
        "\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 4000\n",
        "nh1 = 128\n",
        "nh2 = 64\n",
        "\n",
        "# Train the model\n",
        "parameters, history = create_nn_model(train_X, train_Y, test_X, test_Y, num_iterations, learning_rate, nh1, nh2)\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.plot(history['train_loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jW7OqmxXdAsc",
        "outputId": "ea362331-aac4-4b06-9c32-537a304ef5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0: train_loss:2.3026506900787354 val_loss2.302661418914795\n",
            "iteration 1: train_loss:2.3026492595672607 val_loss2.3026602268218994\n",
            "iteration 2: train_loss:2.302647829055786 val_loss2.3026585578918457\n",
            "iteration 3: train_loss:2.3026461601257324 val_loss2.3026576042175293\n",
            "iteration 4: train_loss:2.3026444911956787 val_loss2.302656412124634\n",
            "iteration 5: train_loss:2.302642822265625 val_loss2.302654981613159\n",
            "iteration 6: train_loss:2.3026416301727295 val_loss2.3026535511016846\n",
            "iteration 7: train_loss:2.302640199661255 val_loss2.302652597427368\n",
            "iteration 8: train_loss:2.302638530731201 val_loss2.3026514053344727\n",
            "iteration 9: train_loss:2.3026371002197266 val_loss2.302649974822998\n",
            "iteration 10: train_loss:2.302635669708252 val_loss2.3026487827301025\n",
            "iteration 11: train_loss:2.3026340007781982 val_loss2.302647352218628\n",
            "iteration 12: train_loss:2.3026323318481445 val_loss2.3026461601257324\n",
            "iteration 13: train_loss:2.30263090133667 val_loss2.302644729614258\n",
            "iteration 14: train_loss:2.3026294708251953 val_loss2.3026435375213623\n",
            "iteration 15: train_loss:2.3026278018951416 val_loss2.302642345428467\n",
            "iteration 16: train_loss:2.302626371383667 val_loss2.302640914916992\n",
            "iteration 17: train_loss:2.3026247024536133 val_loss2.3026397228240967\n",
            "iteration 18: train_loss:2.3026232719421387 val_loss2.302638530731201\n",
            "iteration 19: train_loss:2.302621841430664 val_loss2.3026371002197266\n",
            "iteration 20: train_loss:2.3026204109191895 val_loss2.302635908126831\n",
            "iteration 21: train_loss:2.3026187419891357 val_loss2.3026347160339355\n",
            "iteration 22: train_loss:2.302617311477661 val_loss2.30263352394104\n",
            "iteration 23: train_loss:2.3026156425476074 val_loss2.3026323318481445\n",
            "iteration 24: train_loss:2.302614450454712 val_loss2.30263090133667\n",
            "iteration 25: train_loss:2.302612781524658 val_loss2.3026297092437744\n",
            "iteration 26: train_loss:2.3026115894317627 val_loss2.302628517150879\n",
            "iteration 27: train_loss:2.302609920501709 val_loss2.3026270866394043\n",
            "iteration 28: train_loss:2.3026084899902344 val_loss2.302625894546509\n",
            "iteration 29: train_loss:2.3026068210601807 val_loss2.3026247024536133\n",
            "iteration 30: train_loss:2.302605390548706 val_loss2.3026232719421387\n",
            "iteration 31: train_loss:2.3026039600372314 val_loss2.3026223182678223\n",
            "iteration 32: train_loss:2.3026022911071777 val_loss2.3026211261749268\n",
            "iteration 33: train_loss:2.3026010990142822 val_loss2.3026199340820312\n",
            "iteration 34: train_loss:2.3025994300842285 val_loss2.3026185035705566\n",
            "iteration 35: train_loss:2.302597999572754 val_loss2.302617311477661\n",
            "iteration 36: train_loss:2.3025965690612793 val_loss2.3026161193847656\n",
            "iteration 37: train_loss:2.3025951385498047 val_loss2.30261492729187\n",
            "iteration 38: train_loss:2.30259370803833 val_loss2.3026134967803955\n",
            "iteration 39: train_loss:2.3025920391082764 val_loss2.302612543106079\n",
            "iteration 40: train_loss:2.302590847015381 val_loss2.3026111125946045\n",
            "iteration 41: train_loss:2.302589178085327 val_loss2.302609920501709\n",
            "iteration 42: train_loss:2.3025879859924316 val_loss2.3026087284088135\n",
            "iteration 43: train_loss:2.302586317062378 val_loss2.302607536315918\n",
            "iteration 44: train_loss:2.3025848865509033 val_loss2.3026063442230225\n",
            "iteration 45: train_loss:2.3025834560394287 val_loss2.302604913711548\n",
            "iteration 46: train_loss:2.302582025527954 val_loss2.3026039600372314\n",
            "iteration 47: train_loss:2.3025805950164795 val_loss2.302602767944336\n",
            "iteration 48: train_loss:2.302579402923584 val_loss2.3026015758514404\n",
            "iteration 49: train_loss:2.3025777339935303 val_loss2.302600145339966\n",
            "iteration 50: train_loss:2.3025763034820557 val_loss2.3025991916656494\n",
            "iteration 51: train_loss:2.302574872970581 val_loss2.302597761154175\n",
            "iteration 52: train_loss:2.3025734424591064 val_loss2.3025968074798584\n",
            "iteration 53: train_loss:2.302572011947632 val_loss2.302595615386963\n",
            "iteration 54: train_loss:2.3025705814361572 val_loss2.3025941848754883\n",
            "iteration 55: train_loss:2.3025691509246826 val_loss2.3025929927825928\n",
            "iteration 56: train_loss:2.302567720413208 val_loss2.3025920391082764\n",
            "iteration 57: train_loss:2.3025660514831543 val_loss2.302590847015381\n",
            "iteration 58: train_loss:2.3025646209716797 val_loss2.3025896549224854\n",
            "iteration 59: train_loss:2.302563190460205 val_loss2.3025882244110107\n",
            "iteration 60: train_loss:2.3025619983673096 val_loss2.3025870323181152\n",
            "iteration 61: train_loss:2.302560567855835 val_loss2.302586078643799\n",
            "iteration 62: train_loss:2.3025591373443604 val_loss2.302584648132324\n",
            "iteration 63: train_loss:2.3025574684143066 val_loss2.3025834560394287\n",
            "iteration 64: train_loss:2.302556276321411 val_loss2.3025825023651123\n",
            "iteration 65: train_loss:2.3025548458099365 val_loss2.302581310272217\n",
            "iteration 66: train_loss:2.302553415298462 val_loss2.3025801181793213\n",
            "iteration 67: train_loss:2.3025522232055664 val_loss2.302578926086426\n",
            "iteration 68: train_loss:2.302550792694092 val_loss2.3025777339935303\n",
            "iteration 69: train_loss:2.302549362182617 val_loss2.3025765419006348\n",
            "iteration 70: train_loss:2.3025479316711426 val_loss2.3025755882263184\n",
            "iteration 71: train_loss:2.302546262741089 val_loss2.3025741577148438\n",
            "iteration 72: train_loss:2.3025450706481934 val_loss2.3025729656219482\n",
            "iteration 73: train_loss:2.3025436401367188 val_loss2.302572011947632\n",
            "iteration 74: train_loss:2.302542209625244 val_loss2.3025705814361572\n",
            "iteration 75: train_loss:2.3025407791137695 val_loss2.302569627761841\n",
            "iteration 76: train_loss:2.302539348602295 val_loss2.3025684356689453\n",
            "iteration 77: train_loss:2.3025381565093994 val_loss2.3025670051574707\n",
            "iteration 78: train_loss:2.3025364875793457 val_loss2.3025660514831543\n",
            "iteration 79: train_loss:2.30253529548645 val_loss2.302565097808838\n",
            "iteration 80: train_loss:2.3025336265563965 val_loss2.3025639057159424\n",
            "iteration 81: train_loss:2.302532196044922 val_loss2.3025624752044678\n",
            "iteration 82: train_loss:2.3025310039520264 val_loss2.3025612831115723\n",
            "iteration 83: train_loss:2.3025295734405518 val_loss2.302560329437256\n",
            "iteration 84: train_loss:2.3025283813476562 val_loss2.3025593757629395\n",
            "iteration 85: train_loss:2.3025269508361816 val_loss2.302557945251465\n",
            "iteration 86: train_loss:2.302525520324707 val_loss2.3025569915771484\n",
            "iteration 87: train_loss:2.3025243282318115 val_loss2.302555799484253\n",
            "iteration 88: train_loss:2.302523136138916 val_loss2.3025546073913574\n",
            "iteration 89: train_loss:2.3025214672088623 val_loss2.302553415298462\n",
            "iteration 90: train_loss:2.302520275115967 val_loss2.3025524616241455\n",
            "iteration 91: train_loss:2.302518606185913 val_loss2.302551031112671\n",
            "iteration 92: train_loss:2.3025174140930176 val_loss2.3025500774383545\n",
            "iteration 93: train_loss:2.302515983581543 val_loss2.302548885345459\n",
            "iteration 94: train_loss:2.3025147914886475 val_loss2.3025479316711426\n",
            "iteration 95: train_loss:2.302513360977173 val_loss2.302546977996826\n",
            "iteration 96: train_loss:2.3025119304656982 val_loss2.3025455474853516\n",
            "iteration 97: train_loss:2.3025107383728027 val_loss2.302544355392456\n",
            "iteration 98: train_loss:2.302509069442749 val_loss2.3025431632995605\n",
            "iteration 99: train_loss:2.3025078773498535 val_loss2.302542209625244\n",
            "iteration 100: train_loss:2.302506685256958 val_loss2.3025410175323486\n",
            "iteration 101: train_loss:2.3025050163269043 val_loss2.302539825439453\n",
            "iteration 102: train_loss:2.302503824234009 val_loss2.3025386333465576\n",
            "iteration 103: train_loss:2.302502393722534 val_loss2.302537679672241\n",
            "iteration 104: train_loss:2.3025009632110596 val_loss2.302536725997925\n",
            "iteration 105: train_loss:2.302499532699585 val_loss2.3025355339050293\n",
            "iteration 106: train_loss:2.3024985790252686 val_loss2.302534341812134\n",
            "iteration 107: train_loss:2.302497148513794 val_loss2.3025331497192383\n",
            "iteration 108: train_loss:2.3024959564208984 val_loss2.302532196044922\n",
            "iteration 109: train_loss:2.302494525909424 val_loss2.3025312423706055\n",
            "iteration 110: train_loss:2.302493095397949 val_loss2.302529811859131\n",
            "iteration 111: train_loss:2.3024916648864746 val_loss2.3025288581848145\n",
            "iteration 112: train_loss:2.302490472793579 val_loss2.302527904510498\n",
            "iteration 113: train_loss:2.3024890422821045 val_loss2.3025264739990234\n",
            "iteration 114: train_loss:2.30248761177063 val_loss2.302525520324707\n",
            "iteration 115: train_loss:2.3024864196777344 val_loss2.3025243282318115\n",
            "iteration 116: train_loss:2.302485227584839 val_loss2.302523374557495\n",
            "iteration 117: train_loss:2.3024837970733643 val_loss2.3025221824645996\n",
            "iteration 118: train_loss:2.3024823665618896 val_loss2.302520990371704\n",
            "iteration 119: train_loss:2.302480936050415 val_loss2.3025200366973877\n",
            "iteration 120: train_loss:2.3024797439575195 val_loss2.302518844604492\n",
            "iteration 121: train_loss:2.302478551864624 val_loss2.302517890930176\n",
            "iteration 122: train_loss:2.3024771213531494 val_loss2.3025169372558594\n",
            "iteration 123: train_loss:2.302475690841675 val_loss2.3025155067443848\n",
            "iteration 124: train_loss:2.3024744987487793 val_loss2.3025145530700684\n",
            "iteration 125: train_loss:2.3024730682373047 val_loss2.302513599395752\n",
            "iteration 126: train_loss:2.302471876144409 val_loss2.3025121688842773\n",
            "iteration 127: train_loss:2.3024704456329346 val_loss2.302511215209961\n",
            "iteration 128: train_loss:2.302469253540039 val_loss2.3025102615356445\n",
            "iteration 129: train_loss:2.3024678230285645 val_loss2.302509069442749\n",
            "iteration 130: train_loss:2.302466630935669 val_loss2.3025081157684326\n",
            "iteration 131: train_loss:2.3024654388427734 val_loss2.302507162094116\n",
            "iteration 132: train_loss:2.302464008331299 val_loss2.3025057315826416\n",
            "iteration 133: train_loss:2.302462577819824 val_loss2.302504777908325\n",
            "iteration 134: train_loss:2.3024613857269287 val_loss2.302503824234009\n",
            "iteration 135: train_loss:2.302460193634033 val_loss2.3025028705596924\n",
            "iteration 136: train_loss:2.3024587631225586 val_loss2.302501678466797\n",
            "iteration 137: train_loss:2.302457571029663 val_loss2.3025004863739014\n",
            "iteration 138: train_loss:2.3024563789367676 val_loss2.302499532699585\n",
            "iteration 139: train_loss:2.302454948425293 val_loss2.3024983406066895\n",
            "iteration 140: train_loss:2.3024535179138184 val_loss2.302497386932373\n",
            "iteration 141: train_loss:2.302452325820923 val_loss2.3024961948394775\n",
            "iteration 142: train_loss:2.3024511337280273 val_loss2.302495241165161\n",
            "iteration 143: train_loss:2.3024497032165527 val_loss2.3024942874908447\n",
            "iteration 144: train_loss:2.3024485111236572 val_loss2.302493095397949\n",
            "iteration 145: train_loss:2.3024470806121826 val_loss2.302492141723633\n",
            "iteration 146: train_loss:2.302445888519287 val_loss2.3024909496307373\n",
            "iteration 147: train_loss:2.3024446964263916 val_loss2.302489757537842\n",
            "iteration 148: train_loss:2.302443265914917 val_loss2.3024888038635254\n",
            "iteration 149: train_loss:2.3024420738220215 val_loss2.302487850189209\n",
            "iteration 150: train_loss:2.302440643310547 val_loss2.3024868965148926\n",
            "iteration 151: train_loss:2.3024394512176514 val_loss2.302485466003418\n",
            "iteration 152: train_loss:2.302438259124756 val_loss2.3024845123291016\n",
            "iteration 153: train_loss:2.3024370670318604 val_loss2.302483558654785\n",
            "iteration 154: train_loss:2.3024356365203857 val_loss2.3024823665618896\n",
            "iteration 155: train_loss:2.3024344444274902 val_loss2.3024816513061523\n",
            "iteration 156: train_loss:2.3024330139160156 val_loss2.302480459213257\n",
            "iteration 157: train_loss:2.30243182182312 val_loss2.3024795055389404\n",
            "iteration 158: train_loss:2.3024306297302246 val_loss2.302478313446045\n",
            "iteration 159: train_loss:2.302429437637329 val_loss2.3024773597717285\n",
            "iteration 160: train_loss:2.3024282455444336 val_loss2.302476167678833\n",
            "iteration 161: train_loss:2.302426815032959 val_loss2.3024752140045166\n",
            "iteration 162: train_loss:2.3024256229400635 val_loss2.3024742603302\n",
            "iteration 163: train_loss:2.302424192428589 val_loss2.3024730682373047\n",
            "iteration 164: train_loss:2.3024230003356934 val_loss2.302471876144409\n",
            "iteration 165: train_loss:2.302421808242798 val_loss2.3024709224700928\n",
            "iteration 166: train_loss:2.3024206161499023 val_loss2.3024697303771973\n",
            "iteration 167: train_loss:2.3024191856384277 val_loss2.30246901512146\n",
            "iteration 168: train_loss:2.3024179935455322 val_loss2.3024678230285645\n",
            "iteration 169: train_loss:2.3024168014526367 val_loss2.302466869354248\n",
            "iteration 170: train_loss:2.302415370941162 val_loss2.3024659156799316\n",
            "iteration 171: train_loss:2.3024141788482666 val_loss2.3024649620056152\n",
            "iteration 172: train_loss:2.302412748336792 val_loss2.3024635314941406\n",
            "iteration 173: train_loss:2.3024115562438965 val_loss2.302462577819824\n",
            "iteration 174: train_loss:2.30241060256958 val_loss2.302461624145508\n",
            "iteration 175: train_loss:2.3024094104766846 val_loss2.3024606704711914\n",
            "iteration 176: train_loss:2.30240797996521 val_loss2.302459478378296\n",
            "iteration 177: train_loss:2.3024067878723145 val_loss2.3024585247039795\n",
            "iteration 178: train_loss:2.302405595779419 val_loss2.302457571029663\n",
            "iteration 179: train_loss:2.3024044036865234 val_loss2.3024563789367676\n",
            "iteration 180: train_loss:2.302402973175049 val_loss2.302455425262451\n",
            "iteration 181: train_loss:2.3024017810821533 val_loss2.3024544715881348\n",
            "iteration 182: train_loss:2.302400588989258 val_loss2.3024535179138184\n",
            "iteration 183: train_loss:2.302399158477783 val_loss2.302452325820923\n",
            "iteration 184: train_loss:2.302398204803467 val_loss2.3024511337280273\n",
            "iteration 185: train_loss:2.3023970127105713 val_loss2.302450180053711\n",
            "iteration 186: train_loss:2.3023955821990967 val_loss2.3024492263793945\n",
            "iteration 187: train_loss:2.302394390106201 val_loss2.302448272705078\n",
            "iteration 188: train_loss:2.3023931980133057 val_loss2.3024473190307617\n",
            "iteration 189: train_loss:2.30239200592041 val_loss2.3024463653564453\n",
            "iteration 190: train_loss:2.3023905754089355 val_loss2.302445411682129\n",
            "iteration 191: train_loss:2.302389621734619 val_loss2.3024442195892334\n",
            "iteration 192: train_loss:2.3023881912231445 val_loss2.302443265914917\n",
            "iteration 193: train_loss:2.302386999130249 val_loss2.3024423122406006\n",
            "iteration 194: train_loss:2.3023858070373535 val_loss2.302441358566284\n",
            "iteration 195: train_loss:2.302384376525879 val_loss2.3024404048919678\n",
            "iteration 196: train_loss:2.3023834228515625 val_loss2.3024394512176514\n",
            "iteration 197: train_loss:2.302382230758667 val_loss2.302438259124756\n",
            "iteration 198: train_loss:2.3023810386657715 val_loss2.3024370670318604\n",
            "iteration 199: train_loss:2.302379846572876 val_loss2.302436113357544\n",
            "iteration 200: train_loss:2.3023784160614014 val_loss2.3024351596832275\n",
            "iteration 201: train_loss:2.302377223968506 val_loss2.302434206008911\n",
            "iteration 202: train_loss:2.3023760318756104 val_loss2.3024332523345947\n",
            "iteration 203: train_loss:2.302374839782715 val_loss2.3024322986602783\n",
            "iteration 204: train_loss:2.3023736476898193 val_loss2.302431344985962\n",
            "iteration 205: train_loss:2.302372455596924 val_loss2.3024303913116455\n",
            "iteration 206: train_loss:2.3023712635040283 val_loss2.30242919921875\n",
            "iteration 207: train_loss:2.302370071411133 val_loss2.3024282455444336\n",
            "iteration 208: train_loss:2.302368640899658 val_loss2.302427291870117\n",
            "iteration 209: train_loss:2.302367687225342 val_loss2.302426338195801\n",
            "iteration 210: train_loss:2.3023664951324463 val_loss2.3024251461029053\n",
            "iteration 211: train_loss:2.302365303039551 val_loss2.302424430847168\n",
            "iteration 212: train_loss:2.3023641109466553 val_loss2.3024234771728516\n",
            "iteration 213: train_loss:2.3023626804351807 val_loss2.302422285079956\n",
            "iteration 214: train_loss:2.3023617267608643 val_loss2.3024210929870605\n",
            "iteration 215: train_loss:2.3023602962493896 val_loss2.302420139312744\n",
            "iteration 216: train_loss:2.3023593425750732 val_loss2.302419424057007\n",
            "iteration 217: train_loss:2.3023579120635986 val_loss2.3024182319641113\n",
            "iteration 218: train_loss:2.3023569583892822 val_loss2.302417278289795\n",
            "iteration 219: train_loss:2.3023555278778076 val_loss2.3024163246154785\n",
            "iteration 220: train_loss:2.302354574203491 val_loss2.302415370941162\n",
            "iteration 221: train_loss:2.3023533821105957 val_loss2.3024144172668457\n",
            "iteration 222: train_loss:2.3023521900177 val_loss2.3024134635925293\n",
            "iteration 223: train_loss:2.3023509979248047 val_loss2.302412509918213\n",
            "iteration 224: train_loss:2.30234956741333 val_loss2.3024113178253174\n",
            "iteration 225: train_loss:2.3023486137390137 val_loss2.30241060256958\n",
            "iteration 226: train_loss:2.302347421646118 val_loss2.3024094104766846\n",
            "iteration 227: train_loss:2.3023459911346436 val_loss2.302408456802368\n",
            "iteration 228: train_loss:2.302344799041748 val_loss2.3024075031280518\n",
            "iteration 229: train_loss:2.3023438453674316 val_loss2.3024065494537354\n",
            "iteration 230: train_loss:2.302342414855957 val_loss2.302405595779419\n",
            "iteration 231: train_loss:2.3023412227630615 val_loss2.3024046421051025\n",
            "iteration 232: train_loss:2.302340269088745 val_loss2.3024039268493652\n",
            "iteration 233: train_loss:2.3023388385772705 val_loss2.3024027347564697\n",
            "iteration 234: train_loss:2.302337884902954 val_loss2.3024017810821533\n",
            "iteration 235: train_loss:2.3023366928100586 val_loss2.302401065826416\n",
            "iteration 236: train_loss:2.302335500717163 val_loss2.3023996353149414\n",
            "iteration 237: train_loss:2.3023343086242676 val_loss2.302398681640625\n",
            "iteration 238: train_loss:2.302333116531372 val_loss2.3023979663848877\n",
            "iteration 239: train_loss:2.3023319244384766 val_loss2.302396774291992\n",
            "iteration 240: train_loss:2.302330732345581 val_loss2.302395820617676\n",
            "iteration 241: train_loss:2.3023297786712646 val_loss2.3023948669433594\n",
            "iteration 242: train_loss:2.30232834815979 val_loss2.302393674850464\n",
            "iteration 243: train_loss:2.3023273944854736 val_loss2.3023929595947266\n",
            "iteration 244: train_loss:2.302326202392578 val_loss2.302391767501831\n",
            "iteration 245: train_loss:2.3023250102996826 val_loss2.3023910522460938\n",
            "iteration 246: train_loss:2.302323818206787 val_loss2.3023898601531982\n",
            "iteration 247: train_loss:2.3023226261138916 val_loss2.302388906478882\n",
            "iteration 248: train_loss:2.302321434020996 val_loss2.3023881912231445\n",
            "iteration 249: train_loss:2.3023202419281006 val_loss2.302387237548828\n",
            "iteration 250: train_loss:2.302319288253784 val_loss2.3023862838745117\n",
            "iteration 251: train_loss:2.3023178577423096 val_loss2.3023853302001953\n",
            "iteration 252: train_loss:2.302316904067993 val_loss2.302384376525879\n",
            "iteration 253: train_loss:2.3023154735565186 val_loss2.3023834228515625\n",
            "iteration 254: train_loss:2.302314519882202 val_loss2.302382469177246\n",
            "iteration 255: train_loss:2.3023133277893066 val_loss2.3023815155029297\n",
            "iteration 256: train_loss:2.302312135696411 val_loss2.3023805618286133\n",
            "iteration 257: train_loss:2.3023109436035156 val_loss2.302379608154297\n",
            "iteration 258: train_loss:2.30230975151062 val_loss2.3023786544799805\n",
            "iteration 259: train_loss:2.3023085594177246 val_loss2.302377700805664\n",
            "iteration 260: train_loss:2.302307367324829 val_loss2.3023767471313477\n",
            "iteration 261: train_loss:2.3023064136505127 val_loss2.3023757934570312\n",
            "iteration 262: train_loss:2.302305221557617 val_loss2.302374839782715\n",
            "iteration 263: train_loss:2.3023037910461426 val_loss2.3023738861083984\n",
            "iteration 264: train_loss:2.302302837371826 val_loss2.302372932434082\n",
            "iteration 265: train_loss:2.3023016452789307 val_loss2.3023719787597656\n",
            "iteration 266: train_loss:2.3023006916046143 val_loss2.302371025085449\n",
            "iteration 267: train_loss:2.3022994995117188 val_loss2.302370071411133\n",
            "iteration 268: train_loss:2.302298069000244 val_loss2.3023688793182373\n",
            "iteration 269: train_loss:2.302297353744507 val_loss2.3023681640625\n",
            "iteration 270: train_loss:2.3022959232330322 val_loss2.3023669719696045\n",
            "iteration 271: train_loss:2.302294969558716 val_loss2.302366018295288\n",
            "iteration 272: train_loss:2.3022937774658203 val_loss2.302365303039551\n",
            "iteration 273: train_loss:2.3022923469543457 val_loss2.3023641109466553\n",
            "iteration 274: train_loss:2.3022913932800293 val_loss2.302363395690918\n",
            "iteration 275: train_loss:2.302290201187134 val_loss2.3023624420166016\n",
            "iteration 276: train_loss:2.3022892475128174 val_loss2.302361488342285\n",
            "iteration 277: train_loss:2.302288055419922 val_loss2.3023605346679688\n",
            "iteration 278: train_loss:2.3022868633270264 val_loss2.3023595809936523\n",
            "iteration 279: train_loss:2.302285671234131 val_loss2.302358627319336\n",
            "iteration 280: train_loss:2.3022847175598145 val_loss2.3023576736450195\n",
            "iteration 281: train_loss:2.302283525466919 val_loss2.302356719970703\n",
            "iteration 282: train_loss:2.3022823333740234 val_loss2.3023557662963867\n",
            "iteration 283: train_loss:2.302281141281128 val_loss2.3023548126220703\n",
            "iteration 284: train_loss:2.3022799491882324 val_loss2.302353858947754\n",
            "iteration 285: train_loss:2.302278995513916 val_loss2.3023529052734375\n",
            "iteration 286: train_loss:2.3022775650024414 val_loss2.302351951599121\n",
            "iteration 287: train_loss:2.302276611328125 val_loss2.302351236343384\n",
            "iteration 288: train_loss:2.3022756576538086 val_loss2.3023500442504883\n",
            "iteration 289: train_loss:2.302274227142334 val_loss2.302349090576172\n",
            "iteration 290: train_loss:2.3022732734680176 val_loss2.3023481369018555\n",
            "iteration 291: train_loss:2.302272081375122 val_loss2.302347421646118\n",
            "iteration 292: train_loss:2.3022708892822266 val_loss2.3023464679718018\n",
            "iteration 293: train_loss:2.30226993560791 val_loss2.3023455142974854\n",
            "iteration 294: train_loss:2.3022687435150146 val_loss2.302344560623169\n",
            "iteration 295: train_loss:2.302267551422119 val_loss2.3023436069488525\n",
            "iteration 296: train_loss:2.3022663593292236 val_loss2.302342653274536\n",
            "iteration 297: train_loss:2.3022654056549072 val_loss2.302341938018799\n",
            "iteration 298: train_loss:2.3022642135620117 val_loss2.3023407459259033\n",
            "iteration 299: train_loss:2.302263021469116 val_loss2.302339792251587\n",
            "iteration 300: train_loss:2.3022618293762207 val_loss2.3023388385772705\n",
            "iteration 301: train_loss:2.3022608757019043 val_loss2.302338123321533\n",
            "iteration 302: train_loss:2.302259683609009 val_loss2.302337169647217\n",
            "iteration 303: train_loss:2.302258253097534 val_loss2.3023362159729004\n",
            "iteration 304: train_loss:2.3022572994232178 val_loss2.302335262298584\n",
            "iteration 305: train_loss:2.3022561073303223 val_loss2.3023343086242676\n",
            "iteration 306: train_loss:2.302255153656006 val_loss2.302333354949951\n",
            "iteration 307: train_loss:2.3022541999816895 val_loss2.3023324012756348\n",
            "iteration 308: train_loss:2.302253007888794 val_loss2.3023314476013184\n",
            "iteration 309: train_loss:2.3022518157958984 val_loss2.302330493927002\n",
            "iteration 310: train_loss:2.302250623703003 val_loss2.3023295402526855\n",
            "iteration 311: train_loss:2.3022494316101074 val_loss2.3023288249969482\n",
            "iteration 312: train_loss:2.302248477935791 val_loss2.302327871322632\n",
            "iteration 313: train_loss:2.3022472858428955 val_loss2.3023269176483154\n",
            "iteration 314: train_loss:2.302246332168579 val_loss2.302325963973999\n",
            "iteration 315: train_loss:2.3022449016571045 val_loss2.3023250102996826\n",
            "iteration 316: train_loss:2.302243947982788 val_loss2.302324056625366\n",
            "iteration 317: train_loss:2.3022429943084717 val_loss2.30232310295105\n",
            "iteration 318: train_loss:2.302241563796997 val_loss2.3023221492767334\n",
            "iteration 319: train_loss:2.3022403717041016 val_loss2.302321195602417\n",
            "iteration 320: train_loss:2.302239418029785 val_loss2.3023204803466797\n",
            "iteration 321: train_loss:2.3022384643554688 val_loss2.3023195266723633\n",
            "iteration 322: train_loss:2.3022372722625732 val_loss2.3023183345794678\n",
            "iteration 323: train_loss:2.3022360801696777 val_loss2.3023176193237305\n",
            "iteration 324: train_loss:2.3022351264953613 val_loss2.302316665649414\n",
            "iteration 325: train_loss:2.302234172821045 val_loss2.3023157119750977\n",
            "iteration 326: train_loss:2.3022329807281494 val_loss2.3023147583007812\n",
            "iteration 327: train_loss:2.302231550216675 val_loss2.302313804626465\n",
            "iteration 328: train_loss:2.3022305965423584 val_loss2.3023128509521484\n",
            "iteration 329: train_loss:2.302229404449463 val_loss2.302311897277832\n",
            "iteration 330: train_loss:2.3022282123565674 val_loss2.3023111820220947\n",
            "iteration 331: train_loss:2.302227258682251 val_loss2.3023102283477783\n",
            "iteration 332: train_loss:2.3022263050079346 val_loss2.302309274673462\n",
            "iteration 333: train_loss:2.302225112915039 val_loss2.3023083209991455\n",
            "iteration 334: train_loss:2.3022236824035645 val_loss2.302307367324829\n",
            "iteration 335: train_loss:2.302222728729248 val_loss2.302306652069092\n",
            "iteration 336: train_loss:2.3022217750549316 val_loss2.3023056983947754\n",
            "iteration 337: train_loss:2.302220582962036 val_loss2.302304744720459\n",
            "iteration 338: train_loss:2.3022196292877197 val_loss2.3023037910461426\n",
            "iteration 339: train_loss:2.302218437194824 val_loss2.302302837371826\n",
            "iteration 340: train_loss:2.302217483520508 val_loss2.3023018836975098\n",
            "iteration 341: train_loss:2.302216053009033 val_loss2.3023009300231934\n",
            "iteration 342: train_loss:2.302215099334717 val_loss2.302299976348877\n",
            "iteration 343: train_loss:2.3022139072418213 val_loss2.3022990226745605\n",
            "iteration 344: train_loss:2.302212953567505 val_loss2.302298069000244\n",
            "iteration 345: train_loss:2.3022115230560303 val_loss2.302297353744507\n",
            "iteration 346: train_loss:2.302210569381714 val_loss2.3022964000701904\n",
            "iteration 347: train_loss:2.3022096157073975 val_loss2.302295446395874\n",
            "iteration 348: train_loss:2.302208662033081 val_loss2.3022944927215576\n",
            "iteration 349: train_loss:2.3022072315216064 val_loss2.302293539047241\n",
            "iteration 350: train_loss:2.30220627784729 val_loss2.302292823791504\n",
            "iteration 351: train_loss:2.3022053241729736 val_loss2.3022918701171875\n",
            "iteration 352: train_loss:2.302204132080078 val_loss2.302290916442871\n",
            "iteration 353: train_loss:2.3022029399871826 val_loss2.3022899627685547\n",
            "iteration 354: train_loss:2.302201747894287 val_loss2.3022890090942383\n",
            "iteration 355: train_loss:2.3022007942199707 val_loss2.302288055419922\n",
            "iteration 356: train_loss:2.302199602127075 val_loss2.3022873401641846\n",
            "iteration 357: train_loss:2.302198648452759 val_loss2.302286148071289\n",
            "iteration 358: train_loss:2.3021974563598633 val_loss2.3022854328155518\n",
            "iteration 359: train_loss:2.3021962642669678 val_loss2.3022844791412354\n",
            "iteration 360: train_loss:2.3021953105926514 val_loss2.302283763885498\n",
            "iteration 361: train_loss:2.302194118499756 val_loss2.3022828102111816\n",
            "iteration 362: train_loss:2.3021929264068604 val_loss2.3022818565368652\n",
            "iteration 363: train_loss:2.302191972732544 val_loss2.302280902862549\n",
            "iteration 364: train_loss:2.3021907806396484 val_loss2.3022799491882324\n",
            "iteration 365: train_loss:2.302189826965332 val_loss2.302278995513916\n",
            "iteration 366: train_loss:2.3021888732910156 val_loss2.3022780418395996\n",
            "iteration 367: train_loss:2.302187442779541 val_loss2.3022773265838623\n",
            "iteration 368: train_loss:2.3021864891052246 val_loss2.302276372909546\n",
            "iteration 369: train_loss:2.302185297012329 val_loss2.3022754192352295\n",
            "iteration 370: train_loss:2.3021841049194336 val_loss2.302274465560913\n",
            "iteration 371: train_loss:2.302183151245117 val_loss2.3022735118865967\n",
            "iteration 372: train_loss:2.302182197570801 val_loss2.3022727966308594\n",
            "iteration 373: train_loss:2.3021812438964844 val_loss2.302271604537964\n",
            "iteration 374: train_loss:2.3021798133850098 val_loss2.3022708892822266\n",
            "iteration 375: train_loss:2.3021788597106934 val_loss2.30226993560791\n",
            "iteration 376: train_loss:2.302177906036377 val_loss2.3022689819335938\n",
            "iteration 377: train_loss:2.3021769523620605 val_loss2.3022680282592773\n",
            "iteration 378: train_loss:2.302175760269165 val_loss2.302267074584961\n",
            "iteration 379: train_loss:2.3021745681762695 val_loss2.3022661209106445\n",
            "iteration 380: train_loss:2.302173376083374 val_loss2.3022654056549072\n",
            "iteration 381: train_loss:2.3021724224090576 val_loss2.302264451980591\n",
            "iteration 382: train_loss:2.302171468734741 val_loss2.3022634983062744\n",
            "iteration 383: train_loss:2.3021702766418457 val_loss2.302262544631958\n",
            "iteration 384: train_loss:2.30216908454895 val_loss2.3022618293762207\n",
            "iteration 385: train_loss:2.302168130874634 val_loss2.302260637283325\n",
            "iteration 386: train_loss:2.3021669387817383 val_loss2.302259922027588\n",
            "iteration 387: train_loss:2.3021657466888428 val_loss2.3022589683532715\n",
            "iteration 388: train_loss:2.3021647930145264 val_loss2.302258014678955\n",
            "iteration 389: train_loss:2.30216383934021 val_loss2.3022572994232178\n",
            "iteration 390: train_loss:2.3021624088287354 val_loss2.3022563457489014\n",
            "iteration 391: train_loss:2.302161455154419 val_loss2.302255630493164\n",
            "iteration 392: train_loss:2.3021605014801025 val_loss2.3022546768188477\n",
            "iteration 393: train_loss:2.302159309387207 val_loss2.3022537231445312\n",
            "iteration 394: train_loss:2.3021581172943115 val_loss2.302252769470215\n",
            "iteration 395: train_loss:2.302157163619995 val_loss2.3022518157958984\n",
            "iteration 396: train_loss:2.3021559715270996 val_loss2.302250862121582\n",
            "iteration 397: train_loss:2.302155017852783 val_loss2.3022499084472656\n",
            "iteration 398: train_loss:2.3021538257598877 val_loss2.3022491931915283\n",
            "iteration 399: train_loss:2.302152633666992 val_loss2.302248239517212\n",
            "iteration 400: train_loss:2.302151679992676 val_loss2.3022472858428955\n",
            "iteration 401: train_loss:2.3021507263183594 val_loss2.302246332168579\n",
            "iteration 402: train_loss:2.302149534225464 val_loss2.3022453784942627\n",
            "iteration 403: train_loss:2.3021485805511475 val_loss2.3022446632385254\n",
            "iteration 404: train_loss:2.302147626876831 val_loss2.302243709564209\n",
            "iteration 405: train_loss:2.3021464347839355 val_loss2.3022427558898926\n",
            "iteration 406: train_loss:2.30214524269104 val_loss2.3022420406341553\n",
            "iteration 407: train_loss:2.3021440505981445 val_loss2.302241086959839\n",
            "iteration 408: train_loss:2.302143096923828 val_loss2.3022401332855225\n",
            "iteration 409: train_loss:2.3021421432495117 val_loss2.302239179611206\n",
            "iteration 410: train_loss:2.302140951156616 val_loss2.3022384643554688\n",
            "iteration 411: train_loss:2.3021399974823 val_loss2.3022375106811523\n",
            "iteration 412: train_loss:2.3021388053894043 val_loss2.302236557006836\n",
            "iteration 413: train_loss:2.302137613296509 val_loss2.3022356033325195\n",
            "iteration 414: train_loss:2.3021366596221924 val_loss2.302234649658203\n",
            "iteration 415: train_loss:2.302135467529297 val_loss2.302233934402466\n",
            "iteration 416: train_loss:2.3021342754364014 val_loss2.3022329807281494\n",
            "iteration 417: train_loss:2.302133321762085 val_loss2.302232027053833\n",
            "iteration 418: train_loss:2.3021323680877686 val_loss2.3022308349609375\n",
            "iteration 419: train_loss:2.302131175994873 val_loss2.3022301197052\n",
            "iteration 420: train_loss:2.3021299839019775 val_loss2.302229166030884\n",
            "iteration 421: train_loss:2.302129030227661 val_loss2.3022282123565674\n",
            "iteration 422: train_loss:2.3021278381347656 val_loss2.302227735519409\n",
            "iteration 423: train_loss:2.302126884460449 val_loss2.3022265434265137\n",
            "iteration 424: train_loss:2.3021256923675537 val_loss2.3022255897521973\n",
            "iteration 425: train_loss:2.3021247386932373 val_loss2.302224636077881\n",
            "iteration 426: train_loss:2.302123785018921 val_loss2.3022236824035645\n",
            "iteration 427: train_loss:2.3021225929260254 val_loss2.302222967147827\n",
            "iteration 428: train_loss:2.302121639251709 val_loss2.30222225189209\n",
            "iteration 429: train_loss:2.3021204471588135 val_loss2.3022210597991943\n",
            "iteration 430: train_loss:2.302119493484497 val_loss2.302220344543457\n",
            "iteration 431: train_loss:2.3021183013916016 val_loss2.3022193908691406\n",
            "iteration 432: train_loss:2.302117347717285 val_loss2.302218437194824\n",
            "iteration 433: train_loss:2.3021161556243896 val_loss2.302217483520508\n",
            "iteration 434: train_loss:2.302114963531494 val_loss2.3022165298461914\n",
            "iteration 435: train_loss:2.3021140098571777 val_loss2.302215814590454\n",
            "iteration 436: train_loss:2.3021130561828613 val_loss2.3022148609161377\n",
            "iteration 437: train_loss:2.302111864089966 val_loss2.3022139072418213\n",
            "iteration 438: train_loss:2.3021106719970703 val_loss2.302212953567505\n",
            "iteration 439: train_loss:2.302109718322754 val_loss2.3022119998931885\n",
            "iteration 440: train_loss:2.3021087646484375 val_loss2.302211046218872\n",
            "iteration 441: train_loss:2.302107572555542 val_loss2.3022103309631348\n",
            "iteration 442: train_loss:2.3021066188812256 val_loss2.3022093772888184\n",
            "iteration 443: train_loss:2.30210542678833 val_loss2.302208662033081\n",
            "iteration 444: train_loss:2.3021044731140137 val_loss2.3022074699401855\n",
            "iteration 445: train_loss:2.302103281021118 val_loss2.3022067546844482\n",
            "iteration 446: train_loss:2.3021020889282227 val_loss2.302205801010132\n",
            "iteration 447: train_loss:2.3021013736724854 val_loss2.3022048473358154\n",
            "iteration 448: train_loss:2.3020999431610107 val_loss2.302204132080078\n",
            "iteration 449: train_loss:2.3020989894866943 val_loss2.3022031784057617\n",
            "iteration 450: train_loss:2.302097797393799 val_loss2.3022022247314453\n",
            "iteration 451: train_loss:2.3020966053009033 val_loss2.302201271057129\n",
            "iteration 452: train_loss:2.302095651626587 val_loss2.3022003173828125\n",
            "iteration 453: train_loss:2.3020946979522705 val_loss2.302199602127075\n",
            "iteration 454: train_loss:2.302093744277954 val_loss2.302198886871338\n",
            "iteration 455: train_loss:2.3020925521850586 val_loss2.3021979331970215\n",
            "iteration 456: train_loss:2.302091360092163 val_loss2.302196979522705\n",
            "iteration 457: train_loss:2.3020901679992676 val_loss2.3021960258483887\n",
            "iteration 458: train_loss:2.3020894527435303 val_loss2.3021950721740723\n",
            "iteration 459: train_loss:2.3020882606506348 val_loss2.302194118499756\n",
            "iteration 460: train_loss:2.3020873069763184 val_loss2.3021934032440186\n",
            "iteration 461: train_loss:2.302086114883423 val_loss2.302192449569702\n",
            "iteration 462: train_loss:2.3020849227905273 val_loss2.3021914958953857\n",
            "iteration 463: train_loss:2.30208420753479 val_loss2.3021905422210693\n",
            "iteration 464: train_loss:2.3020830154418945 val_loss2.302189588546753\n",
            "iteration 465: train_loss:2.302081823348999 val_loss2.3021886348724365\n",
            "iteration 466: train_loss:2.3020808696746826 val_loss2.302187919616699\n",
            "iteration 467: train_loss:2.302079916000366 val_loss2.302187204360962\n",
            "iteration 468: train_loss:2.3020784854888916 val_loss2.3021862506866455\n",
            "iteration 469: train_loss:2.3020777702331543 val_loss2.302185297012329\n",
            "iteration 470: train_loss:2.302076578140259 val_loss2.3021843433380127\n",
            "iteration 471: train_loss:2.3020753860473633 val_loss2.302183151245117\n",
            "iteration 472: train_loss:2.3020741939544678 val_loss2.302182674407959\n",
            "iteration 473: train_loss:2.3020734786987305 val_loss2.3021817207336426\n",
            "iteration 474: train_loss:2.302072048187256 val_loss2.302180767059326\n",
            "iteration 475: train_loss:2.3020710945129395 val_loss2.3021798133850098\n",
            "iteration 476: train_loss:2.302070140838623 val_loss2.3021788597106934\n",
            "iteration 477: train_loss:2.3020689487457275 val_loss2.302178144454956\n",
            "iteration 478: train_loss:2.302067995071411 val_loss2.3021769523620605\n",
            "iteration 479: train_loss:2.3020668029785156 val_loss2.3021762371063232\n",
            "iteration 480: train_loss:2.302065849304199 val_loss2.302175521850586\n",
            "iteration 481: train_loss:2.3020646572113037 val_loss2.3021745681762695\n",
            "iteration 482: train_loss:2.3020637035369873 val_loss2.302173376083374\n",
            "iteration 483: train_loss:2.302062511444092 val_loss2.3021726608276367\n",
            "iteration 484: train_loss:2.3020615577697754 val_loss2.3021717071533203\n",
            "iteration 485: train_loss:2.30206036567688 val_loss2.302170753479004\n",
            "iteration 486: train_loss:2.3020591735839844 val_loss2.3021697998046875\n",
            "iteration 487: train_loss:2.302058219909668 val_loss2.302168846130371\n",
            "iteration 488: train_loss:2.3020572662353516 val_loss2.302168130874634\n",
            "iteration 489: train_loss:2.302055835723877 val_loss2.3021671772003174\n",
            "iteration 490: train_loss:2.3020551204681396 val_loss2.302166223526001\n",
            "iteration 491: train_loss:2.3020541667938232 val_loss2.3021652698516846\n",
            "iteration 492: train_loss:2.3020529747009277 val_loss2.302164316177368\n",
            "iteration 493: train_loss:2.3020517826080322 val_loss2.302163600921631\n",
            "iteration 494: train_loss:2.302050828933716 val_loss2.3021626472473145\n",
            "iteration 495: train_loss:2.3020496368408203 val_loss2.302161931991577\n",
            "iteration 496: train_loss:2.302048683166504 val_loss2.3021607398986816\n",
            "iteration 497: train_loss:2.3020474910736084 val_loss2.3021600246429443\n",
            "iteration 498: train_loss:2.302046537399292 val_loss2.302159070968628\n",
            "iteration 499: train_loss:2.3020455837249756 val_loss2.3021581172943115\n",
            "iteration 500: train_loss:2.30204439163208 val_loss2.302157163619995\n",
            "iteration 501: train_loss:2.3020429611206055 val_loss2.3021562099456787\n",
            "iteration 502: train_loss:2.302042245864868 val_loss2.3021554946899414\n",
            "iteration 503: train_loss:2.3020410537719727 val_loss2.302154541015625\n",
            "iteration 504: train_loss:2.302039861679077 val_loss2.3021538257598877\n",
            "iteration 505: train_loss:2.3020389080047607 val_loss2.302152633666992\n",
            "iteration 506: train_loss:2.3020379543304443 val_loss2.302151918411255\n",
            "iteration 507: train_loss:2.302036762237549 val_loss2.3021509647369385\n",
            "iteration 508: train_loss:2.3020358085632324 val_loss2.302150011062622\n",
            "iteration 509: train_loss:2.302034616470337 val_loss2.3021490573883057\n",
            "iteration 510: train_loss:2.3020336627960205 val_loss2.3021481037139893\n",
            "iteration 511: train_loss:2.302032470703125 val_loss2.302147388458252\n",
            "iteration 512: train_loss:2.3020315170288086 val_loss2.3021464347839355\n",
            "iteration 513: train_loss:2.302030324935913 val_loss2.302145481109619\n",
            "iteration 514: train_loss:2.3020293712615967 val_loss2.302144765853882\n",
            "iteration 515: train_loss:2.302027940750122 val_loss2.3021435737609863\n",
            "iteration 516: train_loss:2.3020269870758057 val_loss2.302142858505249\n",
            "iteration 517: train_loss:2.3020260334014893 val_loss2.3021419048309326\n",
            "iteration 518: train_loss:2.302025079727173 val_loss2.302140951156616\n",
            "iteration 519: train_loss:2.3020236492156982 val_loss2.3021399974823\n",
            "iteration 520: train_loss:2.302022933959961 val_loss2.3021390438079834\n",
            "iteration 521: train_loss:2.3020217418670654 val_loss2.302138090133667\n",
            "iteration 522: train_loss:2.302020788192749 val_loss2.302137613296509\n",
            "iteration 523: train_loss:2.3020198345184326 val_loss2.302136182785034\n",
            "iteration 524: train_loss:2.302018642425537 val_loss2.302135467529297\n",
            "iteration 525: train_loss:2.3020174503326416 val_loss2.3021345138549805\n",
            "iteration 526: train_loss:2.302016496658325 val_loss2.302133560180664\n",
            "iteration 527: train_loss:2.3020153045654297 val_loss2.3021328449249268\n",
            "iteration 528: train_loss:2.3020143508911133 val_loss2.3021318912506104\n",
            "iteration 529: train_loss:2.3020131587982178 val_loss2.302130699157715\n",
            "iteration 530: train_loss:2.3020122051239014 val_loss2.3021299839019775\n",
            "iteration 531: train_loss:2.302011013031006 val_loss2.302129030227661\n",
            "iteration 532: train_loss:2.3020100593566895 val_loss2.3021280765533447\n",
            "iteration 533: train_loss:2.302009105682373 val_loss2.3021271228790283\n",
            "iteration 534: train_loss:2.3020079135894775 val_loss2.302126407623291\n",
            "iteration 535: train_loss:2.302006721496582 val_loss2.3021254539489746\n",
            "iteration 536: train_loss:2.3020057678222656 val_loss2.3021247386932373\n",
            "iteration 537: train_loss:2.30200457572937 val_loss2.302123785018921\n",
            "iteration 538: train_loss:2.3020033836364746 val_loss2.3021225929260254\n",
            "iteration 539: train_loss:2.302002191543579 val_loss2.302121877670288\n",
            "iteration 540: train_loss:2.302001476287842 val_loss2.3021209239959717\n",
            "iteration 541: train_loss:2.302000045776367 val_loss2.3021199703216553\n",
            "iteration 542: train_loss:2.301999092102051 val_loss2.302119016647339\n",
            "iteration 543: train_loss:2.3019979000091553 val_loss2.3021180629730225\n",
            "iteration 544: train_loss:2.301996946334839 val_loss2.302117109298706\n",
            "iteration 545: train_loss:2.3019957542419434 val_loss2.3021163940429688\n",
            "iteration 546: train_loss:2.301994562149048 val_loss2.3021154403686523\n",
            "iteration 547: train_loss:2.3019936084747314 val_loss2.302114486694336\n",
            "iteration 548: train_loss:2.301992654800415 val_loss2.3021135330200195\n",
            "iteration 549: train_loss:2.3019917011260986 val_loss2.3021128177642822\n",
            "iteration 550: train_loss:2.301990509033203 val_loss2.3021116256713867\n",
            "iteration 551: train_loss:2.3019893169403076 val_loss2.3021106719970703\n",
            "iteration 552: train_loss:2.301988124847412 val_loss2.302109718322754\n",
            "iteration 553: train_loss:2.3019871711730957 val_loss2.3021090030670166\n",
            "iteration 554: train_loss:2.3019859790802 val_loss2.3021082878112793\n",
            "iteration 555: train_loss:2.301985025405884 val_loss2.302107095718384\n",
            "iteration 556: train_loss:2.3019840717315674 val_loss2.3021061420440674\n",
            "iteration 557: train_loss:2.301982879638672 val_loss2.302105188369751\n",
            "iteration 558: train_loss:2.3019819259643555 val_loss2.3021044731140137\n",
            "iteration 559: train_loss:2.30198073387146 val_loss2.302103281021118\n",
            "iteration 560: train_loss:2.3019795417785645 val_loss2.3021023273468018\n",
            "iteration 561: train_loss:2.301978588104248 val_loss2.3021016120910645\n",
            "iteration 562: train_loss:2.3019773960113525 val_loss2.302100658416748\n",
            "iteration 563: train_loss:2.301976203918457 val_loss2.3020997047424316\n",
            "iteration 564: train_loss:2.3019752502441406 val_loss2.3020989894866943\n",
            "iteration 565: train_loss:2.301974058151245 val_loss2.302097797393799\n",
            "iteration 566: train_loss:2.301973342895508 val_loss2.3020970821380615\n",
            "iteration 567: train_loss:2.301971912384033 val_loss2.302096128463745\n",
            "iteration 568: train_loss:2.301970958709717 val_loss2.3020951747894287\n",
            "iteration 569: train_loss:2.3019697666168213 val_loss2.3020942211151123\n",
            "iteration 570: train_loss:2.301968812942505 val_loss2.302093267440796\n",
            "iteration 571: train_loss:2.3019678592681885 val_loss2.3020923137664795\n",
            "iteration 572: train_loss:2.301966667175293 val_loss2.302091360092163\n",
            "iteration 573: train_loss:2.3019654750823975 val_loss2.3020904064178467\n",
            "iteration 574: train_loss:2.301964521408081 val_loss2.3020896911621094\n",
            "iteration 575: train_loss:2.3019633293151855 val_loss2.302088499069214\n",
            "iteration 576: train_loss:2.30196213722229 val_loss2.3020875453948975\n",
            "iteration 577: train_loss:2.3019611835479736 val_loss2.30208683013916\n",
            "iteration 578: train_loss:2.301959991455078 val_loss2.3020858764648438\n",
            "iteration 579: train_loss:2.3019590377807617 val_loss2.3020849227905273\n",
            "iteration 580: train_loss:2.301957845687866 val_loss2.302083969116211\n",
            "iteration 581: train_loss:2.3019566535949707 val_loss2.3020830154418945\n",
            "iteration 582: train_loss:2.3019556999206543 val_loss2.302082061767578\n",
            "iteration 583: train_loss:2.301954746246338 val_loss2.3020811080932617\n",
            "iteration 584: train_loss:2.3019535541534424 val_loss2.3020801544189453\n",
            "iteration 585: train_loss:2.3019521236419678 val_loss2.302079439163208\n",
            "iteration 586: train_loss:2.3019514083862305 val_loss2.3020784854888916\n",
            "iteration 587: train_loss:2.301949977874756 val_loss2.302077531814575\n",
            "iteration 588: train_loss:2.3019487857818604 val_loss2.3020763397216797\n",
            "iteration 589: train_loss:2.301948070526123 val_loss2.3020756244659424\n",
            "iteration 590: train_loss:2.3019468784332275 val_loss2.302074670791626\n",
            "iteration 591: train_loss:2.301945686340332 val_loss2.3020737171173096\n",
            "iteration 592: train_loss:2.3019447326660156 val_loss2.302072763442993\n",
            "iteration 593: train_loss:2.30194354057312 val_loss2.3020718097686768\n",
            "iteration 594: train_loss:2.3019425868988037 val_loss2.3020710945129395\n",
            "iteration 595: train_loss:2.301941394805908 val_loss2.302069902420044\n",
            "iteration 596: train_loss:2.301940441131592 val_loss2.3020689487457275\n",
            "iteration 597: train_loss:2.3019392490386963 val_loss2.302067995071411\n",
            "iteration 598: train_loss:2.301938056945801 val_loss2.302067279815674\n",
            "iteration 599: train_loss:2.3019371032714844 val_loss2.3020663261413574\n",
            "iteration 600: train_loss:2.301935911178589 val_loss2.302065134048462\n",
            "iteration 601: train_loss:2.3019347190856934 val_loss2.3020641803741455\n",
            "iteration 602: train_loss:2.301933765411377 val_loss2.302063226699829\n",
            "iteration 603: train_loss:2.3019325733184814 val_loss2.302062511444092\n",
            "iteration 604: train_loss:2.301931381225586 val_loss2.3020613193511963\n",
            "iteration 605: train_loss:2.3019304275512695 val_loss2.302060604095459\n",
            "iteration 606: train_loss:2.301929235458374 val_loss2.3020596504211426\n",
            "iteration 607: train_loss:2.3019282817840576 val_loss2.302058458328247\n",
            "iteration 608: train_loss:2.301927089691162 val_loss2.3020575046539307\n",
            "iteration 609: train_loss:2.3019258975982666 val_loss2.3020567893981934\n",
            "iteration 610: train_loss:2.30192494392395 val_loss2.302055835723877\n",
            "iteration 611: train_loss:2.3019237518310547 val_loss2.3020548820495605\n",
            "iteration 612: train_loss:2.301922559738159 val_loss2.302053689956665\n",
            "iteration 613: train_loss:2.3019216060638428 val_loss2.3020529747009277\n",
            "iteration 614: train_loss:2.3019204139709473 val_loss2.3020520210266113\n",
            "iteration 615: train_loss:2.3019192218780518 val_loss2.302050828933716\n",
            "iteration 616: train_loss:2.3019182682037354 val_loss2.3020498752593994\n",
            "iteration 617: train_loss:2.30191707611084 val_loss2.302049160003662\n",
            "iteration 618: train_loss:2.3019161224365234 val_loss2.3020482063293457\n",
            "iteration 619: train_loss:2.301914930343628 val_loss2.3020472526550293\n",
            "iteration 620: train_loss:2.3019137382507324 val_loss2.302046298980713\n",
            "iteration 621: train_loss:2.301912784576416 val_loss2.3020453453063965\n",
            "iteration 622: train_loss:2.3019115924835205 val_loss2.302044153213501\n",
            "iteration 623: train_loss:2.301910400390625 val_loss2.3020431995391846\n",
            "iteration 624: train_loss:2.3019094467163086 val_loss2.302042245864868\n",
            "iteration 625: train_loss:2.301908254623413 val_loss2.302041530609131\n",
            "iteration 626: train_loss:2.3019070625305176 val_loss2.3020403385162354\n",
            "iteration 627: train_loss:2.301906108856201 val_loss2.302039384841919\n",
            "iteration 628: train_loss:2.3019049167633057 val_loss2.3020386695861816\n",
            "iteration 629: train_loss:2.3019039630889893 val_loss2.302037477493286\n",
            "iteration 630: train_loss:2.3019025325775146 val_loss2.3020365238189697\n",
            "iteration 631: train_loss:2.3019015789031982 val_loss2.3020358085632324\n",
            "iteration 632: train_loss:2.3019003868103027 val_loss2.302034616470337\n",
            "iteration 633: train_loss:2.3018994331359863 val_loss2.3020336627960205\n",
            "iteration 634: train_loss:2.301898241043091 val_loss2.302032470703125\n",
            "iteration 635: train_loss:2.3018970489501953 val_loss2.3020317554473877\n",
            "iteration 636: train_loss:2.3018958568573 val_loss2.3020308017730713\n",
            "iteration 637: train_loss:2.3018946647644043 val_loss2.302029609680176\n",
            "iteration 638: train_loss:2.301893472671509 val_loss2.3020288944244385\n",
            "iteration 639: train_loss:2.3018925189971924 val_loss2.302027940750122\n",
            "iteration 640: train_loss:2.301891326904297 val_loss2.3020265102386475\n",
            "iteration 641: train_loss:2.3018901348114014 val_loss2.3020260334014893\n",
            "iteration 642: train_loss:2.301889181137085 val_loss2.3020248413085938\n",
            "iteration 643: train_loss:2.3018879890441895 val_loss2.3020238876342773\n",
            "iteration 644: train_loss:2.301887035369873 val_loss2.302022933959961\n",
            "iteration 645: train_loss:2.3018858432769775 val_loss2.3020219802856445\n",
            "iteration 646: train_loss:2.301884651184082 val_loss2.302021026611328\n",
            "iteration 647: train_loss:2.3018832206726074 val_loss2.3020200729370117\n",
            "iteration 648: train_loss:2.301882266998291 val_loss2.302018880844116\n",
            "iteration 649: train_loss:2.3018813133239746 val_loss2.3020179271698\n",
            "iteration 650: train_loss:2.301880121231079 val_loss2.3020172119140625\n",
            "iteration 651: train_loss:2.3018791675567627 val_loss2.302016019821167\n",
            "iteration 652: train_loss:2.301877737045288 val_loss2.3020150661468506\n",
            "iteration 653: train_loss:2.3018765449523926 val_loss2.302014112472534\n",
            "iteration 654: train_loss:2.301875352859497 val_loss2.3020131587982178\n",
            "iteration 655: train_loss:2.3018741607666016 val_loss2.3020122051239014\n",
            "iteration 656: train_loss:2.301873207092285 val_loss2.302011013031006\n",
            "iteration 657: train_loss:2.3018722534179688 val_loss2.3020100593566895\n",
            "iteration 658: train_loss:2.301870822906494 val_loss2.302009105682373\n",
            "iteration 659: train_loss:2.3018698692321777 val_loss2.3020083904266357\n",
            "iteration 660: train_loss:2.3018686771392822 val_loss2.3020074367523193\n",
            "iteration 661: train_loss:2.301867723464966 val_loss2.3020060062408447\n",
            "iteration 662: train_loss:2.3018665313720703 val_loss2.3020052909851074\n",
            "iteration 663: train_loss:2.301865339279175 val_loss2.302004337310791\n",
            "iteration 664: train_loss:2.3018643856048584 val_loss2.3020031452178955\n",
            "iteration 665: train_loss:2.301862955093384 val_loss2.302002191543579\n",
            "iteration 666: train_loss:2.3018620014190674 val_loss2.3020012378692627\n",
            "iteration 667: train_loss:2.301860809326172 val_loss2.3020002841949463\n",
            "iteration 668: train_loss:2.3018598556518555 val_loss2.30199933052063\n",
            "iteration 669: train_loss:2.301858425140381 val_loss2.3019981384277344\n",
            "iteration 670: train_loss:2.3018574714660645 val_loss2.301997184753418\n",
            "iteration 671: train_loss:2.301856517791748 val_loss2.3019962310791016\n",
            "iteration 672: train_loss:2.3018548488616943 val_loss2.3019955158233643\n",
            "iteration 673: train_loss:2.301853895187378 val_loss2.3019943237304688\n",
            "iteration 674: train_loss:2.3018527030944824 val_loss2.3019931316375732\n",
            "iteration 675: train_loss:2.301851511001587 val_loss2.3019919395446777\n",
            "iteration 676: train_loss:2.3018503189086914 val_loss2.3019912242889404\n",
            "iteration 677: train_loss:2.301849365234375 val_loss2.301990270614624\n",
            "iteration 678: train_loss:2.3018479347229004 val_loss2.3019890785217285\n",
            "iteration 679: train_loss:2.301846981048584 val_loss2.301988124847412\n",
            "iteration 680: train_loss:2.3018460273742676 val_loss2.3019871711730957\n",
            "iteration 681: train_loss:2.301844596862793 val_loss2.3019859790802\n",
            "iteration 682: train_loss:2.3018434047698975 val_loss2.301985025405884\n",
            "iteration 683: train_loss:2.301842212677002 val_loss2.3019840717315674\n",
            "iteration 684: train_loss:2.3018412590026855 val_loss2.301982879638672\n",
            "iteration 685: train_loss:2.301839828491211 val_loss2.3019821643829346\n",
            "iteration 686: train_loss:2.3018388748168945 val_loss2.301981210708618\n",
            "iteration 687: train_loss:2.301837682723999 val_loss2.3019800186157227\n",
            "iteration 688: train_loss:2.3018364906311035 val_loss2.301978826522827\n",
            "iteration 689: train_loss:2.301835536956787 val_loss2.30197811126709\n",
            "iteration 690: train_loss:2.3018343448638916 val_loss2.3019769191741943\n",
            "iteration 691: train_loss:2.301833152770996 val_loss2.301975965499878\n",
            "iteration 692: train_loss:2.3018319606781006 val_loss2.3019750118255615\n",
            "iteration 693: train_loss:2.301830530166626 val_loss2.301974058151245\n",
            "iteration 694: train_loss:2.3018293380737305 val_loss2.3019728660583496\n",
            "iteration 695: train_loss:2.301828384399414 val_loss2.3019721508026123\n",
            "iteration 696: train_loss:2.3018271923065186 val_loss2.3019707202911377\n",
            "iteration 697: train_loss:2.301825761795044 val_loss2.3019697666168213\n",
            "iteration 698: train_loss:2.3018248081207275 val_loss2.301969051361084\n",
            "iteration 699: train_loss:2.301823616027832 val_loss2.3019678592681885\n",
            "iteration 700: train_loss:2.3018224239349365 val_loss2.301966667175293\n",
            "iteration 701: train_loss:2.30182147026062 val_loss2.3019659519195557\n",
            "iteration 702: train_loss:2.3018200397491455 val_loss2.30196475982666\n",
            "iteration 703: train_loss:2.301819086074829 val_loss2.3019635677337646\n",
            "iteration 704: train_loss:2.3018178939819336 val_loss2.3019626140594482\n",
            "iteration 705: train_loss:2.301816701889038 val_loss2.301961660385132\n",
            "iteration 706: train_loss:2.3018155097961426 val_loss2.3019604682922363\n",
            "iteration 707: train_loss:2.301814556121826 val_loss2.30195951461792\n",
            "iteration 708: train_loss:2.3018133640289307 val_loss2.3019585609436035\n",
            "iteration 709: train_loss:2.301811933517456 val_loss2.301957368850708\n",
            "iteration 710: train_loss:2.3018107414245605 val_loss2.3019564151763916\n",
            "iteration 711: train_loss:2.301809787750244 val_loss2.301955461502075\n",
            "iteration 712: train_loss:2.3018085956573486 val_loss2.3019542694091797\n",
            "iteration 713: train_loss:2.301807403564453 val_loss2.301953077316284\n",
            "iteration 714: train_loss:2.3018059730529785 val_loss2.301952362060547\n",
            "iteration 715: train_loss:2.301804780960083 val_loss2.3019511699676514\n",
            "iteration 716: train_loss:2.3018035888671875 val_loss2.301950216293335\n",
            "iteration 717: train_loss:2.301802396774292 val_loss2.3019492626190186\n",
            "iteration 718: train_loss:2.3018012046813965 val_loss2.301948070526123\n",
            "iteration 719: train_loss:2.301800012588501 val_loss2.3019471168518066\n",
            "iteration 720: train_loss:2.3017988204956055 val_loss2.301945924758911\n",
            "iteration 721: train_loss:2.301797866821289 val_loss2.3019449710845947\n",
            "iteration 722: train_loss:2.3017964363098145 val_loss2.301943778991699\n",
            "iteration 723: train_loss:2.301795244216919 val_loss2.301943063735962\n",
            "iteration 724: train_loss:2.3017940521240234 val_loss2.3019416332244873\n",
            "iteration 725: train_loss:2.301792860031128 val_loss2.301940679550171\n",
            "iteration 726: train_loss:2.3017916679382324 val_loss2.3019397258758545\n",
            "iteration 727: train_loss:2.301790475845337 val_loss2.301938772201538\n",
            "iteration 728: train_loss:2.3017892837524414 val_loss2.3019375801086426\n",
            "iteration 729: train_loss:2.301788091659546 val_loss2.301936626434326\n",
            "iteration 730: train_loss:2.3017868995666504 val_loss2.3019354343414307\n",
            "iteration 731: train_loss:2.301785707473755 val_loss2.3019344806671143\n",
            "iteration 732: train_loss:2.3017845153808594 val_loss2.301933526992798\n",
            "iteration 733: train_loss:2.3017830848693848 val_loss2.3019323348999023\n",
            "iteration 734: train_loss:2.3017821311950684 val_loss2.301931142807007\n",
            "iteration 735: train_loss:2.301780939102173 val_loss2.3019299507141113\n",
            "iteration 736: train_loss:2.3017795085906982 val_loss2.301928997039795\n",
            "iteration 737: train_loss:2.301778554916382 val_loss2.3019280433654785\n",
            "iteration 738: train_loss:2.3017773628234863 val_loss2.301927089691162\n",
            "iteration 739: train_loss:2.3017759323120117 val_loss2.3019261360168457\n",
            "iteration 740: train_loss:2.301774740219116 val_loss2.301924705505371\n",
            "iteration 741: train_loss:2.3017735481262207 val_loss2.3019237518310547\n",
            "iteration 742: train_loss:2.3017725944519043 val_loss2.3019227981567383\n",
            "iteration 743: train_loss:2.3017711639404297 val_loss2.3019216060638428\n",
            "iteration 744: train_loss:2.301769971847534 val_loss2.3019204139709473\n",
            "iteration 745: train_loss:2.3017687797546387 val_loss2.301919460296631\n",
            "iteration 746: train_loss:2.301767587661743 val_loss2.3019182682037354\n",
            "iteration 747: train_loss:2.3017663955688477 val_loss2.301917314529419\n",
            "iteration 748: train_loss:2.301765203475952 val_loss2.3019163608551025\n",
            "iteration 749: train_loss:2.3017637729644775 val_loss2.301915168762207\n",
            "iteration 750: train_loss:2.301762580871582 val_loss2.3019139766693115\n",
            "iteration 751: train_loss:2.3017616271972656 val_loss2.301913022994995\n",
            "iteration 752: train_loss:2.301760196685791 val_loss2.3019115924835205\n",
            "iteration 753: train_loss:2.3017590045928955 val_loss2.301910638809204\n",
            "iteration 754: train_loss:2.301757574081421 val_loss2.3019096851348877\n",
            "iteration 755: train_loss:2.3017566204071045 val_loss2.301908493041992\n",
            "iteration 756: train_loss:2.301755428314209 val_loss2.301907539367676\n",
            "iteration 757: train_loss:2.3017537593841553 val_loss2.3019063472747803\n",
            "iteration 758: train_loss:2.301752805709839 val_loss2.3019051551818848\n",
            "iteration 759: train_loss:2.3017518520355225 val_loss2.3019042015075684\n",
            "iteration 760: train_loss:2.301750421524048 val_loss2.301903009414673\n",
            "iteration 761: train_loss:2.3017489910125732 val_loss2.3019018173217773\n",
            "iteration 762: train_loss:2.3017477989196777 val_loss2.301900863647461\n",
            "iteration 763: train_loss:2.3017466068267822 val_loss2.3018996715545654\n",
            "iteration 764: train_loss:2.301745653152466 val_loss2.301898717880249\n",
            "iteration 765: train_loss:2.301744222640991 val_loss2.3018975257873535\n",
            "iteration 766: train_loss:2.3017430305480957 val_loss2.301896333694458\n",
            "iteration 767: train_loss:2.301741600036621 val_loss2.3018951416015625\n",
            "iteration 768: train_loss:2.3017404079437256 val_loss2.301894187927246\n",
            "iteration 769: train_loss:2.30173921585083 val_loss2.3018929958343506\n",
            "iteration 770: train_loss:2.3017377853393555 val_loss2.301892042160034\n",
            "iteration 771: train_loss:2.30173659324646 val_loss2.3018908500671387\n",
            "iteration 772: train_loss:2.3017354011535645 val_loss2.301889657974243\n",
            "iteration 773: train_loss:2.30173397064209 val_loss2.3018887042999268\n",
            "iteration 774: train_loss:2.3017327785491943 val_loss2.3018875122070312\n",
            "iteration 775: train_loss:2.301731586456299 val_loss2.3018863201141357\n",
            "iteration 776: train_loss:2.3017303943634033 val_loss2.3018853664398193\n",
            "iteration 777: train_loss:2.3017289638519287 val_loss2.301884174346924\n",
            "iteration 778: train_loss:2.301727771759033 val_loss2.3018829822540283\n",
            "iteration 779: train_loss:2.3017265796661377 val_loss2.301882028579712\n",
            "iteration 780: train_loss:2.301725149154663 val_loss2.3018808364868164\n",
            "iteration 781: train_loss:2.3017239570617676 val_loss2.301879405975342\n",
            "iteration 782: train_loss:2.301722526550293 val_loss2.3018784523010254\n",
            "iteration 783: train_loss:2.3017215728759766 val_loss2.301877498626709\n",
            "iteration 784: train_loss:2.301720380783081 val_loss2.3018760681152344\n",
            "iteration 785: train_loss:2.3017191886901855 val_loss2.301875114440918\n",
            "iteration 786: train_loss:2.301717758178711 val_loss2.3018741607666016\n",
            "iteration 787: train_loss:2.3017165660858154 val_loss2.301872968673706\n",
            "iteration 788: train_loss:2.30171537399292 val_loss2.3018717765808105\n",
            "iteration 789: train_loss:2.3017139434814453 val_loss2.301870584487915\n",
            "iteration 790: train_loss:2.30171275138855 val_loss2.3018696308135986\n",
            "iteration 791: train_loss:2.3017115592956543 val_loss2.301868200302124\n",
            "iteration 792: train_loss:2.301710367202759 val_loss2.3018672466278076\n",
            "iteration 793: train_loss:2.301708936691284 val_loss2.301866054534912\n",
            "iteration 794: train_loss:2.3017075061798096 val_loss2.3018651008605957\n",
            "iteration 795: train_loss:2.301706314086914 val_loss2.301863670349121\n",
            "iteration 796: train_loss:2.3017051219940186 val_loss2.3018624782562256\n",
            "iteration 797: train_loss:2.301703691482544 val_loss2.30186128616333\n",
            "iteration 798: train_loss:2.3017024993896484 val_loss2.3018600940704346\n",
            "iteration 799: train_loss:2.301701068878174 val_loss2.301859140396118\n",
            "iteration 800: train_loss:2.3016998767852783 val_loss2.3018579483032227\n",
            "iteration 801: train_loss:2.3016984462738037 val_loss2.301856756210327\n",
            "iteration 802: train_loss:2.3016974925994873 val_loss2.3018555641174316\n",
            "iteration 803: train_loss:2.3016960620880127 val_loss2.3018546104431152\n",
            "iteration 804: train_loss:2.301694869995117 val_loss2.3018534183502197\n",
            "iteration 805: train_loss:2.3016934394836426 val_loss2.301852226257324\n",
            "iteration 806: train_loss:2.301692247390747 val_loss2.3018507957458496\n",
            "iteration 807: train_loss:2.3016908168792725 val_loss2.3018500804901123\n",
            "iteration 808: train_loss:2.301689624786377 val_loss2.3018486499786377\n",
            "iteration 809: train_loss:2.3016881942749023 val_loss2.301847457885742\n",
            "iteration 810: train_loss:2.301687002182007 val_loss2.301846504211426\n",
            "iteration 811: train_loss:2.3016855716705322 val_loss2.301845073699951\n",
            "iteration 812: train_loss:2.3016843795776367 val_loss2.3018441200256348\n",
            "iteration 813: train_loss:2.301682949066162 val_loss2.3018429279327393\n",
            "iteration 814: train_loss:2.3016819953918457 val_loss2.3018414974212646\n",
            "iteration 815: train_loss:2.301680564880371 val_loss2.3018405437469482\n",
            "iteration 816: train_loss:2.3016791343688965 val_loss2.3018393516540527\n",
            "iteration 817: train_loss:2.301677703857422 val_loss2.301837921142578\n",
            "iteration 818: train_loss:2.3016765117645264 val_loss2.3018369674682617\n",
            "iteration 819: train_loss:2.3016750812530518 val_loss2.301835775375366\n",
            "iteration 820: train_loss:2.3016738891601562 val_loss2.3018343448638916\n",
            "iteration 821: train_loss:2.3016724586486816 val_loss2.301833391189575\n",
            "iteration 822: train_loss:2.301671266555786 val_loss2.3018319606781006\n",
            "iteration 823: train_loss:2.3016698360443115 val_loss2.301831007003784\n",
            "iteration 824: train_loss:2.301668643951416 val_loss2.3018298149108887\n",
            "iteration 825: train_loss:2.3016672134399414 val_loss2.301828384399414\n",
            "iteration 826: train_loss:2.301666021347046 val_loss2.3018274307250977\n",
            "iteration 827: train_loss:2.3016645908355713 val_loss2.301826238632202\n",
            "iteration 828: train_loss:2.301663398742676 val_loss2.3018250465393066\n",
            "iteration 829: train_loss:2.301661968231201 val_loss2.301823854446411\n",
            "iteration 830: train_loss:2.3016607761383057 val_loss2.3018226623535156\n",
            "iteration 831: train_loss:2.30165958404541 val_loss2.30182147026062\n",
            "iteration 832: train_loss:2.3016581535339355 val_loss2.3018200397491455\n",
            "iteration 833: train_loss:2.301656723022461 val_loss2.30181884765625\n",
            "iteration 834: train_loss:2.3016552925109863 val_loss2.3018178939819336\n",
            "iteration 835: train_loss:2.3016538619995117 val_loss2.301816701889038\n",
            "iteration 836: train_loss:2.301652669906616 val_loss2.3018152713775635\n",
            "iteration 837: train_loss:2.3016514778137207 val_loss2.301814079284668\n",
            "iteration 838: train_loss:2.301650047302246 val_loss2.3018128871917725\n",
            "iteration 839: train_loss:2.3016486167907715 val_loss2.301811695098877\n",
            "iteration 840: train_loss:2.301647424697876 val_loss2.3018105030059814\n",
            "iteration 841: train_loss:2.3016459941864014 val_loss2.301809310913086\n",
            "iteration 842: train_loss:2.3016445636749268 val_loss2.3018078804016113\n",
            "iteration 843: train_loss:2.301643133163452 val_loss2.301806926727295\n",
            "iteration 844: train_loss:2.3016419410705566 val_loss2.3018057346343994\n",
            "iteration 845: train_loss:2.301640510559082 val_loss2.301804304122925\n",
            "iteration 846: train_loss:2.3016393184661865 val_loss2.3018031120300293\n",
            "iteration 847: train_loss:2.301637887954712 val_loss2.301801919937134\n",
            "iteration 848: train_loss:2.3016364574432373 val_loss2.3018007278442383\n",
            "iteration 849: train_loss:2.3016350269317627 val_loss2.3017995357513428\n",
            "iteration 850: train_loss:2.301633834838867 val_loss2.3017983436584473\n",
            "iteration 851: train_loss:2.3016324043273926 val_loss2.3017969131469727\n",
            "iteration 852: train_loss:2.301631212234497 val_loss2.301795482635498\n",
            "iteration 853: train_loss:2.3016297817230225 val_loss2.3017945289611816\n",
            "iteration 854: train_loss:2.301628351211548 val_loss2.301793098449707\n",
            "iteration 855: train_loss:2.3016271591186523 val_loss2.3017921447753906\n",
            "iteration 856: train_loss:2.3016257286071777 val_loss2.301790714263916\n",
            "iteration 857: train_loss:2.301624059677124 val_loss2.3017892837524414\n",
            "iteration 858: train_loss:2.3016228675842285 val_loss2.301788330078125\n",
            "iteration 859: train_loss:2.301621437072754 val_loss2.3017868995666504\n",
            "iteration 860: train_loss:2.3016200065612793 val_loss2.301785469055176\n",
            "iteration 861: train_loss:2.301618814468384 val_loss2.3017845153808594\n",
            "iteration 862: train_loss:2.301617383956909 val_loss2.3017830848693848\n",
            "iteration 863: train_loss:2.3016159534454346 val_loss2.3017818927764893\n",
            "iteration 864: train_loss:2.30161452293396 val_loss2.3017804622650146\n",
            "iteration 865: train_loss:2.3016130924224854 val_loss2.301779270172119\n",
            "iteration 866: train_loss:2.3016116619110107 val_loss2.3017780780792236\n",
            "iteration 867: train_loss:2.3016104698181152 val_loss2.301776647567749\n",
            "iteration 868: train_loss:2.3016090393066406 val_loss2.3017756938934326\n",
            "iteration 869: train_loss:2.301607608795166 val_loss2.301774263381958\n",
            "iteration 870: train_loss:2.3016059398651123 val_loss2.3017728328704834\n",
            "iteration 871: train_loss:2.3016045093536377 val_loss2.301771640777588\n",
            "iteration 872: train_loss:2.301603317260742 val_loss2.3017704486846924\n",
            "iteration 873: train_loss:2.3016018867492676 val_loss2.3017690181732178\n",
            "iteration 874: train_loss:2.301600456237793 val_loss2.3017678260803223\n",
            "iteration 875: train_loss:2.3015992641448975 val_loss2.3017663955688477\n",
            "iteration 876: train_loss:2.301597833633423 val_loss2.301765203475952\n",
            "iteration 877: train_loss:2.3015964031219482 val_loss2.3017640113830566\n",
            "iteration 878: train_loss:2.3015949726104736 val_loss2.301762580871582\n",
            "iteration 879: train_loss:2.301593780517578 val_loss2.3017613887786865\n",
            "iteration 880: train_loss:2.3015923500061035 val_loss2.301760196685791\n",
            "iteration 881: train_loss:2.30159068107605 val_loss2.3017590045928955\n",
            "iteration 882: train_loss:2.3015894889831543 val_loss2.301757574081421\n",
            "iteration 883: train_loss:2.3015878200531006 val_loss2.301755905151367\n",
            "iteration 884: train_loss:2.301586389541626 val_loss2.301754951477051\n",
            "iteration 885: train_loss:2.3015851974487305 val_loss2.301753520965576\n",
            "iteration 886: train_loss:2.3015835285186768 val_loss2.3017523288726807\n",
            "iteration 887: train_loss:2.301582098007202 val_loss2.301750898361206\n",
            "iteration 888: train_loss:2.3015806674957275 val_loss2.3017494678497314\n",
            "iteration 889: train_loss:2.301579236984253 val_loss2.301748275756836\n",
            "iteration 890: train_loss:2.301577568054199 val_loss2.3017468452453613\n",
            "iteration 891: train_loss:2.3015763759613037 val_loss2.301745891571045\n",
            "iteration 892: train_loss:2.301574945449829 val_loss2.301744222640991\n",
            "iteration 893: train_loss:2.3015735149383545 val_loss2.3017430305480957\n",
            "iteration 894: train_loss:2.30157208442688 val_loss2.301741600036621\n",
            "iteration 895: train_loss:2.3015706539154053 val_loss2.3017404079437256\n",
            "iteration 896: train_loss:2.3015692234039307 val_loss2.301738977432251\n",
            "iteration 897: train_loss:2.301567792892456 val_loss2.3017375469207764\n",
            "iteration 898: train_loss:2.3015663623809814 val_loss2.301736354827881\n",
            "iteration 899: train_loss:2.301564931869507 val_loss2.3017351627349854\n",
            "iteration 900: train_loss:2.3015635013580322 val_loss2.3017337322235107\n",
            "iteration 901: train_loss:2.3015618324279785 val_loss2.301732301712036\n",
            "iteration 902: train_loss:2.301560640335083 val_loss2.3017311096191406\n",
            "iteration 903: train_loss:2.3015589714050293 val_loss2.301729679107666\n",
            "iteration 904: train_loss:2.301557779312134 val_loss2.3017284870147705\n",
            "iteration 905: train_loss:2.30155611038208 val_loss2.301727056503296\n",
            "iteration 906: train_loss:2.3015546798706055 val_loss2.301725387573242\n",
            "iteration 907: train_loss:2.3015530109405518 val_loss2.3017241954803467\n",
            "iteration 908: train_loss:2.3015518188476562 val_loss2.301722764968872\n",
            "iteration 909: train_loss:2.3015501499176025 val_loss2.3017215728759766\n",
            "iteration 910: train_loss:2.301548957824707 val_loss2.301720142364502\n",
            "iteration 911: train_loss:2.3015472888946533 val_loss2.3017187118530273\n",
            "iteration 912: train_loss:2.3015456199645996 val_loss2.301717519760132\n",
            "iteration 913: train_loss:2.301544189453125 val_loss2.3017160892486572\n",
            "iteration 914: train_loss:2.3015429973602295 val_loss2.3017148971557617\n",
            "iteration 915: train_loss:2.301541328430176 val_loss2.301713466644287\n",
            "iteration 916: train_loss:2.301539659500122 val_loss2.3017120361328125\n",
            "iteration 917: train_loss:2.3015382289886475 val_loss2.301710605621338\n",
            "iteration 918: train_loss:2.301536798477173 val_loss2.3017094135284424\n",
            "iteration 919: train_loss:2.301535129547119 val_loss2.3017077445983887\n",
            "iteration 920: train_loss:2.3015336990356445 val_loss2.301706314086914\n",
            "iteration 921: train_loss:2.30153226852417 val_loss2.3017048835754395\n",
            "iteration 922: train_loss:2.301530599594116 val_loss2.301703691482544\n",
            "iteration 923: train_loss:2.3015291690826416 val_loss2.3017022609710693\n",
            "iteration 924: train_loss:2.301527500152588 val_loss2.301701068878174\n",
            "iteration 925: train_loss:2.3015260696411133 val_loss2.30169939994812\n",
            "iteration 926: train_loss:2.3015246391296387 val_loss2.3016979694366455\n",
            "iteration 927: train_loss:2.301523208618164 val_loss2.30169677734375\n",
            "iteration 928: train_loss:2.3015217781066895 val_loss2.3016953468322754\n",
            "iteration 929: train_loss:2.301520347595215 val_loss2.301693916320801\n",
            "iteration 930: train_loss:2.301518678665161 val_loss2.301692485809326\n",
            "iteration 931: train_loss:2.3015170097351074 val_loss2.3016910552978516\n",
            "iteration 932: train_loss:2.301515579223633 val_loss2.301689863204956\n",
            "iteration 933: train_loss:2.301514148712158 val_loss2.3016881942749023\n",
            "iteration 934: train_loss:2.3015124797821045 val_loss2.3016867637634277\n",
            "iteration 935: train_loss:2.30151104927063 val_loss2.301685333251953\n",
            "iteration 936: train_loss:2.3015096187591553 val_loss2.3016841411590576\n",
            "iteration 937: train_loss:2.3015079498291016 val_loss2.301682472229004\n",
            "iteration 938: train_loss:2.301506519317627 val_loss2.3016810417175293\n",
            "iteration 939: train_loss:2.3015050888061523 val_loss2.3016796112060547\n",
            "iteration 940: train_loss:2.3015034198760986 val_loss2.301678419113159\n",
            "iteration 941: train_loss:2.301501750946045 val_loss2.3016769886016846\n",
            "iteration 942: train_loss:2.3015003204345703 val_loss2.301675319671631\n",
            "iteration 943: train_loss:2.3014986515045166 val_loss2.3016738891601562\n",
            "iteration 944: train_loss:2.301496982574463 val_loss2.3016724586486816\n",
            "iteration 945: train_loss:2.3014955520629883 val_loss2.301671266555786\n",
            "iteration 946: train_loss:2.3014938831329346 val_loss2.3016695976257324\n",
            "iteration 947: train_loss:2.30149245262146 val_loss2.301668167114258\n",
            "iteration 948: train_loss:2.3014907836914062 val_loss2.301666498184204\n",
            "iteration 949: train_loss:2.3014893531799316 val_loss2.3016653060913086\n",
            "iteration 950: train_loss:2.301487922668457 val_loss2.301663875579834\n",
            "iteration 951: train_loss:2.301486015319824 val_loss2.3016624450683594\n",
            "iteration 952: train_loss:2.3014845848083496 val_loss2.3016607761383057\n",
            "iteration 953: train_loss:2.301483154296875 val_loss2.301659345626831\n",
            "iteration 954: train_loss:2.3014814853668213 val_loss2.3016579151153564\n",
            "iteration 955: train_loss:2.3014798164367676 val_loss2.301656484603882\n",
            "iteration 956: train_loss:2.301478147506714 val_loss2.3016550540924072\n",
            "iteration 957: train_loss:2.30147647857666 val_loss2.3016536235809326\n",
            "iteration 958: train_loss:2.3014748096466064 val_loss2.301651954650879\n",
            "iteration 959: train_loss:2.301473617553711 val_loss2.3016505241394043\n",
            "iteration 960: train_loss:2.301471710205078 val_loss2.3016490936279297\n",
            "iteration 961: train_loss:2.3014702796936035 val_loss2.301647663116455\n",
            "iteration 962: train_loss:2.30146861076355 val_loss2.3016462326049805\n",
            "iteration 963: train_loss:2.301467180252075 val_loss2.3016445636749268\n",
            "iteration 964: train_loss:2.3014655113220215 val_loss2.301643133163452\n",
            "iteration 965: train_loss:2.3014638423919678 val_loss2.3016417026519775\n",
            "iteration 966: train_loss:2.301462173461914 val_loss2.301640033721924\n",
            "iteration 967: train_loss:2.3014607429504395 val_loss2.301638603210449\n",
            "iteration 968: train_loss:2.3014588356018066 val_loss2.3016371726989746\n",
            "iteration 969: train_loss:2.301457405090332 val_loss2.301635503768921\n",
            "iteration 970: train_loss:2.3014559745788574 val_loss2.301633834838867\n",
            "iteration 971: train_loss:2.3014543056488037 val_loss2.3016326427459717\n",
            "iteration 972: train_loss:2.30145263671875 val_loss2.301630973815918\n",
            "iteration 973: train_loss:2.3014509677886963 val_loss2.3016297817230225\n",
            "iteration 974: train_loss:2.3014492988586426 val_loss2.3016281127929688\n",
            "iteration 975: train_loss:2.301447629928589 val_loss2.301626443862915\n",
            "iteration 976: train_loss:2.301445960998535 val_loss2.3016250133514404\n",
            "iteration 977: train_loss:2.3014442920684814 val_loss2.301623582839966\n",
            "iteration 978: train_loss:2.301442861557007 val_loss2.301622152328491\n",
            "iteration 979: train_loss:2.301441192626953 val_loss2.3016202449798584\n",
            "iteration 980: train_loss:2.3014392852783203 val_loss2.301618814468384\n",
            "iteration 981: train_loss:2.3014378547668457 val_loss2.301617383956909\n",
            "iteration 982: train_loss:2.301436424255371 val_loss2.3016157150268555\n",
            "iteration 983: train_loss:2.3014347553253174 val_loss2.301614284515381\n",
            "iteration 984: train_loss:2.3014330863952637 val_loss2.301612615585327\n",
            "iteration 985: train_loss:2.30143141746521 val_loss2.3016111850738525\n",
            "iteration 986: train_loss:2.301429510116577 val_loss2.301609516143799\n",
            "iteration 987: train_loss:2.3014280796051025 val_loss2.301607847213745\n",
            "iteration 988: train_loss:2.3014261722564697 val_loss2.3016064167022705\n",
            "iteration 989: train_loss:2.301424741744995 val_loss2.301604986190796\n",
            "iteration 990: train_loss:2.3014228343963623 val_loss2.301603317260742\n",
            "iteration 991: train_loss:2.3014211654663086 val_loss2.3016016483306885\n",
            "iteration 992: train_loss:2.301419496536255 val_loss2.3015999794006348\n",
            "iteration 993: train_loss:2.301417827606201 val_loss2.30159854888916\n",
            "iteration 994: train_loss:2.3014163970947266 val_loss2.3015971183776855\n",
            "iteration 995: train_loss:2.3014144897460938 val_loss2.301595449447632\n",
            "iteration 996: train_loss:2.301413059234619 val_loss2.301593780517578\n",
            "iteration 997: train_loss:2.3014111518859863 val_loss2.3015925884246826\n",
            "iteration 998: train_loss:2.3014094829559326 val_loss2.30159068107605\n",
            "iteration 999: train_loss:2.301407814025879 val_loss2.301589012145996\n",
            "iteration 1000: train_loss:2.301406145095825 val_loss2.3015875816345215\n",
            "iteration 1001: train_loss:2.3014044761657715 val_loss2.3015859127044678\n",
            "iteration 1002: train_loss:2.3014028072357178 val_loss2.301584243774414\n",
            "iteration 1003: train_loss:2.301400899887085 val_loss2.3015828132629395\n",
            "iteration 1004: train_loss:2.3013992309570312 val_loss2.3015811443328857\n",
            "iteration 1005: train_loss:2.3013975620269775 val_loss2.301579713821411\n",
            "iteration 1006: train_loss:2.301395893096924 val_loss2.3015778064727783\n",
            "iteration 1007: train_loss:2.301393985748291 val_loss2.3015761375427246\n",
            "iteration 1008: train_loss:2.3013923168182373 val_loss2.30157470703125\n",
            "iteration 1009: train_loss:2.3013908863067627 val_loss2.3015730381011963\n",
            "iteration 1010: train_loss:2.30138897895813 val_loss2.3015713691711426\n",
            "iteration 1011: train_loss:2.301387310028076 val_loss2.301569700241089\n",
            "iteration 1012: train_loss:2.3013854026794434 val_loss2.301568031311035\n",
            "iteration 1013: train_loss:2.3013837337493896 val_loss2.3015663623809814\n",
            "iteration 1014: train_loss:2.301382064819336 val_loss2.301564931869507\n",
            "iteration 1015: train_loss:2.301380157470703 val_loss2.301563024520874\n",
            "iteration 1016: train_loss:2.3013784885406494 val_loss2.3015615940093994\n",
            "iteration 1017: train_loss:2.3013768196105957 val_loss2.3015599250793457\n",
            "iteration 1018: train_loss:2.301375150680542 val_loss2.301558256149292\n",
            "iteration 1019: train_loss:2.301373243331909 val_loss2.3015568256378174\n",
            "iteration 1020: train_loss:2.3013715744018555 val_loss2.3015549182891846\n",
            "iteration 1021: train_loss:2.3013699054718018 val_loss2.301553249359131\n",
            "iteration 1022: train_loss:2.301368236541748 val_loss2.3015518188476562\n",
            "iteration 1023: train_loss:2.3013663291931152 val_loss2.3015501499176025\n",
            "iteration 1024: train_loss:2.3013646602630615 val_loss2.301548480987549\n",
            "iteration 1025: train_loss:2.3013627529144287 val_loss2.301546573638916\n",
            "iteration 1026: train_loss:2.301361083984375 val_loss2.3015451431274414\n",
            "iteration 1027: train_loss:2.3013594150543213 val_loss2.3015434741973877\n",
            "iteration 1028: train_loss:2.3013575077056885 val_loss2.301541805267334\n",
            "iteration 1029: train_loss:2.3013558387756348 val_loss2.301539897918701\n",
            "iteration 1030: train_loss:2.301354169845581 val_loss2.3015382289886475\n",
            "iteration 1031: train_loss:2.3013522624969482 val_loss2.301536798477173\n",
            "iteration 1032: train_loss:2.3013503551483154 val_loss2.30153489112854\n",
            "iteration 1033: train_loss:2.3013486862182617 val_loss2.3015332221984863\n",
            "iteration 1034: train_loss:2.301347017288208 val_loss2.3015315532684326\n",
            "iteration 1035: train_loss:2.301345109939575 val_loss2.301529884338379\n",
            "iteration 1036: train_loss:2.3013434410095215 val_loss2.301528215408325\n",
            "iteration 1037: train_loss:2.3013415336608887 val_loss2.3015263080596924\n",
            "iteration 1038: train_loss:2.301339626312256 val_loss2.3015246391296387\n",
            "iteration 1039: train_loss:2.301337957382202 val_loss2.301522970199585\n",
            "iteration 1040: train_loss:2.3013360500335693 val_loss2.3015213012695312\n",
            "iteration 1041: train_loss:2.3013343811035156 val_loss2.3015196323394775\n",
            "iteration 1042: train_loss:2.301332473754883 val_loss2.3015177249908447\n",
            "iteration 1043: train_loss:2.30133056640625 val_loss2.301516056060791\n",
            "iteration 1044: train_loss:2.3013288974761963 val_loss2.3015143871307373\n",
            "iteration 1045: train_loss:2.3013272285461426 val_loss2.3015127182006836\n",
            "iteration 1046: train_loss:2.3013253211975098 val_loss2.301510810852051\n",
            "iteration 1047: train_loss:2.301323413848877 val_loss2.301509380340576\n",
            "iteration 1048: train_loss:2.301321506500244 val_loss2.3015077114105225\n",
            "iteration 1049: train_loss:2.3013198375701904 val_loss2.3015055656433105\n",
            "iteration 1050: train_loss:2.3013179302215576 val_loss2.301504135131836\n",
            "iteration 1051: train_loss:2.301316022872925 val_loss2.301501989364624\n",
            "iteration 1052: train_loss:2.301314115524292 val_loss2.3015005588531494\n",
            "iteration 1053: train_loss:2.301312208175659 val_loss2.3014988899230957\n",
            "iteration 1054: train_loss:2.3013105392456055 val_loss2.301496982574463\n",
            "iteration 1055: train_loss:2.3013088703155518 val_loss2.301495313644409\n",
            "iteration 1056: train_loss:2.30130672454834 val_loss2.3014934062957764\n",
            "iteration 1057: train_loss:2.301305055618286 val_loss2.3014917373657227\n",
            "iteration 1058: train_loss:2.3013031482696533 val_loss2.3014895915985107\n",
            "iteration 1059: train_loss:2.3013012409210205 val_loss2.301488161087036\n",
            "iteration 1060: train_loss:2.301299571990967 val_loss2.3014862537384033\n",
            "iteration 1061: train_loss:2.301297426223755 val_loss2.3014845848083496\n",
            "iteration 1062: train_loss:2.301295518875122 val_loss2.301482915878296\n",
            "iteration 1063: train_loss:2.3012936115264893 val_loss2.301480770111084\n",
            "iteration 1064: train_loss:2.3012919425964355 val_loss2.3014791011810303\n",
            "iteration 1065: train_loss:2.3012897968292236 val_loss2.3014771938323975\n",
            "iteration 1066: train_loss:2.30128812789917 val_loss2.3014755249023438\n",
            "iteration 1067: train_loss:2.301286220550537 val_loss2.301473617553711\n",
            "iteration 1068: train_loss:2.3012843132019043 val_loss2.3014719486236572\n",
            "iteration 1069: train_loss:2.3012824058532715 val_loss2.3014700412750244\n",
            "iteration 1070: train_loss:2.3012804985046387 val_loss2.3014681339263916\n",
            "iteration 1071: train_loss:2.301278591156006 val_loss2.301466464996338\n",
            "iteration 1072: train_loss:2.301276683807373 val_loss2.301464557647705\n",
            "iteration 1073: train_loss:2.301274538040161 val_loss2.3014626502990723\n",
            "iteration 1074: train_loss:2.3012726306915283 val_loss2.3014609813690186\n",
            "iteration 1075: train_loss:2.3012707233428955 val_loss2.3014590740203857\n",
            "iteration 1076: train_loss:2.3012688159942627 val_loss2.301457166671753\n",
            "iteration 1077: train_loss:2.301267147064209 val_loss2.30145525932312\n",
            "iteration 1078: train_loss:2.301265001296997 val_loss2.3014533519744873\n",
            "iteration 1079: train_loss:2.3012630939483643 val_loss2.3014516830444336\n",
            "iteration 1080: train_loss:2.3012611865997314 val_loss2.30145001411438\n",
            "iteration 1081: train_loss:2.3012592792510986 val_loss2.301447868347168\n",
            "iteration 1082: train_loss:2.3012571334838867 val_loss2.3014461994171143\n",
            "iteration 1083: train_loss:2.301255226135254 val_loss2.3014442920684814\n",
            "iteration 1084: train_loss:2.301253318786621 val_loss2.3014423847198486\n",
            "iteration 1085: train_loss:2.3012514114379883 val_loss2.301440715789795\n",
            "iteration 1086: train_loss:2.3012492656707764 val_loss2.301438808441162\n",
            "iteration 1087: train_loss:2.3012473583221436 val_loss2.3014369010925293\n",
            "iteration 1088: train_loss:2.3012454509735107 val_loss2.3014349937438965\n",
            "iteration 1089: train_loss:2.301243305206299 val_loss2.3014328479766846\n",
            "iteration 1090: train_loss:2.301241636276245 val_loss2.3014309406280518\n",
            "iteration 1091: train_loss:2.301239490509033 val_loss2.301429033279419\n",
            "iteration 1092: train_loss:2.3012375831604004 val_loss2.3014273643493652\n",
            "iteration 1093: train_loss:2.3012356758117676 val_loss2.3014254570007324\n",
            "iteration 1094: train_loss:2.3012332916259766 val_loss2.3014235496520996\n",
            "iteration 1095: train_loss:2.301231622695923 val_loss2.3014214038848877\n",
            "iteration 1096: train_loss:2.301229476928711 val_loss2.301419496536255\n",
            "iteration 1097: train_loss:2.301227569580078 val_loss2.301417827606201\n",
            "iteration 1098: train_loss:2.301225423812866 val_loss2.3014156818389893\n",
            "iteration 1099: train_loss:2.3012232780456543 val_loss2.3014140129089355\n",
            "iteration 1100: train_loss:2.3012213706970215 val_loss2.3014118671417236\n",
            "iteration 1101: train_loss:2.3012194633483887 val_loss2.301409959793091\n",
            "iteration 1102: train_loss:2.3012173175811768 val_loss2.301408052444458\n",
            "iteration 1103: train_loss:2.301215410232544 val_loss2.301405906677246\n",
            "iteration 1104: train_loss:2.301213264465332 val_loss2.3014042377471924\n",
            "iteration 1105: train_loss:2.301211357116699 val_loss2.3014020919799805\n",
            "iteration 1106: train_loss:2.3012092113494873 val_loss2.3014004230499268\n",
            "iteration 1107: train_loss:2.3012073040008545 val_loss2.301398277282715\n",
            "iteration 1108: train_loss:2.3012051582336426 val_loss2.301396369934082\n",
            "iteration 1109: train_loss:2.3012032508850098 val_loss2.301394462585449\n",
            "iteration 1110: train_loss:2.301201105117798 val_loss2.3013923168182373\n",
            "iteration 1111: train_loss:2.301198959350586 val_loss2.3013904094696045\n",
            "iteration 1112: train_loss:2.301197052001953 val_loss2.3013885021209717\n",
            "iteration 1113: train_loss:2.301194906234741 val_loss2.301386594772339\n",
            "iteration 1114: train_loss:2.3011927604675293 val_loss2.301384449005127\n",
            "iteration 1115: train_loss:2.3011908531188965 val_loss2.301382541656494\n",
            "iteration 1116: train_loss:2.3011887073516846 val_loss2.3013803958892822\n",
            "iteration 1117: train_loss:2.3011865615844727 val_loss2.3013784885406494\n",
            "iteration 1118: train_loss:2.30118465423584 val_loss2.3013763427734375\n",
            "iteration 1119: train_loss:2.301182270050049 val_loss2.3013744354248047\n",
            "iteration 1120: train_loss:2.301180362701416 val_loss2.301372528076172\n",
            "iteration 1121: train_loss:2.301177978515625 val_loss2.30137038230896\n",
            "iteration 1122: train_loss:2.301176071166992 val_loss2.301368474960327\n",
            "iteration 1123: train_loss:2.3011741638183594 val_loss2.3013663291931152\n",
            "iteration 1124: train_loss:2.3011717796325684 val_loss2.3013644218444824\n",
            "iteration 1125: train_loss:2.3011698722839355 val_loss2.3013622760772705\n",
            "iteration 1126: train_loss:2.3011677265167236 val_loss2.3013603687286377\n",
            "iteration 1127: train_loss:2.3011655807495117 val_loss2.301358461380005\n",
            "iteration 1128: train_loss:2.3011634349823 val_loss2.301356077194214\n",
            "iteration 1129: train_loss:2.301161050796509 val_loss2.301354169845581\n",
            "iteration 1130: train_loss:2.301159143447876 val_loss2.301352024078369\n",
            "iteration 1131: train_loss:2.301156997680664 val_loss2.3013498783111572\n",
            "iteration 1132: train_loss:2.301154851913452 val_loss2.3013477325439453\n",
            "iteration 1133: train_loss:2.301152467727661 val_loss2.3013458251953125\n",
            "iteration 1134: train_loss:2.301150321960449 val_loss2.3013436794281006\n",
            "iteration 1135: train_loss:2.3011481761932373 val_loss2.3013417720794678\n",
            "iteration 1136: train_loss:2.3011460304260254 val_loss2.301339626312256\n",
            "iteration 1137: train_loss:2.3011441230773926 val_loss2.301337480545044\n",
            "iteration 1138: train_loss:2.3011417388916016 val_loss2.301335334777832\n",
            "iteration 1139: train_loss:2.3011398315429688 val_loss2.301333427429199\n",
            "iteration 1140: train_loss:2.3011374473571777 val_loss2.3013312816619873\n",
            "iteration 1141: train_loss:2.301135301589966 val_loss2.3013291358947754\n",
            "iteration 1142: train_loss:2.301132917404175 val_loss2.3013272285461426\n",
            "iteration 1143: train_loss:2.301131010055542 val_loss2.3013248443603516\n",
            "iteration 1144: train_loss:2.301128625869751 val_loss2.3013226985931396\n",
            "iteration 1145: train_loss:2.301126480102539 val_loss2.3013205528259277\n",
            "iteration 1146: train_loss:2.301124334335327 val_loss2.301318645477295\n",
            "iteration 1147: train_loss:2.3011221885681152 val_loss2.301316261291504\n",
            "iteration 1148: train_loss:2.301119804382324 val_loss2.301314353942871\n",
            "iteration 1149: train_loss:2.301117420196533 val_loss2.301312208175659\n",
            "iteration 1150: train_loss:2.3011155128479004 val_loss2.301309823989868\n",
            "iteration 1151: train_loss:2.3011131286621094 val_loss2.3013079166412354\n",
            "iteration 1152: train_loss:2.3011109828948975 val_loss2.3013055324554443\n",
            "iteration 1153: train_loss:2.3011088371276855 val_loss2.3013033866882324\n",
            "iteration 1154: train_loss:2.3011064529418945 val_loss2.3013012409210205\n",
            "iteration 1155: train_loss:2.3011043071746826 val_loss2.3012990951538086\n",
            "iteration 1156: train_loss:2.3011021614074707 val_loss2.301297187805176\n",
            "iteration 1157: train_loss:2.3010997772216797 val_loss2.3012948036193848\n",
            "iteration 1158: train_loss:2.3010973930358887 val_loss2.301292657852173\n",
            "iteration 1159: train_loss:2.3010952472686768 val_loss2.301290512084961\n",
            "iteration 1160: train_loss:2.3010928630828857 val_loss2.301288366317749\n",
            "iteration 1161: train_loss:2.3010904788970947 val_loss2.301285982131958\n",
            "iteration 1162: train_loss:2.301088333129883 val_loss2.301283836364746\n",
            "iteration 1163: train_loss:2.301086187362671 val_loss2.301281690597534\n",
            "iteration 1164: train_loss:2.301083564758301 val_loss2.301279306411743\n",
            "iteration 1165: train_loss:2.301081418991089 val_loss2.3012773990631104\n",
            "iteration 1166: train_loss:2.301079273223877 val_loss2.3012750148773193\n",
            "iteration 1167: train_loss:2.301076889038086 val_loss2.3012728691101074\n",
            "iteration 1168: train_loss:2.301074504852295 val_loss2.3012704849243164\n",
            "iteration 1169: train_loss:2.301072359085083 val_loss2.3012683391571045\n",
            "iteration 1170: train_loss:2.301069974899292 val_loss2.3012661933898926\n",
            "iteration 1171: train_loss:2.30106782913208 val_loss2.3012638092041016\n",
            "iteration 1172: train_loss:2.301065444946289 val_loss2.3012614250183105\n",
            "iteration 1173: train_loss:2.301062822341919 val_loss2.3012592792510986\n",
            "iteration 1174: train_loss:2.301060914993286 val_loss2.3012571334838867\n",
            "iteration 1175: train_loss:2.301058292388916 val_loss2.3012547492980957\n",
            "iteration 1176: train_loss:2.301056146621704 val_loss2.301252603530884\n",
            "iteration 1177: train_loss:2.301053762435913 val_loss2.3012502193450928\n",
            "iteration 1178: train_loss:2.301051378250122 val_loss2.3012478351593018\n",
            "iteration 1179: train_loss:2.301048994064331 val_loss2.30124568939209\n",
            "iteration 1180: train_loss:2.30104660987854 val_loss2.301243305206299\n",
            "iteration 1181: train_loss:2.301044225692749 val_loss2.301241159439087\n",
            "iteration 1182: train_loss:2.301042079925537 val_loss2.301238775253296\n",
            "iteration 1183: train_loss:2.301039457321167 val_loss2.301236391067505\n",
            "iteration 1184: train_loss:2.301037073135376 val_loss2.301234245300293\n",
            "iteration 1185: train_loss:2.301034688949585 val_loss2.301231861114502\n",
            "iteration 1186: train_loss:2.301032543182373 val_loss2.301229476928711\n",
            "iteration 1187: train_loss:2.301029920578003 val_loss2.30122709274292\n",
            "iteration 1188: train_loss:2.301027536392212 val_loss2.301224946975708\n",
            "iteration 1189: train_loss:2.301025152206421 val_loss2.301222562789917\n",
            "iteration 1190: train_loss:2.301022529602051 val_loss2.301220178604126\n",
            "iteration 1191: train_loss:2.301020383834839 val_loss2.301218032836914\n",
            "iteration 1192: train_loss:2.301017999649048 val_loss2.301215410232544\n",
            "iteration 1193: train_loss:2.301015615463257 val_loss2.301213264465332\n",
            "iteration 1194: train_loss:2.301013231277466 val_loss2.301210641860962\n",
            "iteration 1195: train_loss:2.3010106086730957 val_loss2.301208734512329\n",
            "iteration 1196: train_loss:2.3010079860687256 val_loss2.301206111907959\n",
            "iteration 1197: train_loss:2.3010056018829346 val_loss2.301203727722168\n",
            "iteration 1198: train_loss:2.3010032176971436 val_loss2.301201105117798\n",
            "iteration 1199: train_loss:2.3010010719299316 val_loss2.301198959350586\n",
            "iteration 1200: train_loss:2.3009984493255615 val_loss2.301196575164795\n",
            "iteration 1201: train_loss:2.3009958267211914 val_loss2.301194190979004\n",
            "iteration 1202: train_loss:2.3009932041168213 val_loss2.301191568374634\n",
            "iteration 1203: train_loss:2.3009908199310303 val_loss2.301189422607422\n",
            "iteration 1204: train_loss:2.30098819732666 val_loss2.3011868000030518\n",
            "iteration 1205: train_loss:2.3009860515594482 val_loss2.3011844158172607\n",
            "iteration 1206: train_loss:2.300983428955078 val_loss2.3011817932128906\n",
            "iteration 1207: train_loss:2.300981044769287 val_loss2.3011796474456787\n",
            "iteration 1208: train_loss:2.300978422164917 val_loss2.3011772632598877\n",
            "iteration 1209: train_loss:2.300976037979126 val_loss2.3011746406555176\n",
            "iteration 1210: train_loss:2.300973415374756 val_loss2.3011722564697266\n",
            "iteration 1211: train_loss:2.3009707927703857 val_loss2.3011698722839355\n",
            "iteration 1212: train_loss:2.3009681701660156 val_loss2.3011672496795654\n",
            "iteration 1213: train_loss:2.3009657859802246 val_loss2.3011648654937744\n",
            "iteration 1214: train_loss:2.3009631633758545 val_loss2.3011624813079834\n",
            "iteration 1215: train_loss:2.3009605407714844 val_loss2.3011598587036133\n",
            "iteration 1216: train_loss:2.3009581565856934 val_loss2.3011574745178223\n",
            "iteration 1217: train_loss:2.3009557723999023 val_loss2.3011550903320312\n",
            "iteration 1218: train_loss:2.300952911376953 val_loss2.301152467727661\n",
            "iteration 1219: train_loss:2.300950527191162 val_loss2.30115008354187\n",
            "iteration 1220: train_loss:2.300947904586792 val_loss2.3011474609375\n",
            "iteration 1221: train_loss:2.300945281982422 val_loss2.301145076751709\n",
            "iteration 1222: train_loss:2.3009426593780518 val_loss2.301142454147339\n",
            "iteration 1223: train_loss:2.3009402751922607 val_loss2.3011398315429688\n",
            "iteration 1224: train_loss:2.3009374141693115 val_loss2.3011374473571777\n",
            "iteration 1225: train_loss:2.3009350299835205 val_loss2.3011348247528076\n",
            "iteration 1226: train_loss:2.3009321689605713 val_loss2.3011322021484375\n",
            "iteration 1227: train_loss:2.3009297847747803 val_loss2.3011298179626465\n",
            "iteration 1228: train_loss:2.30092716217041 val_loss2.3011269569396973\n",
            "iteration 1229: train_loss:2.30092453956604 val_loss2.3011248111724854\n",
            "iteration 1230: train_loss:2.30092191696167 val_loss2.3011221885681152\n",
            "iteration 1231: train_loss:2.3009192943573 val_loss2.301119565963745\n",
            "iteration 1232: train_loss:2.3009166717529297 val_loss2.301117181777954\n",
            "iteration 1233: train_loss:2.3009140491485596 val_loss2.301114320755005\n",
            "iteration 1234: train_loss:2.3009114265441895 val_loss2.301111936569214\n",
            "iteration 1235: train_loss:2.3009088039398193 val_loss2.3011090755462646\n",
            "iteration 1236: train_loss:2.300905704498291 val_loss2.3011066913604736\n",
            "iteration 1237: train_loss:2.3009033203125 val_loss2.3011040687561035\n",
            "iteration 1238: train_loss:2.300900459289551 val_loss2.3011014461517334\n",
            "iteration 1239: train_loss:2.3008978366851807 val_loss2.3010990619659424\n",
            "iteration 1240: train_loss:2.3008954524993896 val_loss2.301096200942993\n",
            "iteration 1241: train_loss:2.3008925914764404 val_loss2.301093578338623\n",
            "iteration 1242: train_loss:2.3008899688720703 val_loss2.301090955734253\n",
            "iteration 1243: train_loss:2.3008873462677 val_loss2.301088333129883\n",
            "iteration 1244: train_loss:2.300884485244751 val_loss2.3010857105255127\n",
            "iteration 1245: train_loss:2.300881862640381 val_loss2.3010830879211426\n",
            "iteration 1246: train_loss:2.3008790016174316 val_loss2.3010804653167725\n",
            "iteration 1247: train_loss:2.3008763790130615 val_loss2.3010778427124023\n",
            "iteration 1248: train_loss:2.3008737564086914 val_loss2.3010752201080322\n",
            "iteration 1249: train_loss:2.300870656967163 val_loss2.301072120666504\n",
            "iteration 1250: train_loss:2.300868034362793 val_loss2.301069498062134\n",
            "iteration 1251: train_loss:2.300865411758423 val_loss2.3010671138763428\n",
            "iteration 1252: train_loss:2.3008627891540527 val_loss2.3010642528533936\n",
            "iteration 1253: train_loss:2.3008599281311035 val_loss2.3010618686676025\n",
            "iteration 1254: train_loss:2.3008570671081543 val_loss2.301058769226074\n",
            "iteration 1255: train_loss:2.3008546829223633 val_loss2.301056146621704\n",
            "iteration 1256: train_loss:2.300851821899414 val_loss2.301053524017334\n",
            "iteration 1257: train_loss:2.300848960876465 val_loss2.3010506629943848\n",
            "iteration 1258: train_loss:2.3008460998535156 val_loss2.3010480403900146\n",
            "iteration 1259: train_loss:2.3008432388305664 val_loss2.3010451793670654\n",
            "iteration 1260: train_loss:2.3008406162261963 val_loss2.3010427951812744\n",
            "iteration 1261: train_loss:2.300837755203247 val_loss2.301039934158325\n",
            "iteration 1262: train_loss:2.300835132598877 val_loss2.301037073135376\n",
            "iteration 1263: train_loss:2.3008322715759277 val_loss2.301034450531006\n",
            "iteration 1264: train_loss:2.3008291721343994 val_loss2.3010313510894775\n",
            "iteration 1265: train_loss:2.3008265495300293 val_loss2.3010287284851074\n",
            "iteration 1266: train_loss:2.30082368850708 val_loss2.301025867462158\n",
            "iteration 1267: train_loss:2.3008205890655518 val_loss2.301023006439209\n",
            "iteration 1268: train_loss:2.3008179664611816 val_loss2.301020383834839\n",
            "iteration 1269: train_loss:2.3008151054382324 val_loss2.3010172843933105\n",
            "iteration 1270: train_loss:2.300812244415283 val_loss2.3010146617889404\n",
            "iteration 1271: train_loss:2.300809383392334 val_loss2.3010120391845703\n",
            "iteration 1272: train_loss:2.3008065223693848 val_loss2.301008939743042\n",
            "iteration 1273: train_loss:2.3008036613464355 val_loss2.301006317138672\n",
            "iteration 1274: train_loss:2.3008008003234863 val_loss2.3010034561157227\n",
            "iteration 1275: train_loss:2.300797700881958 val_loss2.3010008335113525\n",
            "iteration 1276: train_loss:2.300795078277588 val_loss2.300997734069824\n",
            "iteration 1277: train_loss:2.3007919788360596 val_loss2.300994873046875\n",
            "iteration 1278: train_loss:2.3007888793945312 val_loss2.300992012023926\n",
            "iteration 1279: train_loss:2.300786018371582 val_loss2.3009889125823975\n",
            "iteration 1280: train_loss:2.300783157348633 val_loss2.3009862899780273\n",
            "iteration 1281: train_loss:2.3007800579071045 val_loss2.300983428955078\n",
            "iteration 1282: train_loss:2.3007774353027344 val_loss2.300980567932129\n",
            "iteration 1283: train_loss:2.300774335861206 val_loss2.3009777069091797\n",
            "iteration 1284: train_loss:2.300771474838257 val_loss2.3009746074676514\n",
            "iteration 1285: train_loss:2.3007683753967285 val_loss2.300971746444702\n",
            "iteration 1286: train_loss:2.3007652759552 val_loss2.300968885421753\n",
            "iteration 1287: train_loss:2.300762414932251 val_loss2.3009660243988037\n",
            "iteration 1288: train_loss:2.3007595539093018 val_loss2.3009629249572754\n",
            "iteration 1289: train_loss:2.3007564544677734 val_loss2.300960063934326\n",
            "iteration 1290: train_loss:2.300753593444824 val_loss2.300957202911377\n",
            "iteration 1291: train_loss:2.300750255584717 val_loss2.3009541034698486\n",
            "iteration 1292: train_loss:2.3007473945617676 val_loss2.3009512424468994\n",
            "iteration 1293: train_loss:2.3007445335388184 val_loss2.300948143005371\n",
            "iteration 1294: train_loss:2.300741195678711 val_loss2.3009450435638428\n",
            "iteration 1295: train_loss:2.3007380962371826 val_loss2.3009424209594727\n",
            "iteration 1296: train_loss:2.3007354736328125 val_loss2.3009390830993652\n",
            "iteration 1297: train_loss:2.300732374191284 val_loss2.300936222076416\n",
            "iteration 1298: train_loss:2.300729274749756 val_loss2.300933361053467\n",
            "iteration 1299: train_loss:2.3007261753082275 val_loss2.3009302616119385\n",
            "iteration 1300: train_loss:2.300723075866699 val_loss2.30092716217041\n",
            "iteration 1301: train_loss:2.300719976425171 val_loss2.300924301147461\n",
            "iteration 1302: train_loss:2.3007168769836426 val_loss2.3009212017059326\n",
            "iteration 1303: train_loss:2.3007137775421143 val_loss2.3009181022644043\n",
            "iteration 1304: train_loss:2.300710678100586 val_loss2.300915002822876\n",
            "iteration 1305: train_loss:2.3007075786590576 val_loss2.3009121417999268\n",
            "iteration 1306: train_loss:2.3007044792175293 val_loss2.3009088039398193\n",
            "iteration 1307: train_loss:2.30070161819458 val_loss2.30090594291687\n",
            "iteration 1308: train_loss:2.3006982803344727 val_loss2.300902843475342\n",
            "iteration 1309: train_loss:2.3006951808929443 val_loss2.3008997440338135\n",
            "iteration 1310: train_loss:2.300692081451416 val_loss2.300896644592285\n",
            "iteration 1311: train_loss:2.3006887435913086 val_loss2.300893545150757\n",
            "iteration 1312: train_loss:2.300685405731201 val_loss2.3008906841278076\n",
            "iteration 1313: train_loss:2.300682544708252 val_loss2.3008873462677\n",
            "iteration 1314: train_loss:2.3006792068481445 val_loss2.300884246826172\n",
            "iteration 1315: train_loss:2.300676107406616 val_loss2.3008809089660645\n",
            "iteration 1316: train_loss:2.300673007965088 val_loss2.3008780479431152\n",
            "iteration 1317: train_loss:2.3006699085235596 val_loss2.300874710083008\n",
            "iteration 1318: train_loss:2.300666570663452 val_loss2.3008716106414795\n",
            "iteration 1319: train_loss:2.300663471221924 val_loss2.300868272781372\n",
            "iteration 1320: train_loss:2.3006598949432373 val_loss2.3008651733398438\n",
            "iteration 1321: train_loss:2.300657033920288 val_loss2.3008620738983154\n",
            "iteration 1322: train_loss:2.3006536960601807 val_loss2.300858974456787\n",
            "iteration 1323: train_loss:2.3006505966186523 val_loss2.300855875015259\n",
            "iteration 1324: train_loss:2.300647258758545 val_loss2.3008527755737305\n",
            "iteration 1325: train_loss:2.3006439208984375 val_loss2.300849437713623\n",
            "iteration 1326: train_loss:2.30064058303833 val_loss2.3008460998535156\n",
            "iteration 1327: train_loss:2.3006374835968018 val_loss2.300842761993408\n",
            "iteration 1328: train_loss:2.3006341457366943 val_loss2.30083966255188\n",
            "iteration 1329: train_loss:2.300630807876587 val_loss2.3008365631103516\n",
            "iteration 1330: train_loss:2.3006277084350586 val_loss2.300833225250244\n",
            "iteration 1331: train_loss:2.300624370574951 val_loss2.3008296489715576\n",
            "iteration 1332: train_loss:2.3006210327148438 val_loss2.3008267879486084\n",
            "iteration 1333: train_loss:2.3006176948547363 val_loss2.300823211669922\n",
            "iteration 1334: train_loss:2.300614356994629 val_loss2.3008201122283936\n",
            "iteration 1335: train_loss:2.3006110191345215 val_loss2.300816774368286\n",
            "iteration 1336: train_loss:2.300607681274414 val_loss2.3008134365081787\n",
            "iteration 1337: train_loss:2.3006041049957275 val_loss2.3008103370666504\n",
            "iteration 1338: train_loss:2.30060076713562 val_loss2.300806760787964\n",
            "iteration 1339: train_loss:2.300597667694092 val_loss2.3008036613464355\n",
            "iteration 1340: train_loss:2.3005943298339844 val_loss2.300800085067749\n",
            "iteration 1341: train_loss:2.300590991973877 val_loss2.3007969856262207\n",
            "iteration 1342: train_loss:2.3005876541137695 val_loss2.300793409347534\n",
            "iteration 1343: train_loss:2.300584077835083 val_loss2.300790309906006\n",
            "iteration 1344: train_loss:2.3005807399749756 val_loss2.3007867336273193\n",
            "iteration 1345: train_loss:2.300577163696289 val_loss2.300783395767212\n",
            "iteration 1346: train_loss:2.3005738258361816 val_loss2.3007800579071045\n",
            "iteration 1347: train_loss:2.300570487976074 val_loss2.300776720046997\n",
            "iteration 1348: train_loss:2.3005669116973877 val_loss2.3007731437683105\n",
            "iteration 1349: train_loss:2.300563335418701 val_loss2.300769805908203\n",
            "iteration 1350: train_loss:2.300560235977173 val_loss2.3007664680480957\n",
            "iteration 1351: train_loss:2.3005568981170654 val_loss2.3007631301879883\n",
            "iteration 1352: train_loss:2.3005530834198 val_loss2.3007595539093018\n",
            "iteration 1353: train_loss:2.3005495071411133 val_loss2.3007562160491943\n",
            "iteration 1354: train_loss:2.300546169281006 val_loss2.300752639770508\n",
            "iteration 1355: train_loss:2.3005425930023193 val_loss2.3007490634918213\n",
            "iteration 1356: train_loss:2.300539255142212 val_loss2.300745725631714\n",
            "iteration 1357: train_loss:2.3005356788635254 val_loss2.3007423877716064\n",
            "iteration 1358: train_loss:2.300532102584839 val_loss2.300738573074341\n",
            "iteration 1359: train_loss:2.3005287647247314 val_loss2.3007354736328125\n",
            "iteration 1360: train_loss:2.300524950027466 val_loss2.300731897354126\n",
            "iteration 1361: train_loss:2.3005216121673584 val_loss2.3007283210754395\n",
            "iteration 1362: train_loss:2.300518035888672 val_loss2.300724744796753\n",
            "iteration 1363: train_loss:2.3005144596099854 val_loss2.3007211685180664\n",
            "iteration 1364: train_loss:2.300510883331299 val_loss2.300717830657959\n",
            "iteration 1365: train_loss:2.300507068634033 val_loss2.3007142543792725\n",
            "iteration 1366: train_loss:2.3005034923553467 val_loss2.300710678100586\n",
            "iteration 1367: train_loss:2.30049991607666 val_loss2.3007071018218994\n",
            "iteration 1368: train_loss:2.3004963397979736 val_loss2.300703287124634\n",
            "iteration 1369: train_loss:2.300492763519287 val_loss2.3006999492645264\n",
            "iteration 1370: train_loss:2.3004891872406006 val_loss2.30069637298584\n",
            "iteration 1371: train_loss:2.300485610961914 val_loss2.3006927967071533\n",
            "iteration 1372: train_loss:2.3004820346832275 val_loss2.300689220428467\n",
            "iteration 1373: train_loss:2.300478219985962 val_loss2.3006856441497803\n",
            "iteration 1374: train_loss:2.3004746437072754 val_loss2.3006818294525146\n",
            "iteration 1375: train_loss:2.3004708290100098 val_loss2.300678253173828\n",
            "iteration 1376: train_loss:2.3004672527313232 val_loss2.3006746768951416\n",
            "iteration 1377: train_loss:2.3004634380340576 val_loss2.300670862197876\n",
            "iteration 1378: train_loss:2.300459861755371 val_loss2.3006672859191895\n",
            "iteration 1379: train_loss:2.3004560470581055 val_loss2.300663709640503\n",
            "iteration 1380: train_loss:2.300452470779419 val_loss2.3006598949432373\n",
            "iteration 1381: train_loss:2.3004486560821533 val_loss2.300656318664551\n",
            "iteration 1382: train_loss:2.300445079803467 val_loss2.300652503967285\n",
            "iteration 1383: train_loss:2.300441265106201 val_loss2.3006489276885986\n",
            "iteration 1384: train_loss:2.3004374504089355 val_loss2.300645351409912\n",
            "iteration 1385: train_loss:2.30043363571167 val_loss2.3006412982940674\n",
            "iteration 1386: train_loss:2.3004298210144043 val_loss2.3006374835968018\n",
            "iteration 1387: train_loss:2.3004260063171387 val_loss2.3006339073181152\n",
            "iteration 1388: train_loss:2.300422430038452 val_loss2.3006303310394287\n",
            "iteration 1389: train_loss:2.3004183769226074 val_loss2.300626277923584\n",
            "iteration 1390: train_loss:2.300414800643921 val_loss2.3006227016448975\n",
            "iteration 1391: train_loss:2.3004109859466553 val_loss2.300618886947632\n",
            "iteration 1392: train_loss:2.3004071712493896 val_loss2.300615072250366\n",
            "iteration 1393: train_loss:2.300403118133545 val_loss2.3006112575531006\n",
            "iteration 1394: train_loss:2.3003993034362793 val_loss2.300607442855835\n",
            "iteration 1395: train_loss:2.3003954887390137 val_loss2.3006036281585693\n",
            "iteration 1396: train_loss:2.300391674041748 val_loss2.3005998134613037\n",
            "iteration 1397: train_loss:2.3003876209259033 val_loss2.300595760345459\n",
            "iteration 1398: train_loss:2.3003838062286377 val_loss2.3005921840667725\n",
            "iteration 1399: train_loss:2.300379991531372 val_loss2.3005881309509277\n",
            "iteration 1400: train_loss:2.3003761768341064 val_loss2.300584316253662\n",
            "iteration 1401: train_loss:2.3003718852996826 val_loss2.3005802631378174\n",
            "iteration 1402: train_loss:2.300368309020996 val_loss2.3005764484405518\n",
            "iteration 1403: train_loss:2.3003642559051514 val_loss2.300572633743286\n",
            "iteration 1404: train_loss:2.3003604412078857 val_loss2.3005685806274414\n",
            "iteration 1405: train_loss:2.30035662651062 val_loss2.300564765930176\n",
            "iteration 1406: train_loss:2.3003523349761963 val_loss2.300560712814331\n",
            "iteration 1407: train_loss:2.3003482818603516 val_loss2.3005568981170654\n",
            "iteration 1408: train_loss:2.300344467163086 val_loss2.3005528450012207\n",
            "iteration 1409: train_loss:2.300340414047241 val_loss2.300549030303955\n",
            "iteration 1410: train_loss:2.3003363609313965 val_loss2.3005449771881104\n",
            "iteration 1411: train_loss:2.3003323078155518 val_loss2.3005409240722656\n",
            "iteration 1412: train_loss:2.300328493118286 val_loss2.300536870956421\n",
            "iteration 1413: train_loss:2.3003244400024414 val_loss2.300532817840576\n",
            "iteration 1414: train_loss:2.3003203868865967 val_loss2.3005290031433105\n",
            "iteration 1415: train_loss:2.300316095352173 val_loss2.300524950027466\n",
            "iteration 1416: train_loss:2.3003122806549072 val_loss2.300520658493042\n",
            "iteration 1417: train_loss:2.3003079891204834 val_loss2.3005166053771973\n",
            "iteration 1418: train_loss:2.3003039360046387 val_loss2.3005125522613525\n",
            "iteration 1419: train_loss:2.300300121307373 val_loss2.300508499145508\n",
            "iteration 1420: train_loss:2.300295829772949 val_loss2.300504446029663\n",
            "iteration 1421: train_loss:2.3002917766571045 val_loss2.3005003929138184\n",
            "iteration 1422: train_loss:2.3002874851226807 val_loss2.3004963397979736\n",
            "iteration 1423: train_loss:2.300283432006836 val_loss2.300492286682129\n",
            "iteration 1424: train_loss:2.300279378890991 val_loss2.300487995147705\n",
            "iteration 1425: train_loss:2.3002750873565674 val_loss2.3004837036132812\n",
            "iteration 1426: train_loss:2.3002707958221436 val_loss2.3004796504974365\n",
            "iteration 1427: train_loss:2.300266742706299 val_loss2.300475597381592\n",
            "iteration 1428: train_loss:2.300262451171875 val_loss2.300471305847168\n",
            "iteration 1429: train_loss:2.3002583980560303 val_loss2.300467014312744\n",
            "iteration 1430: train_loss:2.3002541065216064 val_loss2.3004629611968994\n",
            "iteration 1431: train_loss:2.3002498149871826 val_loss2.3004586696624756\n",
            "iteration 1432: train_loss:2.300245523452759 val_loss2.3004543781280518\n",
            "iteration 1433: train_loss:2.300241470336914 val_loss2.300450325012207\n",
            "iteration 1434: train_loss:2.3002371788024902 val_loss2.300446033477783\n",
            "iteration 1435: train_loss:2.3002326488494873 val_loss2.3004417419433594\n",
            "iteration 1436: train_loss:2.3002285957336426 val_loss2.3004372119903564\n",
            "iteration 1437: train_loss:2.3002243041992188 val_loss2.300433397293091\n",
            "iteration 1438: train_loss:2.300220012664795 val_loss2.300428867340088\n",
            "iteration 1439: train_loss:2.300215482711792 val_loss2.300424575805664\n",
            "iteration 1440: train_loss:2.3002114295959473 val_loss2.3004202842712402\n",
            "iteration 1441: train_loss:2.3002066612243652 val_loss2.3004157543182373\n",
            "iteration 1442: train_loss:2.3002023696899414 val_loss2.3004117012023926\n",
            "iteration 1443: train_loss:2.3001980781555176 val_loss2.3004074096679688\n",
            "iteration 1444: train_loss:2.3001937866210938 val_loss2.300402879714966\n",
            "iteration 1445: train_loss:2.30018949508667 val_loss2.300398588180542\n",
            "iteration 1446: train_loss:2.300184965133667 val_loss2.300394058227539\n",
            "iteration 1447: train_loss:2.300180435180664 val_loss2.3003897666931152\n",
            "iteration 1448: train_loss:2.3001761436462402 val_loss2.3003852367401123\n",
            "iteration 1449: train_loss:2.3001716136932373 val_loss2.3003807067871094\n",
            "iteration 1450: train_loss:2.3001675605773926 val_loss2.3003764152526855\n",
            "iteration 1451: train_loss:2.3001630306243896 val_loss2.3003718852996826\n",
            "iteration 1452: train_loss:2.300158739089966 val_loss2.3003673553466797\n",
            "iteration 1453: train_loss:2.300153970718384 val_loss2.300363063812256\n",
            "iteration 1454: train_loss:2.300149440765381 val_loss2.300358295440674\n",
            "iteration 1455: train_loss:2.300144910812378 val_loss2.30035400390625\n",
            "iteration 1456: train_loss:2.300140380859375 val_loss2.300349235534668\n",
            "iteration 1457: train_loss:2.300135850906372 val_loss2.300344944000244\n",
            "iteration 1458: train_loss:2.300131320953369 val_loss2.300340414047241\n",
            "iteration 1459: train_loss:2.300126552581787 val_loss2.3003358840942383\n",
            "iteration 1460: train_loss:2.3001222610473633 val_loss2.3003313541412354\n",
            "iteration 1461: train_loss:2.3001177310943604 val_loss2.3003268241882324\n",
            "iteration 1462: train_loss:2.3001132011413574 val_loss2.3003220558166504\n",
            "iteration 1463: train_loss:2.3001081943511963 val_loss2.3003172874450684\n",
            "iteration 1464: train_loss:2.3001039028167725 val_loss2.3003129959106445\n",
            "iteration 1465: train_loss:2.3000993728637695 val_loss2.3003082275390625\n",
            "iteration 1466: train_loss:2.3000943660736084 val_loss2.3003034591674805\n",
            "iteration 1467: train_loss:2.3000898361206055 val_loss2.3002989292144775\n",
            "iteration 1468: train_loss:2.3000853061676025 val_loss2.3002943992614746\n",
            "iteration 1469: train_loss:2.3000805377960205 val_loss2.3002896308898926\n",
            "iteration 1470: train_loss:2.3000757694244385 val_loss2.3002848625183105\n",
            "iteration 1471: train_loss:2.3000712394714355 val_loss2.3002800941467285\n",
            "iteration 1472: train_loss:2.3000662326812744 val_loss2.3002755641937256\n",
            "iteration 1473: train_loss:2.3000617027282715 val_loss2.3002707958221436\n",
            "iteration 1474: train_loss:2.3000569343566895 val_loss2.3002657890319824\n",
            "iteration 1475: train_loss:2.3000521659851074 val_loss2.3002612590789795\n",
            "iteration 1476: train_loss:2.3000473976135254 val_loss2.3002564907073975\n",
            "iteration 1477: train_loss:2.3000423908233643 val_loss2.3002517223358154\n",
            "iteration 1478: train_loss:2.3000378608703613 val_loss2.3002469539642334\n",
            "iteration 1479: train_loss:2.3000328540802 val_loss2.3002421855926514\n",
            "iteration 1480: train_loss:2.300028085708618 val_loss2.3002374172210693\n",
            "iteration 1481: train_loss:2.300023317337036 val_loss2.300232410430908\n",
            "iteration 1482: train_loss:2.300018548965454 val_loss2.300227642059326\n",
            "iteration 1483: train_loss:2.300013542175293 val_loss2.300222635269165\n",
            "iteration 1484: train_loss:2.300008773803711 val_loss2.300217628479004\n",
            "iteration 1485: train_loss:2.30000376701355 val_loss2.300212860107422\n",
            "iteration 1486: train_loss:2.2999989986419678 val_loss2.3002078533172607\n",
            "iteration 1487: train_loss:2.2999939918518066 val_loss2.3002030849456787\n",
            "iteration 1488: train_loss:2.2999889850616455 val_loss2.3001980781555176\n",
            "iteration 1489: train_loss:2.2999842166900635 val_loss2.3001930713653564\n",
            "iteration 1490: train_loss:2.2999792098999023 val_loss2.3001880645751953\n",
            "iteration 1491: train_loss:2.299973964691162 val_loss2.300183057785034\n",
            "iteration 1492: train_loss:2.29996919631958 val_loss2.300178289413452\n",
            "iteration 1493: train_loss:2.299964189529419 val_loss2.300173044204712\n",
            "iteration 1494: train_loss:2.299959182739258 val_loss2.300168037414551\n",
            "iteration 1495: train_loss:2.2999541759490967 val_loss2.3001630306243896\n",
            "iteration 1496: train_loss:2.2999491691589355 val_loss2.3001580238342285\n",
            "iteration 1497: train_loss:2.2999439239501953 val_loss2.3001527786254883\n",
            "iteration 1498: train_loss:2.299938917160034 val_loss2.300147771835327\n",
            "iteration 1499: train_loss:2.299934148788452 val_loss2.300142765045166\n",
            "iteration 1500: train_loss:2.299928903579712 val_loss2.300137758255005\n",
            "iteration 1501: train_loss:2.2999236583709717 val_loss2.3001325130462646\n",
            "iteration 1502: train_loss:2.2999186515808105 val_loss2.3001277446746826\n",
            "iteration 1503: train_loss:2.2999134063720703 val_loss2.3001222610473633\n",
            "iteration 1504: train_loss:2.299908399581909 val_loss2.300117254257202\n",
            "iteration 1505: train_loss:2.299903154373169 val_loss2.300112009048462\n",
            "iteration 1506: train_loss:2.2998979091644287 val_loss2.3001067638397217\n",
            "iteration 1507: train_loss:2.2998929023742676 val_loss2.3001017570495605\n",
            "iteration 1508: train_loss:2.2998876571655273 val_loss2.300096273422241\n",
            "iteration 1509: train_loss:2.299882173538208 val_loss2.300091028213501\n",
            "iteration 1510: train_loss:2.299877166748047 val_loss2.3000857830047607\n",
            "iteration 1511: train_loss:2.2998719215393066 val_loss2.3000805377960205\n",
            "iteration 1512: train_loss:2.2998664379119873 val_loss2.3000752925872803\n",
            "iteration 1513: train_loss:2.299861192703247 val_loss2.300069808959961\n",
            "iteration 1514: train_loss:2.299855947494507 val_loss2.3000645637512207\n",
            "iteration 1515: train_loss:2.2998504638671875 val_loss2.3000593185424805\n",
            "iteration 1516: train_loss:2.2998452186584473 val_loss2.300053834915161\n",
            "iteration 1517: train_loss:2.299839973449707 val_loss2.300048589706421\n",
            "iteration 1518: train_loss:2.299834728240967 val_loss2.3000431060791016\n",
            "iteration 1519: train_loss:2.2998294830322266 val_loss2.3000378608703613\n",
            "iteration 1520: train_loss:2.299823760986328 val_loss2.300032377243042\n",
            "iteration 1521: train_loss:2.299818277359009 val_loss2.3000268936157227\n",
            "iteration 1522: train_loss:2.2998130321502686 val_loss2.3000216484069824\n",
            "iteration 1523: train_loss:2.2998077869415283 val_loss2.300015926361084\n",
            "iteration 1524: train_loss:2.299802303314209 val_loss2.3000104427337646\n",
            "iteration 1525: train_loss:2.2997965812683105 val_loss2.3000049591064453\n",
            "iteration 1526: train_loss:2.299791097640991 val_loss2.299999713897705\n",
            "iteration 1527: train_loss:2.299785614013672 val_loss2.2999939918518066\n",
            "iteration 1528: train_loss:2.2997801303863525 val_loss2.299988269805908\n",
            "iteration 1529: train_loss:2.299774646759033 val_loss2.299982786178589\n",
            "iteration 1530: train_loss:2.299769163131714 val_loss2.2999773025512695\n",
            "iteration 1531: train_loss:2.2997634410858154 val_loss2.299971580505371\n",
            "iteration 1532: train_loss:2.299757957458496 val_loss2.2999658584594727\n",
            "iteration 1533: train_loss:2.2997522354125977 val_loss2.2999603748321533\n",
            "iteration 1534: train_loss:2.2997467517852783 val_loss2.299954652786255\n",
            "iteration 1535: train_loss:2.299741268157959 val_loss2.2999491691589355\n",
            "iteration 1536: train_loss:2.2997353076934814 val_loss2.299943447113037\n",
            "iteration 1537: train_loss:2.299729824066162 val_loss2.2999377250671387\n",
            "iteration 1538: train_loss:2.2997241020202637 val_loss2.299931764602661\n",
            "iteration 1539: train_loss:2.2997183799743652 val_loss2.299926280975342\n",
            "iteration 1540: train_loss:2.299712657928467 val_loss2.2999205589294434\n",
            "iteration 1541: train_loss:2.2997069358825684 val_loss2.299914836883545\n",
            "iteration 1542: train_loss:2.29970121383667 val_loss2.2999088764190674\n",
            "iteration 1543: train_loss:2.2996952533721924 val_loss2.299903154373169\n",
            "iteration 1544: train_loss:2.299689531326294 val_loss2.2998971939086914\n",
            "iteration 1545: train_loss:2.2996838092803955 val_loss2.299891233444214\n",
            "iteration 1546: train_loss:2.299678087234497 val_loss2.2998855113983154\n",
            "iteration 1547: train_loss:2.2996718883514404 val_loss2.299879550933838\n",
            "iteration 1548: train_loss:2.299666166305542 val_loss2.2998738288879395\n",
            "iteration 1549: train_loss:2.2996604442596436 val_loss2.299868106842041\n",
            "iteration 1550: train_loss:2.299654722213745 val_loss2.2998619079589844\n",
            "iteration 1551: train_loss:2.2996485233306885 val_loss2.299855947494507\n",
            "iteration 1552: train_loss:2.299642562866211 val_loss2.2998502254486084\n",
            "iteration 1553: train_loss:2.2996366024017334 val_loss2.299844264984131\n",
            "iteration 1554: train_loss:2.299630880355835 val_loss2.299838066101074\n",
            "iteration 1555: train_loss:2.2996249198913574 val_loss2.2998321056365967\n",
            "iteration 1556: train_loss:2.299618721008301 val_loss2.299826145172119\n",
            "iteration 1557: train_loss:2.2996127605438232 val_loss2.2998199462890625\n",
            "iteration 1558: train_loss:2.2996065616607666 val_loss2.299813985824585\n",
            "iteration 1559: train_loss:2.299600601196289 val_loss2.2998077869415283\n",
            "iteration 1560: train_loss:2.2995944023132324 val_loss2.2998015880584717\n",
            "iteration 1561: train_loss:2.299588441848755 val_loss2.299795389175415\n",
            "iteration 1562: train_loss:2.2995822429656982 val_loss2.2997894287109375\n",
            "iteration 1563: train_loss:2.2995762825012207 val_loss2.299783229827881\n",
            "iteration 1564: train_loss:2.299570083618164 val_loss2.299777030944824\n",
            "iteration 1565: train_loss:2.2995638847351074 val_loss2.2997708320617676\n",
            "iteration 1566: train_loss:2.299557685852051 val_loss2.299764633178711\n",
            "iteration 1567: train_loss:2.2995517253875732 val_loss2.299758195877075\n",
            "iteration 1568: train_loss:2.2995455265045166 val_loss2.2997522354125977\n",
            "iteration 1569: train_loss:2.29953932762146 val_loss2.299745798110962\n",
            "iteration 1570: train_loss:2.299532890319824 val_loss2.299739360809326\n",
            "iteration 1571: train_loss:2.2995266914367676 val_loss2.2997331619262695\n",
            "iteration 1572: train_loss:2.299520492553711 val_loss2.299726724624634\n",
            "iteration 1573: train_loss:2.299514055252075 val_loss2.299720525741577\n",
            "iteration 1574: train_loss:2.2995078563690186 val_loss2.2997140884399414\n",
            "iteration 1575: train_loss:2.299501419067383 val_loss2.2997076511383057\n",
            "iteration 1576: train_loss:2.299495220184326 val_loss2.29970121383667\n",
            "iteration 1577: train_loss:2.2994887828826904 val_loss2.299694776535034\n",
            "iteration 1578: train_loss:2.299482583999634 val_loss2.2996883392333984\n",
            "iteration 1579: train_loss:2.299475908279419 val_loss2.2996819019317627\n",
            "iteration 1580: train_loss:2.299469232559204 val_loss2.299675226211548\n",
            "iteration 1581: train_loss:2.2994627952575684 val_loss2.299668788909912\n",
            "iteration 1582: train_loss:2.2994563579559326 val_loss2.2996621131896973\n",
            "iteration 1583: train_loss:2.2994496822357178 val_loss2.2996556758880615\n",
            "iteration 1584: train_loss:2.299443244934082 val_loss2.2996490001678467\n",
            "iteration 1585: train_loss:2.2994370460510254 val_loss2.299642562866211\n",
            "iteration 1586: train_loss:2.2994303703308105 val_loss2.299635887145996\n",
            "iteration 1587: train_loss:2.299423933029175 val_loss2.2996292114257812\n",
            "iteration 1588: train_loss:2.29941725730896 val_loss2.2996225357055664\n",
            "iteration 1589: train_loss:2.299410581588745 val_loss2.2996158599853516\n",
            "iteration 1590: train_loss:2.2994039058685303 val_loss2.2996089458465576\n",
            "iteration 1591: train_loss:2.2993972301483154 val_loss2.299602508544922\n",
            "iteration 1592: train_loss:2.2993905544281006 val_loss2.299595594406128\n",
            "iteration 1593: train_loss:2.2993838787078857 val_loss2.299588680267334\n",
            "iteration 1594: train_loss:2.299377202987671 val_loss2.299582004547119\n",
            "iteration 1595: train_loss:2.299370288848877 val_loss2.299575090408325\n",
            "iteration 1596: train_loss:2.299363613128662 val_loss2.2995684146881104\n",
            "iteration 1597: train_loss:2.2993569374084473 val_loss2.2995612621307373\n",
            "iteration 1598: train_loss:2.2993502616882324 val_loss2.2995545864105225\n",
            "iteration 1599: train_loss:2.2993433475494385 val_loss2.2995476722717285\n",
            "iteration 1600: train_loss:2.2993364334106445 val_loss2.2995407581329346\n",
            "iteration 1601: train_loss:2.2993295192718506 val_loss2.2995338439941406\n",
            "iteration 1602: train_loss:2.2993226051330566 val_loss2.2995266914367676\n",
            "iteration 1603: train_loss:2.2993156909942627 val_loss2.2995197772979736\n",
            "iteration 1604: train_loss:2.2993087768554688 val_loss2.2995126247406006\n",
            "iteration 1605: train_loss:2.299301862716675 val_loss2.2995059490203857\n",
            "iteration 1606: train_loss:2.2992947101593018 val_loss2.2994985580444336\n",
            "iteration 1607: train_loss:2.299287796020508 val_loss2.2994914054870605\n",
            "iteration 1608: train_loss:2.2992806434631348 val_loss2.2994844913482666\n",
            "iteration 1609: train_loss:2.299273729324341 val_loss2.2994773387908936\n",
            "iteration 1610: train_loss:2.2992665767669678 val_loss2.2994701862335205\n",
            "iteration 1611: train_loss:2.2992594242095947 val_loss2.2994627952575684\n",
            "iteration 1612: train_loss:2.299252510070801 val_loss2.2994558811187744\n",
            "iteration 1613: train_loss:2.2992453575134277 val_loss2.2994484901428223\n",
            "iteration 1614: train_loss:2.2992379665374756 val_loss2.299441337585449\n",
            "iteration 1615: train_loss:2.2992310523986816 val_loss2.299433946609497\n",
            "iteration 1616: train_loss:2.2992236614227295 val_loss2.299426555633545\n",
            "iteration 1617: train_loss:2.2992165088653564 val_loss2.299419403076172\n",
            "iteration 1618: train_loss:2.2992091178894043 val_loss2.2994120121002197\n",
            "iteration 1619: train_loss:2.299201726913452 val_loss2.2994046211242676\n",
            "iteration 1620: train_loss:2.299194574356079 val_loss2.2993972301483154\n",
            "iteration 1621: train_loss:2.299187183380127 val_loss2.2993898391723633\n",
            "iteration 1622: train_loss:2.299179792404175 val_loss2.299382448196411\n",
            "iteration 1623: train_loss:2.2991726398468018 val_loss2.299375057220459\n",
            "iteration 1624: train_loss:2.2991650104522705 val_loss2.2993674278259277\n",
            "iteration 1625: train_loss:2.2991578578948975 val_loss2.2993597984313965\n",
            "iteration 1626: train_loss:2.299150228500366 val_loss2.2993521690368652\n",
            "iteration 1627: train_loss:2.299142837524414 val_loss2.299344539642334\n",
            "iteration 1628: train_loss:2.299135446548462 val_loss2.299337148666382\n",
            "iteration 1629: train_loss:2.2991278171539307 val_loss2.2993295192718506\n",
            "iteration 1630: train_loss:2.2991201877593994 val_loss2.2993218898773193\n",
            "iteration 1631: train_loss:2.2991127967834473 val_loss2.299314260482788\n",
            "iteration 1632: train_loss:2.299105167388916 val_loss2.2993063926696777\n",
            "iteration 1633: train_loss:2.2990975379943848 val_loss2.2992990016937256\n",
            "iteration 1634: train_loss:2.2990896701812744 val_loss2.2992911338806152\n",
            "iteration 1635: train_loss:2.2990822792053223 val_loss2.299283266067505\n",
            "iteration 1636: train_loss:2.299074649810791 val_loss2.2992753982543945\n",
            "iteration 1637: train_loss:2.2990667819976807 val_loss2.299267530441284\n",
            "iteration 1638: train_loss:2.2990589141845703 val_loss2.299259662628174\n",
            "iteration 1639: train_loss:2.29905104637146 val_loss2.2992517948150635\n",
            "iteration 1640: train_loss:2.2990434169769287 val_loss2.2992441654205322\n",
            "iteration 1641: train_loss:2.2990357875823975 val_loss2.2992360591888428\n",
            "iteration 1642: train_loss:2.299027919769287 val_loss2.2992279529571533\n",
            "iteration 1643: train_loss:2.2990200519561768 val_loss2.299220085144043\n",
            "iteration 1644: train_loss:2.2990121841430664 val_loss2.2992122173309326\n",
            "iteration 1645: train_loss:2.299004077911377 val_loss2.299204111099243\n",
            "iteration 1646: train_loss:2.2989962100982666 val_loss2.299196243286133\n",
            "iteration 1647: train_loss:2.2989883422851562 val_loss2.2991878986358643\n",
            "iteration 1648: train_loss:2.298980236053467 val_loss2.299179792404175\n",
            "iteration 1649: train_loss:2.2989723682403564 val_loss2.2991714477539062\n",
            "iteration 1650: train_loss:2.298964262008667 val_loss2.299163579940796\n",
            "iteration 1651: train_loss:2.2989561557769775 val_loss2.2991552352905273\n",
            "iteration 1652: train_loss:2.298948049545288 val_loss2.299147129058838\n",
            "iteration 1653: train_loss:2.2989399433135986 val_loss2.2991387844085693\n",
            "iteration 1654: train_loss:2.29893159866333 val_loss2.299130439758301\n",
            "iteration 1655: train_loss:2.2989237308502197 val_loss2.2991220951080322\n",
            "iteration 1656: train_loss:2.298915386199951 val_loss2.2991139888763428\n",
            "iteration 1657: train_loss:2.2989070415496826 val_loss2.299105644226074\n",
            "iteration 1658: train_loss:2.298898935317993 val_loss2.2990972995758057\n",
            "iteration 1659: train_loss:2.2988905906677246 val_loss2.299088716506958\n",
            "iteration 1660: train_loss:2.298882246017456 val_loss2.2990803718566895\n",
            "iteration 1661: train_loss:2.2988736629486084 val_loss2.299071788787842\n",
            "iteration 1662: train_loss:2.298865556716919 val_loss2.299063205718994\n",
            "iteration 1663: train_loss:2.2988572120666504 val_loss2.2990548610687256\n",
            "iteration 1664: train_loss:2.2988486289978027 val_loss2.299046039581299\n",
            "iteration 1665: train_loss:2.298840284347534 val_loss2.299037456512451\n",
            "iteration 1666: train_loss:2.2988319396972656 val_loss2.2990291118621826\n",
            "iteration 1667: train_loss:2.298823356628418 val_loss2.299020290374756\n",
            "iteration 1668: train_loss:2.2988147735595703 val_loss2.299011707305908\n",
            "iteration 1669: train_loss:2.2988061904907227 val_loss2.2990028858184814\n",
            "iteration 1670: train_loss:2.298797607421875 val_loss2.2989940643310547\n",
            "iteration 1671: train_loss:2.2987890243530273 val_loss2.298985242843628\n",
            "iteration 1672: train_loss:2.2987804412841797 val_loss2.298976421356201\n",
            "iteration 1673: train_loss:2.298771619796753 val_loss2.2989675998687744\n",
            "iteration 1674: train_loss:2.298762798309326 val_loss2.2989587783813477\n",
            "iteration 1675: train_loss:2.2987542152404785 val_loss2.298949956893921\n",
            "iteration 1676: train_loss:2.2987453937530518 val_loss2.298940896987915\n",
            "iteration 1677: train_loss:2.298736572265625 val_loss2.298931837081909\n",
            "iteration 1678: train_loss:2.2987277507781982 val_loss2.2989230155944824\n",
            "iteration 1679: train_loss:2.2987189292907715 val_loss2.2989139556884766\n",
            "iteration 1680: train_loss:2.2987098693847656 val_loss2.2989048957824707\n",
            "iteration 1681: train_loss:2.298701047897339 val_loss2.298895835876465\n",
            "iteration 1682: train_loss:2.298692226409912 val_loss2.298886775970459\n",
            "iteration 1683: train_loss:2.2986831665039062 val_loss2.298877477645874\n",
            "iteration 1684: train_loss:2.2986743450164795 val_loss2.298868417739868\n",
            "iteration 1685: train_loss:2.2986652851104736 val_loss2.2988593578338623\n",
            "iteration 1686: train_loss:2.2986557483673096 val_loss2.2988498210906982\n",
            "iteration 1687: train_loss:2.2986466884613037 val_loss2.2988407611846924\n",
            "iteration 1688: train_loss:2.298637628555298 val_loss2.2988312244415283\n",
            "iteration 1689: train_loss:2.298628568649292 val_loss2.2988219261169434\n",
            "iteration 1690: train_loss:2.298619508743286 val_loss2.2988126277923584\n",
            "iteration 1691: train_loss:2.298610210418701 val_loss2.2988030910491943\n",
            "iteration 1692: train_loss:2.298600912094116 val_loss2.2987937927246094\n",
            "iteration 1693: train_loss:2.2985916137695312 val_loss2.298784017562866\n",
            "iteration 1694: train_loss:2.298582077026367 val_loss2.298774480819702\n",
            "iteration 1695: train_loss:2.2985730171203613 val_loss2.298765182495117\n",
            "iteration 1696: train_loss:2.2985634803771973 val_loss2.298755407333374\n",
            "iteration 1697: train_loss:2.2985541820526123 val_loss2.298746109008789\n",
            "iteration 1698: train_loss:2.2985446453094482 val_loss2.298736333847046\n",
            "iteration 1699: train_loss:2.298535108566284 val_loss2.2987265586853027\n",
            "iteration 1700: train_loss:2.29852557182312 val_loss2.2987170219421387\n",
            "iteration 1701: train_loss:2.298516035079956 val_loss2.2987072467803955\n",
            "iteration 1702: train_loss:2.298506498336792 val_loss2.2986974716186523\n",
            "iteration 1703: train_loss:2.298496723175049 val_loss2.29868745803833\n",
            "iteration 1704: train_loss:2.2984871864318848 val_loss2.298677444458008\n",
            "iteration 1705: train_loss:2.2984774112701416 val_loss2.2986676692962646\n",
            "iteration 1706: train_loss:2.2984678745269775 val_loss2.2986578941345215\n",
            "iteration 1707: train_loss:2.2984580993652344 val_loss2.298647880554199\n",
            "iteration 1708: train_loss:2.298448324203491 val_loss2.298638105392456\n",
            "iteration 1709: train_loss:2.298438310623169 val_loss2.2986278533935547\n",
            "iteration 1710: train_loss:2.298428535461426 val_loss2.2986178398132324\n",
            "iteration 1711: train_loss:2.2984187602996826 val_loss2.29860782623291\n",
            "iteration 1712: train_loss:2.2984087467193604 val_loss2.298597574234009\n",
            "iteration 1713: train_loss:2.298398733139038 val_loss2.2985873222351074\n",
            "iteration 1714: train_loss:2.298388719558716 val_loss2.298577070236206\n",
            "iteration 1715: train_loss:2.2983787059783936 val_loss2.298567056655884\n",
            "iteration 1716: train_loss:2.2983686923980713 val_loss2.2985568046569824\n",
            "iteration 1717: train_loss:2.29835844039917 val_loss2.298546314239502\n",
            "iteration 1718: train_loss:2.2983484268188477 val_loss2.2985358238220215\n",
            "iteration 1719: train_loss:2.2983381748199463 val_loss2.298525810241699\n",
            "iteration 1720: train_loss:2.298328161239624 val_loss2.2985153198242188\n",
            "iteration 1721: train_loss:2.2983176708221436 val_loss2.298504590988159\n",
            "iteration 1722: train_loss:2.298307418823242 val_loss2.298494338989258\n",
            "iteration 1723: train_loss:2.2982969284057617 val_loss2.2984836101531982\n",
            "iteration 1724: train_loss:2.2982866764068604 val_loss2.2984731197357178\n",
            "iteration 1725: train_loss:2.29827618598938 val_loss2.298462390899658\n",
            "iteration 1726: train_loss:2.2982659339904785 val_loss2.2984516620635986\n",
            "iteration 1727: train_loss:2.298255443572998 val_loss2.298441171646118\n",
            "iteration 1728: train_loss:2.2982449531555176 val_loss2.2984304428100586\n",
            "iteration 1729: train_loss:2.298234224319458 val_loss2.29841947555542\n",
            "iteration 1730: train_loss:2.2982237339019775 val_loss2.2984089851379395\n",
            "iteration 1731: train_loss:2.298213005065918 val_loss2.2983977794647217\n",
            "iteration 1732: train_loss:2.2982025146484375 val_loss2.298387050628662\n",
            "iteration 1733: train_loss:2.298191785812378 val_loss2.2983760833740234\n",
            "iteration 1734: train_loss:2.2981812953948975 val_loss2.2983651161193848\n",
            "iteration 1735: train_loss:2.298170328140259 val_loss2.298354148864746\n",
            "iteration 1736: train_loss:2.298159599304199 val_loss2.2983429431915283\n",
            "iteration 1737: train_loss:2.2981486320495605 val_loss2.2983317375183105\n",
            "iteration 1738: train_loss:2.298137664794922 val_loss2.298320770263672\n",
            "iteration 1739: train_loss:2.2981269359588623 val_loss2.298309564590454\n",
            "iteration 1740: train_loss:2.2981159687042236 val_loss2.2982981204986572\n",
            "iteration 1741: train_loss:2.298105001449585 val_loss2.2982871532440186\n",
            "iteration 1742: train_loss:2.298093795776367 val_loss2.2982757091522217\n",
            "iteration 1743: train_loss:2.2980828285217285 val_loss2.298264265060425\n",
            "iteration 1744: train_loss:2.2980716228485107 val_loss2.298252820968628\n",
            "iteration 1745: train_loss:2.298060417175293 val_loss2.298241376876831\n",
            "iteration 1746: train_loss:2.298049211502075 val_loss2.298229932785034\n",
            "iteration 1747: train_loss:2.2980380058288574 val_loss2.2982184886932373\n",
            "iteration 1748: train_loss:2.2980265617370605 val_loss2.2982070446014404\n",
            "iteration 1749: train_loss:2.2980153560638428 val_loss2.2981953620910645\n",
            "iteration 1750: train_loss:2.298003911972046 val_loss2.2981836795806885\n",
            "iteration 1751: train_loss:2.297992706298828 val_loss2.2981719970703125\n",
            "iteration 1752: train_loss:2.297981023788452 val_loss2.2981605529785156\n",
            "iteration 1753: train_loss:2.2979695796966553 val_loss2.2981483936309814\n",
            "iteration 1754: train_loss:2.2979581356048584 val_loss2.2981367111206055\n",
            "iteration 1755: train_loss:2.2979462146759033 val_loss2.2981247901916504\n",
            "iteration 1756: train_loss:2.2979347705841064 val_loss2.2981128692626953\n",
            "iteration 1757: train_loss:2.2979230880737305 val_loss2.298100709915161\n",
            "iteration 1758: train_loss:2.2979114055633545 val_loss2.298088788986206\n",
            "iteration 1759: train_loss:2.2978997230529785 val_loss2.298076868057251\n",
            "iteration 1760: train_loss:2.2978878021240234 val_loss2.2980644702911377\n",
            "iteration 1761: train_loss:2.2978758811950684 val_loss2.2980525493621826\n",
            "iteration 1762: train_loss:2.2978639602661133 val_loss2.2980403900146484\n",
            "iteration 1763: train_loss:2.297852039337158 val_loss2.298027992248535\n",
            "iteration 1764: train_loss:2.297840118408203 val_loss2.298015832901001\n",
            "iteration 1765: train_loss:2.29782772064209 val_loss2.2980034351348877\n",
            "iteration 1766: train_loss:2.2978157997131348 val_loss2.2979910373687744\n",
            "iteration 1767: train_loss:2.2978036403656006 val_loss2.297978639602661\n",
            "iteration 1768: train_loss:2.2977914810180664 val_loss2.297966241836548\n",
            "iteration 1769: train_loss:2.2977793216705322 val_loss2.2979536056518555\n",
            "iteration 1770: train_loss:2.297767162322998 val_loss2.297941207885742\n",
            "iteration 1771: train_loss:2.2977547645568848 val_loss2.2979283332824707\n",
            "iteration 1772: train_loss:2.2977423667907715 val_loss2.2979156970977783\n",
            "iteration 1773: train_loss:2.297729969024658 val_loss2.297903060913086\n",
            "iteration 1774: train_loss:2.297717571258545 val_loss2.2978901863098145\n",
            "iteration 1775: train_loss:2.2977051734924316 val_loss2.297877550125122\n",
            "iteration 1776: train_loss:2.2976925373077393 val_loss2.2978646755218506\n",
            "iteration 1777: train_loss:2.297679901123047 val_loss2.2978515625\n",
            "iteration 1778: train_loss:2.2976675033569336 val_loss2.2978386878967285\n",
            "iteration 1779: train_loss:2.297654628753662 val_loss2.297825574874878\n",
            "iteration 1780: train_loss:2.2976419925689697 val_loss2.2978124618530273\n",
            "iteration 1781: train_loss:2.2976291179656982 val_loss2.297799587249756\n",
            "iteration 1782: train_loss:2.297616481781006 val_loss2.297786235809326\n",
            "iteration 1783: train_loss:2.2976033687591553 val_loss2.2977731227874756\n",
            "iteration 1784: train_loss:2.297590494155884 val_loss2.297759771347046\n",
            "iteration 1785: train_loss:2.297577381134033 val_loss2.297746419906616\n",
            "iteration 1786: train_loss:2.2975645065307617 val_loss2.2977333068847656\n",
            "iteration 1787: train_loss:2.297551393508911 val_loss2.297719717025757\n",
            "iteration 1788: train_loss:2.2975382804870605 val_loss2.297706127166748\n",
            "iteration 1789: train_loss:2.297524929046631 val_loss2.2976927757263184\n",
            "iteration 1790: train_loss:2.2975118160247803 val_loss2.2976791858673096\n",
            "iteration 1791: train_loss:2.2974982261657715 val_loss2.297665596008301\n",
            "iteration 1792: train_loss:2.297485113143921 val_loss2.297652006149292\n",
            "iteration 1793: train_loss:2.297471761703491 val_loss2.297638177871704\n",
            "iteration 1794: train_loss:2.2974584102630615 val_loss2.297624349594116\n",
            "iteration 1795: train_loss:2.2974448204040527 val_loss2.2976105213165283\n",
            "iteration 1796: train_loss:2.297431230545044 val_loss2.2975964546203613\n",
            "iteration 1797: train_loss:2.297417640686035 val_loss2.2975823879241943\n",
            "iteration 1798: train_loss:2.2974040508270264 val_loss2.2975685596466064\n",
            "iteration 1799: train_loss:2.2973904609680176 val_loss2.2975547313690186\n",
            "iteration 1800: train_loss:2.2973766326904297 val_loss2.2975404262542725\n",
            "iteration 1801: train_loss:2.297362804412842 val_loss2.2975263595581055\n",
            "iteration 1802: train_loss:2.297348976135254 val_loss2.2975120544433594\n",
            "iteration 1803: train_loss:2.297334909439087 val_loss2.2974977493286133\n",
            "iteration 1804: train_loss:2.29732084274292 val_loss2.297483205795288\n",
            "iteration 1805: train_loss:2.297306776046753 val_loss2.297468900680542\n",
            "iteration 1806: train_loss:2.297292709350586 val_loss2.297454357147217\n",
            "iteration 1807: train_loss:2.297278642654419 val_loss2.2974400520324707\n",
            "iteration 1808: train_loss:2.297264337539673 val_loss2.2974255084991455\n",
            "iteration 1809: train_loss:2.297250270843506 val_loss2.297410726547241\n",
            "iteration 1810: train_loss:2.2972359657287598 val_loss2.297395944595337\n",
            "iteration 1811: train_loss:2.2972216606140137 val_loss2.2973811626434326\n",
            "iteration 1812: train_loss:2.2972071170806885 val_loss2.2973666191101074\n",
            "iteration 1813: train_loss:2.2971925735473633 val_loss2.297351598739624\n",
            "iteration 1814: train_loss:2.297178030014038 val_loss2.2973365783691406\n",
            "iteration 1815: train_loss:2.297163486480713 val_loss2.2973215579986572\n",
            "iteration 1816: train_loss:2.2971487045288086 val_loss2.297306537628174\n",
            "iteration 1817: train_loss:2.2971341609954834 val_loss2.2972912788391113\n",
            "iteration 1818: train_loss:2.297119617462158 val_loss2.297276258468628\n",
            "iteration 1819: train_loss:2.297104597091675 val_loss2.2972612380981445\n",
            "iteration 1820: train_loss:2.2970895767211914 val_loss2.297245740890503\n",
            "iteration 1821: train_loss:2.297075033187866 val_loss2.2972302436828613\n",
            "iteration 1822: train_loss:2.2970597743988037 val_loss2.297214984893799\n",
            "iteration 1823: train_loss:2.2970447540283203 val_loss2.2971994876861572\n",
            "iteration 1824: train_loss:2.297029733657837 val_loss2.2971837520599365\n",
            "iteration 1825: train_loss:2.2970144748687744 val_loss2.297168254852295\n",
            "iteration 1826: train_loss:2.296999216079712 val_loss2.297152519226074\n",
            "iteration 1827: train_loss:2.2969837188720703 val_loss2.2971365451812744\n",
            "iteration 1828: train_loss:2.296968460083008 val_loss2.297121286392212\n",
            "iteration 1829: train_loss:2.296952962875366 val_loss2.297105073928833\n",
            "iteration 1830: train_loss:2.2969377040863037 val_loss2.2970893383026123\n",
            "iteration 1831: train_loss:2.296921968460083 val_loss2.2970733642578125\n",
            "iteration 1832: train_loss:2.2969062328338623 val_loss2.2970571517944336\n",
            "iteration 1833: train_loss:2.2968904972076416 val_loss2.297041177749634\n",
            "iteration 1834: train_loss:2.296874761581421 val_loss2.297024965286255\n",
            "iteration 1835: train_loss:2.2968590259552 val_loss2.2970082759857178\n",
            "iteration 1836: train_loss:2.2968432903289795 val_loss2.296992063522339\n",
            "iteration 1837: train_loss:2.2968273162841797 val_loss2.29697585105896\n",
            "iteration 1838: train_loss:2.29681134223938 val_loss2.296959400177002\n",
            "iteration 1839: train_loss:2.296794891357422 val_loss2.296942710876465\n",
            "iteration 1840: train_loss:2.296778917312622 val_loss2.296926259994507\n",
            "iteration 1841: train_loss:2.2967629432678223 val_loss2.2969093322753906\n",
            "iteration 1842: train_loss:2.2967464923858643 val_loss2.2968928813934326\n",
            "iteration 1843: train_loss:2.2967302799224854 val_loss2.2968761920928955\n",
            "iteration 1844: train_loss:2.2967138290405273 val_loss2.2968592643737793\n",
            "iteration 1845: train_loss:2.2966973781585693 val_loss2.296842098236084\n",
            "iteration 1846: train_loss:2.2966806888580322 val_loss2.2968249320983887\n",
            "iteration 1847: train_loss:2.296663999557495 val_loss2.2968080043792725\n",
            "iteration 1848: train_loss:2.296647310256958 val_loss2.296790838241577\n",
            "iteration 1849: train_loss:2.296630620956421 val_loss2.296773672103882\n",
            "iteration 1850: train_loss:2.2966136932373047 val_loss2.2967562675476074\n",
            "iteration 1851: train_loss:2.2965972423553467 val_loss2.296738862991333\n",
            "iteration 1852: train_loss:2.2965800762176514 val_loss2.2967214584350586\n",
            "iteration 1853: train_loss:2.296562910079956 val_loss2.296704053878784\n",
            "iteration 1854: train_loss:2.29654598236084 val_loss2.2966864109039307\n",
            "iteration 1855: train_loss:2.2965288162231445 val_loss2.296668529510498\n",
            "iteration 1856: train_loss:2.296511650085449 val_loss2.2966508865356445\n",
            "iteration 1857: train_loss:2.296494245529175 val_loss2.296633005142212\n",
            "iteration 1858: train_loss:2.2964768409729004 val_loss2.2966151237487793\n",
            "iteration 1859: train_loss:2.296459197998047 val_loss2.2965972423553467\n",
            "iteration 1860: train_loss:2.2964417934417725 val_loss2.296579122543335\n",
            "iteration 1861: train_loss:2.296424150466919 val_loss2.296560764312744\n",
            "iteration 1862: train_loss:2.2964065074920654 val_loss2.2965428829193115\n",
            "iteration 1863: train_loss:2.296388864517212 val_loss2.2965245246887207\n",
            "iteration 1864: train_loss:2.2963712215423584 val_loss2.29650616645813\n",
            "iteration 1865: train_loss:2.296353340148926 val_loss2.29648756980896\n",
            "iteration 1866: train_loss:2.296335220336914 val_loss2.296469211578369\n",
            "iteration 1867: train_loss:2.2963171005249023 val_loss2.29645037651062\n",
            "iteration 1868: train_loss:2.2962989807128906 val_loss2.2964320182800293\n",
            "iteration 1869: train_loss:2.296280860900879 val_loss2.296412944793701\n",
            "iteration 1870: train_loss:2.296262502670288 val_loss2.296394109725952\n",
            "iteration 1871: train_loss:2.2962441444396973 val_loss2.296375274658203\n",
            "iteration 1872: train_loss:2.2962255477905273 val_loss2.296356439590454\n",
            "iteration 1873: train_loss:2.2962071895599365 val_loss2.296337127685547\n",
            "iteration 1874: train_loss:2.2961885929107666 val_loss2.2963180541992188\n",
            "iteration 1875: train_loss:2.2961699962615967 val_loss2.2962987422943115\n",
            "iteration 1876: train_loss:2.2961509227752686 val_loss2.2962794303894043\n",
            "iteration 1877: train_loss:2.2961323261260986 val_loss2.296260118484497\n",
            "iteration 1878: train_loss:2.2961132526397705 val_loss2.2962403297424316\n",
            "iteration 1879: train_loss:2.2960944175720215 val_loss2.2962207794189453\n",
            "iteration 1880: train_loss:2.2960753440856934 val_loss2.296201229095459\n",
            "iteration 1881: train_loss:2.296056032180786 val_loss2.2961814403533936\n",
            "iteration 1882: train_loss:2.296036720275879 val_loss2.296161651611328\n",
            "iteration 1883: train_loss:2.2960174083709717 val_loss2.2961418628692627\n",
            "iteration 1884: train_loss:2.2959983348846436 val_loss2.296121597290039\n",
            "iteration 1885: train_loss:2.2959787845611572 val_loss2.2961015701293945\n",
            "iteration 1886: train_loss:2.295958995819092 val_loss2.296081304550171\n",
            "iteration 1887: train_loss:2.2959392070770264 val_loss2.2960610389709473\n",
            "iteration 1888: train_loss:2.29591965675354 val_loss2.2960405349731445\n",
            "iteration 1889: train_loss:2.2958998680114746 val_loss2.296020269393921\n",
            "iteration 1890: train_loss:2.29587984085083 val_loss2.295999526977539\n",
            "iteration 1891: train_loss:2.2958598136901855 val_loss2.2959790229797363\n",
            "iteration 1892: train_loss:2.295839786529541 val_loss2.2959582805633545\n",
            "iteration 1893: train_loss:2.2958195209503174 val_loss2.2959375381469727\n",
            "iteration 1894: train_loss:2.295799493789673 val_loss2.2959165573120117\n",
            "iteration 1895: train_loss:2.29577898979187 val_loss2.295895576477051\n",
            "iteration 1896: train_loss:2.2957582473754883 val_loss2.2958743572235107\n",
            "iteration 1897: train_loss:2.2957382202148438 val_loss2.29585337638855\n",
            "iteration 1898: train_loss:2.295717239379883 val_loss2.2958319187164307\n",
            "iteration 1899: train_loss:2.29569673538208 val_loss2.2958104610443115\n",
            "iteration 1900: train_loss:2.295675754547119 val_loss2.2957890033721924\n",
            "iteration 1901: train_loss:2.295654773712158 val_loss2.2957675457000732\n",
            "iteration 1902: train_loss:2.2956337928771973 val_loss2.295745849609375\n",
            "iteration 1903: train_loss:2.2956128120422363 val_loss2.2957239151000977\n",
            "iteration 1904: train_loss:2.2955915927886963 val_loss2.2957022190093994\n",
            "iteration 1905: train_loss:2.2955703735351562 val_loss2.295680284500122\n",
            "iteration 1906: train_loss:2.295549154281616 val_loss2.2956581115722656\n",
            "iteration 1907: train_loss:2.295527458190918 val_loss2.295635938644409\n",
            "iteration 1908: train_loss:2.295506000518799 val_loss2.2956137657165527\n",
            "iteration 1909: train_loss:2.2954843044281006 val_loss2.295591354370117\n",
            "iteration 1910: train_loss:2.2954626083374023 val_loss2.2955687046051025\n",
            "iteration 1911: train_loss:2.295440673828125 val_loss2.295546293258667\n",
            "iteration 1912: train_loss:2.2954187393188477 val_loss2.2955236434936523\n",
            "iteration 1913: train_loss:2.295396566390991 val_loss2.2955007553100586\n",
            "iteration 1914: train_loss:2.295374631881714 val_loss2.295477867126465\n",
            "iteration 1915: train_loss:2.2953522205352783 val_loss2.295454978942871\n",
            "iteration 1916: train_loss:2.295330047607422 val_loss2.2954320907592773\n",
            "iteration 1917: train_loss:2.2953076362609863 val_loss2.2954087257385254\n",
            "iteration 1918: train_loss:2.2952849864959717 val_loss2.2953853607177734\n",
            "iteration 1919: train_loss:2.295262336730957 val_loss2.2953619956970215\n",
            "iteration 1920: train_loss:2.2952394485473633 val_loss2.2953383922576904\n",
            "iteration 1921: train_loss:2.2952167987823486 val_loss2.2953150272369385\n",
            "iteration 1922: train_loss:2.295193910598755 val_loss2.2952911853790283\n",
            "iteration 1923: train_loss:2.295170783996582 val_loss2.295267343521118\n",
            "iteration 1924: train_loss:2.29514741897583 val_loss2.295243263244629\n",
            "iteration 1925: train_loss:2.2951242923736572 val_loss2.2952194213867188\n",
            "iteration 1926: train_loss:2.2951009273529053 val_loss2.2951951026916504\n",
            "iteration 1927: train_loss:2.2950775623321533 val_loss2.295170783996582\n",
            "iteration 1928: train_loss:2.295053720474243 val_loss2.2951464653015137\n",
            "iteration 1929: train_loss:2.295030117034912 val_loss2.2951221466064453\n",
            "iteration 1930: train_loss:2.295006513595581 val_loss2.2950973510742188\n",
            "iteration 1931: train_loss:2.2949821949005127 val_loss2.2950727939605713\n",
            "iteration 1932: train_loss:2.2949585914611816 val_loss2.2950477600097656\n",
            "iteration 1933: train_loss:2.2949342727661133 val_loss2.295022964477539\n",
            "iteration 1934: train_loss:2.294909954071045 val_loss2.2949976921081543\n",
            "iteration 1935: train_loss:2.2948856353759766 val_loss2.2949726581573486\n",
            "iteration 1936: train_loss:2.294861078262329 val_loss2.2949471473693848\n",
            "iteration 1937: train_loss:2.2948365211486816 val_loss2.294921636581421\n",
            "iteration 1938: train_loss:2.294811725616455 val_loss2.294896364212036\n",
            "iteration 1939: train_loss:2.2947871685028076 val_loss2.294870615005493\n",
            "iteration 1940: train_loss:2.294762134552002 val_loss2.294844627380371\n",
            "iteration 1941: train_loss:2.2947371006011963 val_loss2.294818878173828\n",
            "iteration 1942: train_loss:2.2947118282318115 val_loss2.294793128967285\n",
            "iteration 1943: train_loss:2.2946865558624268 val_loss2.294766664505005\n",
            "iteration 1944: train_loss:2.294661283493042 val_loss2.2947404384613037\n",
            "iteration 1945: train_loss:2.294635772705078 val_loss2.2947142124176025\n",
            "iteration 1946: train_loss:2.2946102619171143 val_loss2.2946877479553223\n",
            "iteration 1947: train_loss:2.294584274291992 val_loss2.294661045074463\n",
            "iteration 1948: train_loss:2.294558525085449 val_loss2.2946341037750244\n",
            "iteration 1949: train_loss:2.294532537460327 val_loss2.294607400894165\n",
            "iteration 1950: train_loss:2.294506311416626 val_loss2.2945802211761475\n",
            "iteration 1951: train_loss:2.294480323791504 val_loss2.294553279876709\n",
            "iteration 1952: train_loss:2.2944536209106445 val_loss2.2945258617401123\n",
            "iteration 1953: train_loss:2.2944271564483643 val_loss2.2944984436035156\n",
            "iteration 1954: train_loss:2.294400453567505 val_loss2.294471025466919\n",
            "iteration 1955: train_loss:2.2943737506866455 val_loss2.294443130493164\n",
            "iteration 1956: train_loss:2.294347047805786 val_loss2.2944154739379883\n",
            "iteration 1957: train_loss:2.2943201065063477 val_loss2.2943873405456543\n",
            "iteration 1958: train_loss:2.29429292678833 val_loss2.2943594455718994\n",
            "iteration 1959: train_loss:2.2942657470703125 val_loss2.2943313121795654\n",
            "iteration 1960: train_loss:2.294238328933716 val_loss2.2943029403686523\n",
            "iteration 1961: train_loss:2.29421067237854 val_loss2.2942745685577393\n",
            "iteration 1962: train_loss:2.2941830158233643 val_loss2.294245958328247\n",
            "iteration 1963: train_loss:2.2941553592681885 val_loss2.294217109680176\n",
            "iteration 1964: train_loss:2.2941277027130127 val_loss2.2941882610321045\n",
            "iteration 1965: train_loss:2.2940995693206787 val_loss2.294158935546875\n",
            "iteration 1966: train_loss:2.2940711975097656 val_loss2.2941298484802246\n",
            "iteration 1967: train_loss:2.2940430641174316 val_loss2.294100761413574\n",
            "iteration 1968: train_loss:2.2940144538879395 val_loss2.2940711975097656\n",
            "iteration 1969: train_loss:2.2939860820770264 val_loss2.294041633605957\n",
            "iteration 1970: train_loss:2.293957233428955 val_loss2.2940118312835693\n",
            "iteration 1971: train_loss:2.293928384780884 val_loss2.2939822673797607\n",
            "iteration 1972: train_loss:2.2938995361328125 val_loss2.293952226638794\n",
            "iteration 1973: train_loss:2.293870210647583 val_loss2.293921947479248\n",
            "iteration 1974: train_loss:2.2938411235809326 val_loss2.293891668319702\n",
            "iteration 1975: train_loss:2.293811798095703 val_loss2.293861150741577\n",
            "iteration 1976: train_loss:2.2937822341918945 val_loss2.293830394744873\n",
            "iteration 1977: train_loss:2.293752431869507 val_loss2.293799877166748\n",
            "iteration 1978: train_loss:2.293722629547119 val_loss2.293768882751465\n",
            "iteration 1979: train_loss:2.2936925888061523 val_loss2.2937376499176025\n",
            "iteration 1980: train_loss:2.2936625480651855 val_loss2.2937066555023193\n",
            "iteration 1981: train_loss:2.2936322689056396 val_loss2.293675184249878\n",
            "iteration 1982: train_loss:2.2936017513275146 val_loss2.2936434745788574\n",
            "iteration 1983: train_loss:2.2935712337493896 val_loss2.293612003326416\n",
            "iteration 1984: train_loss:2.2935404777526855 val_loss2.2935800552368164\n",
            "iteration 1985: train_loss:2.2935097217559814 val_loss2.293548107147217\n",
            "iteration 1986: train_loss:2.293478488922119 val_loss2.293515920639038\n",
            "iteration 1987: train_loss:2.293447732925415 val_loss2.2934837341308594\n",
            "iteration 1988: train_loss:2.2934162616729736 val_loss2.2934513092041016\n",
            "iteration 1989: train_loss:2.293384552001953 val_loss2.2934186458587646\n",
            "iteration 1990: train_loss:2.2933528423309326 val_loss2.2933857440948486\n",
            "iteration 1991: train_loss:2.293321132659912 val_loss2.2933528423309326\n",
            "iteration 1992: train_loss:2.2932894229888916 val_loss2.2933197021484375\n",
            "iteration 1993: train_loss:2.293256998062134 val_loss2.2932865619659424\n",
            "iteration 1994: train_loss:2.293224811553955 val_loss2.293252944946289\n",
            "iteration 1995: train_loss:2.2931923866271973 val_loss2.293219566345215\n",
            "iteration 1996: train_loss:2.2931599617004395 val_loss2.2931857109069824\n",
            "iteration 1997: train_loss:2.2931270599365234 val_loss2.293151617050171\n",
            "iteration 1998: train_loss:2.2930939197540283 val_loss2.2931175231933594\n",
            "iteration 1999: train_loss:2.293060779571533 val_loss2.2930829524993896\n",
            "iteration 2000: train_loss:2.293027639389038 val_loss2.293048620223999\n",
            "iteration 2001: train_loss:2.292994260787964 val_loss2.2930140495300293\n",
            "iteration 2002: train_loss:2.2929604053497314 val_loss2.2929792404174805\n",
            "iteration 2003: train_loss:2.2929270267486572 val_loss2.2929441928863525\n",
            "iteration 2004: train_loss:2.292893171310425 val_loss2.2929089069366455\n",
            "iteration 2005: train_loss:2.292858839035034 val_loss2.2928736209869385\n",
            "iteration 2006: train_loss:2.2928247451782227 val_loss2.2928380966186523\n",
            "iteration 2007: train_loss:2.292790174484253 val_loss2.292802333831787\n",
            "iteration 2008: train_loss:2.292755603790283 val_loss2.2927663326263428\n",
            "iteration 2009: train_loss:2.2927207946777344 val_loss2.2927305698394775\n",
            "iteration 2010: train_loss:2.2926857471466064 val_loss2.292694091796875\n",
            "iteration 2011: train_loss:2.2926506996154785 val_loss2.2926576137542725\n",
            "iteration 2012: train_loss:2.2926154136657715 val_loss2.292620897293091\n",
            "iteration 2013: train_loss:2.2925798892974854 val_loss2.292584180831909\n",
            "iteration 2014: train_loss:2.29254412651062 val_loss2.2925469875335693\n",
            "iteration 2015: train_loss:2.292508363723755 val_loss2.2925097942352295\n",
            "iteration 2016: train_loss:2.2924721240997314 val_loss2.2924723625183105\n",
            "iteration 2017: train_loss:2.292436122894287 val_loss2.2924349308013916\n",
            "iteration 2018: train_loss:2.2923996448516846 val_loss2.2923970222473145\n",
            "iteration 2019: train_loss:2.292363166809082 val_loss2.2923591136932373\n",
            "iteration 2020: train_loss:2.2923262119293213 val_loss2.292320728302002\n",
            "iteration 2021: train_loss:2.2922894954681396 val_loss2.292282819747925\n",
            "iteration 2022: train_loss:2.2922520637512207 val_loss2.2922439575195312\n",
            "iteration 2023: train_loss:2.292214870452881 val_loss2.2922050952911377\n",
            "iteration 2024: train_loss:2.292177200317383 val_loss2.292166233062744\n",
            "iteration 2025: train_loss:2.2921392917633057 val_loss2.2921268939971924\n",
            "iteration 2026: train_loss:2.2921016216278076 val_loss2.2920875549316406\n",
            "iteration 2027: train_loss:2.2920634746551514 val_loss2.292048215866089\n",
            "iteration 2028: train_loss:2.292025089263916 val_loss2.2920081615448\n",
            "iteration 2029: train_loss:2.2919864654541016 val_loss2.29196834564209\n",
            "iteration 2030: train_loss:2.291947841644287 val_loss2.2919280529022217\n",
            "iteration 2031: train_loss:2.2919089794158936 val_loss2.2918875217437744\n",
            "iteration 2032: train_loss:2.291869640350342 val_loss2.291846990585327\n",
            "iteration 2033: train_loss:2.29183030128479 val_loss2.2918059825897217\n",
            "iteration 2034: train_loss:2.291790723800659 val_loss2.2917652130126953\n",
            "iteration 2035: train_loss:2.2917511463165283 val_loss2.2917237281799316\n",
            "iteration 2036: train_loss:2.2917113304138184 val_loss2.291682243347168\n",
            "iteration 2037: train_loss:2.29167103767395 val_loss2.291640520095825\n",
            "iteration 2038: train_loss:2.291630744934082 val_loss2.2915987968444824\n",
            "iteration 2039: train_loss:2.2915902137756348 val_loss2.2915563583374023\n",
            "iteration 2040: train_loss:2.2915494441986084 val_loss2.2915141582489014\n",
            "iteration 2041: train_loss:2.291508436203003 val_loss2.291471481323242\n",
            "iteration 2042: train_loss:2.2914669513702393 val_loss2.291428565979004\n",
            "iteration 2043: train_loss:2.2914257049560547 val_loss2.2913854122161865\n",
            "iteration 2044: train_loss:2.291383981704712 val_loss2.291342258453369\n",
            "iteration 2045: train_loss:2.291342258453369 val_loss2.2912988662719727\n",
            "iteration 2046: train_loss:2.291300058364868 val_loss2.291254997253418\n",
            "iteration 2047: train_loss:2.291257619857788 val_loss2.291210889816284\n",
            "iteration 2048: train_loss:2.291215181350708 val_loss2.2911670207977295\n",
            "iteration 2049: train_loss:2.291172504425049 val_loss2.2911221981048584\n",
            "iteration 2050: train_loss:2.2911295890808105 val_loss2.2910776138305664\n",
            "iteration 2051: train_loss:2.291086196899414 val_loss2.291032552719116\n",
            "iteration 2052: train_loss:2.2910428047180176 val_loss2.290987491607666\n",
            "iteration 2053: train_loss:2.290999174118042 val_loss2.2909419536590576\n",
            "iteration 2054: train_loss:2.2909553050994873 val_loss2.290896415710449\n",
            "iteration 2055: train_loss:2.2909109592437744 val_loss2.2908504009246826\n",
            "iteration 2056: train_loss:2.2908666133880615 val_loss2.290804386138916\n",
            "iteration 2057: train_loss:2.2908220291137695 val_loss2.290757894515991\n",
            "iteration 2058: train_loss:2.2907772064208984 val_loss2.2907111644744873\n",
            "iteration 2059: train_loss:2.2907321453094482 val_loss2.2906641960144043\n",
            "iteration 2060: train_loss:2.29068660736084 val_loss2.2906172275543213\n",
            "iteration 2061: train_loss:2.2906413078308105 val_loss2.290569543838501\n",
            "iteration 2062: train_loss:2.290595293045044 val_loss2.2905218601226807\n",
            "iteration 2063: train_loss:2.2905490398406982 val_loss2.2904741764068604\n",
            "iteration 2064: train_loss:2.2905027866363525 val_loss2.290426015853882\n",
            "iteration 2065: train_loss:2.2904560565948486 val_loss2.290377378463745\n",
            "iteration 2066: train_loss:2.290409564971924 val_loss2.2903287410736084\n",
            "iteration 2067: train_loss:2.2903623580932617 val_loss2.2902798652648926\n",
            "iteration 2068: train_loss:2.2903151512145996 val_loss2.2902305126190186\n",
            "iteration 2069: train_loss:2.2902672290802 val_loss2.2901809215545654\n",
            "iteration 2070: train_loss:2.29021954536438 val_loss2.290131092071533\n",
            "iteration 2071: train_loss:2.2901713848114014 val_loss2.290081024169922\n",
            "iteration 2072: train_loss:2.2901229858398438 val_loss2.2900307178497314\n",
            "iteration 2073: train_loss:2.290074348449707 val_loss2.289980173110962\n",
            "iteration 2074: train_loss:2.290025472640991 val_loss2.289929151535034\n",
            "iteration 2075: train_loss:2.289976119995117 val_loss2.2898778915405273\n",
            "iteration 2076: train_loss:2.2899270057678223 val_loss2.2898266315460205\n",
            "iteration 2077: train_loss:2.28987717628479 val_loss2.2897748947143555\n",
            "iteration 2078: train_loss:2.2898271083831787 val_loss2.2897226810455322\n",
            "iteration 2079: train_loss:2.2897768020629883 val_loss2.289670467376709\n",
            "iteration 2080: train_loss:2.2897262573242188 val_loss2.2896177768707275\n",
            "iteration 2081: train_loss:2.28967547416687 val_loss2.289565086364746\n",
            "iteration 2082: train_loss:2.2896244525909424 val_loss2.2895116806030273\n",
            "iteration 2083: train_loss:2.2895729541778564 val_loss2.2894582748413086\n",
            "iteration 2084: train_loss:2.2895212173461914 val_loss2.2894043922424316\n",
            "iteration 2085: train_loss:2.2894692420959473 val_loss2.2893502712249756\n",
            "iteration 2086: train_loss:2.289416790008545 val_loss2.2892961502075195\n",
            "iteration 2087: train_loss:2.2893643379211426 val_loss2.289241313934326\n",
            "iteration 2088: train_loss:2.289311408996582 val_loss2.2891862392425537\n",
            "iteration 2089: train_loss:2.2892584800720215 val_loss2.289130926132202\n",
            "iteration 2090: train_loss:2.2892050743103027 val_loss2.2890753746032715\n",
            "iteration 2091: train_loss:2.289151430130005 val_loss2.2890193462371826\n",
            "iteration 2092: train_loss:2.289097309112549 val_loss2.2889630794525146\n",
            "iteration 2093: train_loss:2.2890429496765137 val_loss2.2889065742492676\n",
            "iteration 2094: train_loss:2.2889883518218994 val_loss2.2888498306274414\n",
            "iteration 2095: train_loss:2.288933515548706 val_loss2.288792371749878\n",
            "iteration 2096: train_loss:2.2888784408569336 val_loss2.2887349128723145\n",
            "iteration 2097: train_loss:2.288822889328003 val_loss2.288677215576172\n",
            "iteration 2098: train_loss:2.288766860961914 val_loss2.288618803024292\n",
            "iteration 2099: train_loss:2.288710594177246 val_loss2.288560390472412\n",
            "iteration 2100: train_loss:2.288654327392578 val_loss2.288501501083374\n",
            "iteration 2101: train_loss:2.288597345352173 val_loss2.288442373275757\n",
            "iteration 2102: train_loss:2.2885403633117676 val_loss2.2883827686309814\n",
            "iteration 2103: train_loss:2.288482666015625 val_loss2.288323163986206\n",
            "iteration 2104: train_loss:2.2884252071380615 val_loss2.2882628440856934\n",
            "iteration 2105: train_loss:2.2883667945861816 val_loss2.2882022857666016\n",
            "iteration 2106: train_loss:2.2883083820343018 val_loss2.2881412506103516\n",
            "iteration 2107: train_loss:2.2882497310638428 val_loss2.2880799770355225\n",
            "iteration 2108: train_loss:2.2881906032562256 val_loss2.2880184650421143\n",
            "iteration 2109: train_loss:2.2881312370300293 val_loss2.287956476211548\n",
            "iteration 2110: train_loss:2.288071393966675 val_loss2.2878944873809814\n",
            "iteration 2111: train_loss:2.288011074066162 val_loss2.2878315448760986\n",
            "iteration 2112: train_loss:2.2879509925842285 val_loss2.287768602371216\n",
            "iteration 2113: train_loss:2.2878899574279785 val_loss2.287705183029175\n",
            "iteration 2114: train_loss:2.2878286838531494 val_loss2.2876415252685547\n",
            "iteration 2115: train_loss:2.2877674102783203 val_loss2.2875773906707764\n",
            "iteration 2116: train_loss:2.287705421447754 val_loss2.287513017654419\n",
            "iteration 2117: train_loss:2.2876431941986084 val_loss2.287447929382324\n",
            "iteration 2118: train_loss:2.287580728530884 val_loss2.2873828411102295\n",
            "iteration 2119: train_loss:2.287517547607422 val_loss2.2873170375823975\n",
            "iteration 2120: train_loss:2.28745436668396 val_loss2.2872512340545654\n",
            "iteration 2121: train_loss:2.28739070892334 val_loss2.287184715270996\n",
            "iteration 2122: train_loss:2.2873265743255615 val_loss2.2871179580688477\n",
            "iteration 2123: train_loss:2.287262201309204 val_loss2.287050724029541\n",
            "iteration 2124: train_loss:2.2871973514556885 val_loss2.2869832515716553\n",
            "iteration 2125: train_loss:2.2871322631835938 val_loss2.2869153022766113\n",
            "iteration 2126: train_loss:2.287066698074341 val_loss2.286846876144409\n",
            "iteration 2127: train_loss:2.287000894546509 val_loss2.286778211593628\n",
            "iteration 2128: train_loss:2.2869346141815186 val_loss2.2867090702056885\n",
            "iteration 2129: train_loss:2.286867618560791 val_loss2.286639451980591\n",
            "iteration 2130: train_loss:2.2868008613586426 val_loss2.286569595336914\n",
            "iteration 2131: train_loss:2.2867331504821777 val_loss2.286499500274658\n",
            "iteration 2132: train_loss:2.286665439605713 val_loss2.286428451538086\n",
            "iteration 2133: train_loss:2.2865970134735107 val_loss2.2863571643829346\n",
            "iteration 2134: train_loss:2.2865283489227295 val_loss2.286285400390625\n",
            "iteration 2135: train_loss:2.28645920753479 val_loss2.2862136363983154\n",
            "iteration 2136: train_loss:2.2863898277282715 val_loss2.2861411571502686\n",
            "iteration 2137: train_loss:2.2863199710845947 val_loss2.2860682010650635\n",
            "iteration 2138: train_loss:2.2862496376037598 val_loss2.2859950065612793\n",
            "iteration 2139: train_loss:2.2861790657043457 val_loss2.285921096801758\n",
            "iteration 2140: train_loss:2.2861077785491943 val_loss2.285846710205078\n",
            "iteration 2141: train_loss:2.2860360145568848 val_loss2.2857720851898193\n",
            "iteration 2142: train_loss:2.285964250564575 val_loss2.2856972217559814\n",
            "iteration 2143: train_loss:2.2858917713165283 val_loss2.285621404647827\n",
            "iteration 2144: train_loss:2.2858190536499023 val_loss2.285545587539673\n",
            "iteration 2145: train_loss:2.285745859146118 val_loss2.2854690551757812\n",
            "iteration 2146: train_loss:2.2856719493865967 val_loss2.2853922843933105\n",
            "iteration 2147: train_loss:2.285597801208496 val_loss2.2853147983551025\n",
            "iteration 2148: train_loss:2.2855231761932373 val_loss2.2852370738983154\n",
            "iteration 2149: train_loss:2.2854483127593994 val_loss2.285158634185791\n",
            "iteration 2150: train_loss:2.2853729724884033 val_loss2.2850799560546875\n",
            "iteration 2151: train_loss:2.285297155380249 val_loss2.285000801086426\n",
            "iteration 2152: train_loss:2.2852206230163574 val_loss2.284921169281006\n",
            "iteration 2153: train_loss:2.2851438522338867 val_loss2.2848408222198486\n",
            "iteration 2154: train_loss:2.2850663661956787 val_loss2.2847602367401123\n",
            "iteration 2155: train_loss:2.2849888801574707 val_loss2.2846789360046387\n",
            "iteration 2156: train_loss:2.2849106788635254 val_loss2.284597396850586\n",
            "iteration 2157: train_loss:2.284832000732422 val_loss2.284515142440796\n",
            "iteration 2158: train_loss:2.284752607345581 val_loss2.2844324111938477\n",
            "iteration 2159: train_loss:2.284672975540161 val_loss2.2843494415283203\n",
            "iteration 2160: train_loss:2.284592866897583 val_loss2.2842657566070557\n",
            "iteration 2161: train_loss:2.2845122814178467 val_loss2.284181833267212\n",
            "iteration 2162: train_loss:2.284431219100952 val_loss2.2840969562530518\n",
            "iteration 2163: train_loss:2.2843496799468994 val_loss2.2840118408203125\n",
            "iteration 2164: train_loss:2.2842676639556885 val_loss2.283926248550415\n",
            "iteration 2165: train_loss:2.2841851711273193 val_loss2.2838399410247803\n",
            "iteration 2166: train_loss:2.284101963043213 val_loss2.2837531566619873\n",
            "iteration 2167: train_loss:2.2840185165405273 val_loss2.283665895462036\n",
            "iteration 2168: train_loss:2.2839343547821045 val_loss2.2835781574249268\n",
            "iteration 2169: train_loss:2.2838497161865234 val_loss2.283489942550659\n",
            "iteration 2170: train_loss:2.283764600753784 val_loss2.2834010124206543\n",
            "iteration 2171: train_loss:2.283679246902466 val_loss2.283311605453491\n",
            "iteration 2172: train_loss:2.283592939376831 val_loss2.283221483230591\n",
            "iteration 2173: train_loss:2.283506155014038 val_loss2.2831308841705322\n",
            "iteration 2174: train_loss:2.283418893814087 val_loss2.2830400466918945\n",
            "iteration 2175: train_loss:2.2833311557769775 val_loss2.2829484939575195\n",
            "iteration 2176: train_loss:2.28324294090271 val_loss2.282855987548828\n",
            "iteration 2177: train_loss:2.283154249191284 val_loss2.2827632427215576\n",
            "iteration 2178: train_loss:2.283064842224121 val_loss2.282670259475708\n",
            "iteration 2179: train_loss:2.2829749584198 val_loss2.282576322555542\n",
            "iteration 2180: train_loss:2.282884359359741 val_loss2.2824816703796387\n",
            "iteration 2181: train_loss:2.2827932834625244 val_loss2.2823867797851562\n",
            "iteration 2182: train_loss:2.2827019691467285 val_loss2.2822909355163574\n",
            "iteration 2183: train_loss:2.2826099395751953 val_loss2.2821948528289795\n",
            "iteration 2184: train_loss:2.282517194747925 val_loss2.282097816467285\n",
            "iteration 2185: train_loss:2.282423734664917 val_loss2.2820005416870117\n",
            "iteration 2186: train_loss:2.28233003616333 val_loss2.281902551651001\n",
            "iteration 2187: train_loss:2.282235622406006 val_loss2.281804084777832\n",
            "iteration 2188: train_loss:2.2821404933929443 val_loss2.2817044258117676\n",
            "iteration 2189: train_loss:2.2820448875427246 val_loss2.281604766845703\n",
            "iteration 2190: train_loss:2.2819488048553467 val_loss2.2815043926239014\n",
            "iteration 2191: train_loss:2.2818520069122314 val_loss2.281403064727783\n",
            "iteration 2192: train_loss:2.281754493713379 val_loss2.281301498413086\n",
            "iteration 2193: train_loss:2.281656503677368 val_loss2.2811989784240723\n",
            "iteration 2194: train_loss:2.281558036804199 val_loss2.2810959815979004\n",
            "iteration 2195: train_loss:2.281458854675293 val_loss2.280992269515991\n",
            "iteration 2196: train_loss:2.2813591957092285 val_loss2.2808878421783447\n",
            "iteration 2197: train_loss:2.2812583446502686 val_loss2.28078293800354\n",
            "iteration 2198: train_loss:2.2811574935913086 val_loss2.280677318572998\n",
            "iteration 2199: train_loss:2.2810559272766113 val_loss2.2805709838867188\n",
            "iteration 2200: train_loss:2.2809536457061768 val_loss2.280463933944702\n",
            "iteration 2201: train_loss:2.280850648880005 val_loss2.2803564071655273\n",
            "iteration 2202: train_loss:2.2807469367980957 val_loss2.2802481651306152\n",
            "iteration 2203: train_loss:2.2806427478790283 val_loss2.280139207839966\n",
            "iteration 2204: train_loss:2.2805376052856445 val_loss2.280029535293579\n",
            "iteration 2205: train_loss:2.2804322242736816 val_loss2.279919147491455\n",
            "iteration 2206: train_loss:2.2803258895874023 val_loss2.2798080444335938\n",
            "iteration 2207: train_loss:2.2802188396453857 val_loss2.279696226119995\n",
            "iteration 2208: train_loss:2.28011155128479 val_loss2.279583692550659\n",
            "iteration 2209: train_loss:2.280003309249878 val_loss2.279470443725586\n",
            "iteration 2210: train_loss:2.2798943519592285 val_loss2.2793567180633545\n",
            "iteration 2211: train_loss:2.279784679412842 val_loss2.2792420387268066\n",
            "iteration 2212: train_loss:2.279674530029297 val_loss2.2791266441345215\n",
            "iteration 2213: train_loss:2.2795634269714355 val_loss2.279010772705078\n",
            "iteration 2214: train_loss:2.279451608657837 val_loss2.2788937091827393\n",
            "iteration 2215: train_loss:2.27933931350708 val_loss2.2787764072418213\n",
            "iteration 2216: train_loss:2.279226303100586 val_loss2.2786576747894287\n",
            "iteration 2217: train_loss:2.2791125774383545 val_loss2.278538942337036\n",
            "iteration 2218: train_loss:2.2789978981018066 val_loss2.278419256210327\n",
            "iteration 2219: train_loss:2.2788827419281006 val_loss2.2782986164093018\n",
            "iteration 2220: train_loss:2.278766632080078 val_loss2.278177261352539\n",
            "iteration 2221: train_loss:2.2786498069763184 val_loss2.278055191040039\n",
            "iteration 2222: train_loss:2.2785325050354004 val_loss2.2779321670532227\n",
            "iteration 2223: train_loss:2.278414249420166 val_loss2.277808666229248\n",
            "iteration 2224: train_loss:2.2782952785491943 val_loss2.277683973312378\n",
            "iteration 2225: train_loss:2.2781755924224854 val_loss2.2775590419769287\n",
            "iteration 2226: train_loss:2.27805495262146 val_loss2.277432918548584\n",
            "iteration 2227: train_loss:2.2779340744018555 val_loss2.277306079864502\n",
            "iteration 2228: train_loss:2.2778117656707764 val_loss2.2771785259246826\n",
            "iteration 2229: train_loss:2.277688980102539 val_loss2.277050256729126\n",
            "iteration 2230: train_loss:2.2775652408599854 val_loss2.276920795440674\n",
            "iteration 2231: train_loss:2.2774410247802734 val_loss2.2767906188964844\n",
            "iteration 2232: train_loss:2.277316093444824 val_loss2.2766597270965576\n",
            "iteration 2233: train_loss:2.2771902084350586 val_loss2.2765281200408936\n",
            "iteration 2234: train_loss:2.2770633697509766 val_loss2.276395320892334\n",
            "iteration 2235: train_loss:2.2769358158111572 val_loss2.2762622833251953\n",
            "iteration 2236: train_loss:2.2768075466156006 val_loss2.276127815246582\n",
            "iteration 2237: train_loss:2.2766783237457275 val_loss2.2759926319122314\n",
            "iteration 2238: train_loss:2.276548385620117 val_loss2.2758567333221436\n",
            "iteration 2239: train_loss:2.2764174938201904 val_loss2.2757198810577393\n",
            "iteration 2240: train_loss:2.2762856483459473 val_loss2.2755823135375977\n",
            "iteration 2241: train_loss:2.276153326034546 val_loss2.2754435539245605\n",
            "iteration 2242: train_loss:2.276020050048828 val_loss2.275304079055786\n",
            "iteration 2243: train_loss:2.275886058807373 val_loss2.2751636505126953\n",
            "iteration 2244: train_loss:2.2757508754730225 val_loss2.275022268295288\n",
            "iteration 2245: train_loss:2.2756149768829346 val_loss2.2748804092407227\n",
            "iteration 2246: train_loss:2.2754783630371094 val_loss2.2747373580932617\n",
            "iteration 2247: train_loss:2.2753405570983887 val_loss2.2745931148529053\n",
            "iteration 2248: train_loss:2.2752020359039307 val_loss2.2744481563568115\n",
            "iteration 2249: train_loss:2.2750627994537354 val_loss2.2743024826049805\n",
            "iteration 2250: train_loss:2.2749226093292236 val_loss2.274155616760254\n",
            "iteration 2251: train_loss:2.2747814655303955 val_loss2.27400803565979\n",
            "iteration 2252: train_loss:2.274639368057251 val_loss2.2738590240478516\n",
            "iteration 2253: train_loss:2.274496555328369 val_loss2.273709774017334\n",
            "iteration 2254: train_loss:2.2743523120880127 val_loss2.273559093475342\n",
            "iteration 2255: train_loss:2.274207830429077 val_loss2.273407220840454\n",
            "iteration 2256: train_loss:2.274061918258667 val_loss2.273254871368408\n",
            "iteration 2257: train_loss:2.2739155292510986 val_loss2.273101329803467\n",
            "iteration 2258: train_loss:2.2737679481506348 val_loss2.272946834564209\n",
            "iteration 2259: train_loss:2.2736191749572754 val_loss2.2727913856506348\n",
            "iteration 2260: train_loss:2.2734696865081787 val_loss2.272634744644165\n",
            "iteration 2261: train_loss:2.2733192443847656 val_loss2.272477388381958\n",
            "iteration 2262: train_loss:2.273167848587036 val_loss2.2723188400268555\n",
            "iteration 2263: train_loss:2.2730154991149902 val_loss2.2721593379974365\n",
            "iteration 2264: train_loss:2.272862195968628 val_loss2.271998882293701\n",
            "iteration 2265: train_loss:2.272707939147949 val_loss2.2718374729156494\n",
            "iteration 2266: train_loss:2.272552490234375 val_loss2.271674871444702\n",
            "iteration 2267: train_loss:2.2723963260650635 val_loss2.2715113162994385\n",
            "iteration 2268: train_loss:2.2722387313842773 val_loss2.2713465690612793\n",
            "iteration 2269: train_loss:2.272080659866333 val_loss2.271181106567383\n",
            "iteration 2270: train_loss:2.271921396255493 val_loss2.2710139751434326\n",
            "iteration 2271: train_loss:2.271760940551758 val_loss2.270846128463745\n",
            "iteration 2272: train_loss:2.271599531173706 val_loss2.270677089691162\n",
            "iteration 2273: train_loss:2.271437168121338 val_loss2.2705070972442627\n",
            "iteration 2274: train_loss:2.271273374557495 val_loss2.2703359127044678\n",
            "iteration 2275: train_loss:2.271109104156494 val_loss2.2701635360717773\n",
            "iteration 2276: train_loss:2.2709434032440186 val_loss2.2699899673461914\n",
            "iteration 2277: train_loss:2.2707767486572266 val_loss2.269815683364868\n",
            "iteration 2278: train_loss:2.270609140396118 val_loss2.2696402072906494\n",
            "iteration 2279: train_loss:2.2704405784606934 val_loss2.269463300704956\n",
            "iteration 2280: train_loss:2.270270347595215 val_loss2.269285202026367\n",
            "iteration 2281: train_loss:2.270099639892578 val_loss2.269106388092041\n",
            "iteration 2282: train_loss:2.269927501678467 val_loss2.268925905227661\n",
            "iteration 2283: train_loss:2.26975417137146 val_loss2.268744707107544\n",
            "iteration 2284: train_loss:2.269580125808716 val_loss2.268562078475952\n",
            "iteration 2285: train_loss:2.269404649734497 val_loss2.268378257751465\n",
            "iteration 2286: train_loss:2.269228219985962 val_loss2.268193483352661\n",
            "iteration 2287: train_loss:2.2690505981445312 val_loss2.268007516860962\n",
            "iteration 2288: train_loss:2.268872022628784 val_loss2.267820119857788\n",
            "iteration 2289: train_loss:2.2686920166015625 val_loss2.2676315307617188\n",
            "iteration 2290: train_loss:2.2685110569000244 val_loss2.267441749572754\n",
            "iteration 2291: train_loss:2.2683286666870117 val_loss2.2672510147094727\n",
            "iteration 2292: train_loss:2.2681453227996826 val_loss2.2670586109161377\n",
            "iteration 2293: train_loss:2.267960786819458 val_loss2.2668654918670654\n",
            "iteration 2294: train_loss:2.267775297164917 val_loss2.2666709423065186\n",
            "iteration 2295: train_loss:2.2675883769989014 val_loss2.266475200653076\n",
            "iteration 2296: train_loss:2.2674002647399902 val_loss2.266278028488159\n",
            "iteration 2297: train_loss:2.2672111988067627 val_loss2.2660796642303467\n",
            "iteration 2298: train_loss:2.2670207023620605 val_loss2.2658798694610596\n",
            "iteration 2299: train_loss:2.266829013824463 val_loss2.265679121017456\n",
            "iteration 2300: train_loss:2.2666361331939697 val_loss2.265476942062378\n",
            "iteration 2301: train_loss:2.266442060470581 val_loss2.2652735710144043\n",
            "iteration 2302: train_loss:2.2662465572357178 val_loss2.265068769454956\n",
            "iteration 2303: train_loss:2.266049861907959 val_loss2.2648627758026123\n",
            "iteration 2304: train_loss:2.265852212905884 val_loss2.264655351638794\n",
            "iteration 2305: train_loss:2.265653133392334 val_loss2.264446496963501\n",
            "iteration 2306: train_loss:2.2654528617858887 val_loss2.2642364501953125\n",
            "iteration 2307: train_loss:2.265251398086548 val_loss2.2640252113342285\n",
            "iteration 2308: train_loss:2.2650485038757324 val_loss2.263812303543091\n",
            "iteration 2309: train_loss:2.2648444175720215 val_loss2.2635982036590576\n",
            "iteration 2310: train_loss:2.264638900756836 val_loss2.263382911682129\n",
            "iteration 2311: train_loss:2.264432191848755 val_loss2.2631659507751465\n",
            "iteration 2312: train_loss:2.2642245292663574 val_loss2.2629477977752686\n",
            "iteration 2313: train_loss:2.2640156745910645 val_loss2.262728214263916\n",
            "iteration 2314: train_loss:2.2638046741485596 val_loss2.262507200241089\n",
            "iteration 2315: train_loss:2.2635929584503174 val_loss2.262284994125366\n",
            "iteration 2316: train_loss:2.2633795738220215 val_loss2.26206111907959\n",
            "iteration 2317: train_loss:2.263165235519409 val_loss2.261836051940918\n",
            "iteration 2318: train_loss:2.2629494667053223 val_loss2.2616095542907715\n",
            "iteration 2319: train_loss:2.2627320289611816 val_loss2.2613813877105713\n",
            "iteration 2320: train_loss:2.2625133991241455 val_loss2.2611520290374756\n",
            "iteration 2321: train_loss:2.262293577194214 val_loss2.2609212398529053\n",
            "iteration 2322: train_loss:2.2620720863342285 val_loss2.2606887817382812\n",
            "iteration 2323: train_loss:2.2618494033813477 val_loss2.2604551315307617\n",
            "iteration 2324: train_loss:2.261625289916992 val_loss2.2602200508117676\n",
            "iteration 2325: train_loss:2.261399745941162 val_loss2.259983539581299\n",
            "iteration 2326: train_loss:2.2611727714538574 val_loss2.2597453594207764\n",
            "iteration 2327: train_loss:2.260944366455078 val_loss2.2595057487487793\n",
            "iteration 2328: train_loss:2.2607147693634033 val_loss2.2592647075653076\n",
            "iteration 2329: train_loss:2.260483503341675 val_loss2.2590222358703613\n",
            "iteration 2330: train_loss:2.260251045227051 val_loss2.2587780952453613\n",
            "iteration 2331: train_loss:2.260016918182373 val_loss2.2585322856903076\n",
            "iteration 2332: train_loss:2.2597813606262207 val_loss2.2582852840423584\n",
            "iteration 2333: train_loss:2.2595443725585938 val_loss2.2580368518829346\n",
            "iteration 2334: train_loss:2.259305953979492 val_loss2.257786512374878\n",
            "iteration 2335: train_loss:2.259066104888916 val_loss2.257534980773926\n",
            "iteration 2336: train_loss:2.2588248252868652 val_loss2.257281541824341\n",
            "iteration 2337: train_loss:2.258582353591919 val_loss2.2570266723632812\n",
            "iteration 2338: train_loss:2.25833797454834 val_loss2.256770610809326\n",
            "iteration 2339: train_loss:2.2580924034118652 val_loss2.256512403488159\n",
            "iteration 2340: train_loss:2.257845401763916 val_loss2.2562527656555176\n",
            "iteration 2341: train_loss:2.257596731185913 val_loss2.2559916973114014\n",
            "iteration 2342: train_loss:2.2573463916778564 val_loss2.2557289600372314\n",
            "iteration 2343: train_loss:2.257094621658325 val_loss2.255464792251587\n",
            "iteration 2344: train_loss:2.2568416595458984 val_loss2.2551989555358887\n",
            "iteration 2345: train_loss:2.2565865516662598 val_loss2.2549314498901367\n",
            "iteration 2346: train_loss:2.2563304901123047 val_loss2.254662275314331\n",
            "iteration 2347: train_loss:2.256072521209717 val_loss2.254391670227051\n",
            "iteration 2348: train_loss:2.2558133602142334 val_loss2.254119396209717\n",
            "iteration 2349: train_loss:2.2555525302886963 val_loss2.253845453262329\n",
            "iteration 2350: train_loss:2.2552900314331055 val_loss2.2535698413848877\n",
            "iteration 2351: train_loss:2.25502610206604 val_loss2.2532925605773926\n",
            "iteration 2352: train_loss:2.254760265350342 val_loss2.253013849258423\n",
            "iteration 2353: train_loss:2.254493236541748 val_loss2.2527332305908203\n",
            "iteration 2354: train_loss:2.2542245388031006 val_loss2.252450942993164\n",
            "iteration 2355: train_loss:2.2539541721343994 val_loss2.2521674633026123\n",
            "iteration 2356: train_loss:2.2536821365356445 val_loss2.2518820762634277\n",
            "iteration 2357: train_loss:2.253408670425415 val_loss2.2515950202941895\n",
            "iteration 2358: train_loss:2.253133773803711 val_loss2.2513060569763184\n",
            "iteration 2359: train_loss:2.252856731414795 val_loss2.2510156631469727\n",
            "iteration 2360: train_loss:2.2525782585144043 val_loss2.2507236003875732\n",
            "iteration 2361: train_loss:2.252298593521118 val_loss2.250429630279541\n",
            "iteration 2362: train_loss:2.252017021179199 val_loss2.2501344680786133\n",
            "iteration 2363: train_loss:2.2517337799072266 val_loss2.2498371601104736\n",
            "iteration 2364: train_loss:2.251448631286621 val_loss2.2495386600494385\n",
            "iteration 2365: train_loss:2.25116229057312 val_loss2.2492380142211914\n",
            "iteration 2366: train_loss:2.2508740425109863 val_loss2.2489356994628906\n",
            "iteration 2367: train_loss:2.250584363937378 val_loss2.248631715774536\n",
            "iteration 2368: train_loss:2.2502927780151367 val_loss2.248325824737549\n",
            "iteration 2369: train_loss:2.249999523162842 val_loss2.248018264770508\n",
            "iteration 2370: train_loss:2.249704599380493 val_loss2.247709035873413\n",
            "iteration 2371: train_loss:2.24940824508667 val_loss2.2473981380462646\n",
            "iteration 2372: train_loss:2.249109983444214 val_loss2.2470855712890625\n",
            "iteration 2373: train_loss:2.248810052871704 val_loss2.2467708587646484\n",
            "iteration 2374: train_loss:2.2485084533691406 val_loss2.2464542388916016\n",
            "iteration 2375: train_loss:2.2482054233551025 val_loss2.246136426925659\n",
            "iteration 2376: train_loss:2.2479004859924316 val_loss2.245816469192505\n",
            "iteration 2377: train_loss:2.247593879699707 val_loss2.245494842529297\n",
            "iteration 2378: train_loss:2.2472856044769287 val_loss2.245171546936035\n",
            "iteration 2379: train_loss:2.2469754219055176 val_loss2.2448463439941406\n",
            "iteration 2380: train_loss:2.246663808822632 val_loss2.2445194721221924\n",
            "iteration 2381: train_loss:2.2463502883911133 val_loss2.2441904544830322\n",
            "iteration 2382: train_loss:2.246035575866699 val_loss2.2438600063323975\n",
            "iteration 2383: train_loss:2.245718479156494 val_loss2.24352765083313\n",
            "iteration 2384: train_loss:2.2454001903533936 val_loss2.2431933879852295\n",
            "iteration 2385: train_loss:2.245079755783081 val_loss2.2428574562072754\n",
            "iteration 2386: train_loss:2.244757890701294 val_loss2.2425196170806885\n",
            "iteration 2387: train_loss:2.244434356689453 val_loss2.2421798706054688\n",
            "iteration 2388: train_loss:2.2441086769104004 val_loss2.2418384552001953\n",
            "iteration 2389: train_loss:2.243781805038452 val_loss2.241495370864868\n",
            "iteration 2390: train_loss:2.243452787399292 val_loss2.241150140762329\n",
            "iteration 2391: train_loss:2.2431223392486572 val_loss2.2408032417297363\n",
            "iteration 2392: train_loss:2.2427897453308105 val_loss2.2404544353485107\n",
            "iteration 2393: train_loss:2.2424559593200684 val_loss2.2401037216186523\n",
            "iteration 2394: train_loss:2.2421202659606934 val_loss2.2397515773773193\n",
            "iteration 2395: train_loss:2.2417829036712646 val_loss2.2393970489501953\n",
            "iteration 2396: train_loss:2.241443395614624 val_loss2.239041328430176\n",
            "iteration 2397: train_loss:2.241102695465088 val_loss2.2386834621429443\n",
            "iteration 2398: train_loss:2.24075984954834 val_loss2.238323926925659\n",
            "iteration 2399: train_loss:2.2404158115386963 val_loss2.237962245941162\n",
            "iteration 2400: train_loss:2.2400693893432617 val_loss2.2375988960266113\n",
            "iteration 2401: train_loss:2.2397215366363525 val_loss2.237233877182007\n",
            "iteration 2402: train_loss:2.2393717765808105 val_loss2.2368671894073486\n",
            "iteration 2403: train_loss:2.239020586013794 val_loss2.2364983558654785\n",
            "iteration 2404: train_loss:2.2386674880981445 val_loss2.2361276149749756\n",
            "iteration 2405: train_loss:2.2383124828338623 val_loss2.235755205154419\n",
            "iteration 2406: train_loss:2.2379558086395264 val_loss2.2353811264038086\n",
            "iteration 2407: train_loss:2.2375974655151367 val_loss2.2350051403045654\n",
            "iteration 2408: train_loss:2.2372374534606934 val_loss2.2346272468566895\n",
            "iteration 2409: train_loss:2.236875534057617 val_loss2.2342474460601807\n",
            "iteration 2410: train_loss:2.236511707305908 val_loss2.233865976333618\n",
            "iteration 2411: train_loss:2.2361466884613037 val_loss2.233482599258423\n",
            "iteration 2412: train_loss:2.2357795238494873 val_loss2.2330973148345947\n",
            "iteration 2413: train_loss:2.235410690307617 val_loss2.232710361480713\n",
            "iteration 2414: train_loss:2.2350401878356934 val_loss2.2323215007781982\n",
            "iteration 2415: train_loss:2.2346677780151367 val_loss2.231930732727051\n",
            "iteration 2416: train_loss:2.2342939376831055 val_loss2.2315385341644287\n",
            "iteration 2417: train_loss:2.2339181900024414 val_loss2.2311441898345947\n",
            "iteration 2418: train_loss:2.2335410118103027 val_loss2.230747938156128\n",
            "iteration 2419: train_loss:2.233161449432373 val_loss2.2303502559661865\n",
            "iteration 2420: train_loss:2.232780694961548 val_loss2.2299506664276123\n",
            "iteration 2421: train_loss:2.23239803314209 val_loss2.2295491695404053\n",
            "iteration 2422: train_loss:2.232013702392578 val_loss2.2291460037231445\n",
            "iteration 2423: train_loss:2.2316274642944336 val_loss2.228740930557251\n",
            "iteration 2424: train_loss:2.2312397956848145 val_loss2.2283341884613037\n",
            "iteration 2425: train_loss:2.2308504581451416 val_loss2.2279253005981445\n",
            "iteration 2426: train_loss:2.230459213256836 val_loss2.2275149822235107\n",
            "iteration 2427: train_loss:2.2300662994384766 val_loss2.227102756500244\n",
            "iteration 2428: train_loss:2.2296714782714844 val_loss2.226688861846924\n",
            "iteration 2429: train_loss:2.2292749881744385 val_loss2.226273536682129\n",
            "iteration 2430: train_loss:2.228877067565918 val_loss2.225856065750122\n",
            "iteration 2431: train_loss:2.2284772396087646 val_loss2.2254366874694824\n",
            "iteration 2432: train_loss:2.2280759811401367 val_loss2.225015878677368\n",
            "iteration 2433: train_loss:2.227672815322876 val_loss2.224593162536621\n",
            "iteration 2434: train_loss:2.2272682189941406 val_loss2.2241687774658203\n",
            "iteration 2435: train_loss:2.2268619537353516 val_loss2.223742723464966\n",
            "iteration 2436: train_loss:2.2264537811279297 val_loss2.2233150005340576\n",
            "iteration 2437: train_loss:2.226044178009033 val_loss2.2228856086730957\n",
            "iteration 2438: train_loss:2.225633144378662 val_loss2.222454309463501\n",
            "iteration 2439: train_loss:2.2252204418182373 val_loss2.2220215797424316\n",
            "iteration 2440: train_loss:2.224806308746338 val_loss2.2215864658355713\n",
            "iteration 2441: train_loss:2.2243902683258057 val_loss2.2211503982543945\n",
            "iteration 2442: train_loss:2.2239725589752197 val_loss2.220712423324585\n",
            "iteration 2443: train_loss:2.223553419113159 val_loss2.2202727794647217\n",
            "iteration 2444: train_loss:2.223132848739624 val_loss2.2198314666748047\n",
            "iteration 2445: train_loss:2.222710132598877 val_loss2.219388484954834\n",
            "iteration 2446: train_loss:2.2222862243652344 val_loss2.2189438343048096\n",
            "iteration 2447: train_loss:2.221860885620117 val_loss2.2184977531433105\n",
            "iteration 2448: train_loss:2.221433639526367 val_loss2.218050003051758\n",
            "iteration 2449: train_loss:2.221005439758301 val_loss2.2176003456115723\n",
            "iteration 2450: train_loss:2.2205750942230225 val_loss2.217149257659912\n",
            "iteration 2451: train_loss:2.2201437950134277 val_loss2.2166967391967773\n",
            "iteration 2452: train_loss:2.2197105884552 val_loss2.216242551803589\n",
            "iteration 2453: train_loss:2.219276189804077 val_loss2.2157866954803467\n",
            "iteration 2454: train_loss:2.2188401222229004 val_loss2.215329170227051\n",
            "iteration 2455: train_loss:2.218402624130249 val_loss2.2148704528808594\n",
            "iteration 2456: train_loss:2.217963457107544 val_loss2.2144103050231934\n",
            "iteration 2457: train_loss:2.2175230979919434 val_loss2.2139482498168945\n",
            "iteration 2458: train_loss:2.217081069946289 val_loss2.2134850025177\n",
            "iteration 2459: train_loss:2.21663761138916 val_loss2.213020086288452\n",
            "iteration 2460: train_loss:2.2161929607391357 val_loss2.2125537395477295\n",
            "iteration 2461: train_loss:2.2157466411590576 val_loss2.2120859622955322\n",
            "iteration 2462: train_loss:2.215299129486084 val_loss2.2116165161132812\n",
            "iteration 2463: train_loss:2.2148501873016357 val_loss2.2111458778381348\n",
            "iteration 2464: train_loss:2.214399814605713 val_loss2.2106735706329346\n",
            "iteration 2465: train_loss:2.2139482498168945 val_loss2.210200071334839\n",
            "iteration 2466: train_loss:2.2134954929351807 val_loss2.2097249031066895\n",
            "iteration 2467: train_loss:2.2130415439605713 val_loss2.2092483043670654\n",
            "iteration 2468: train_loss:2.2125861644744873 val_loss2.208770513534546\n",
            "iteration 2469: train_loss:2.212129592895508 val_loss2.2082908153533936\n",
            "iteration 2470: train_loss:2.211671829223633 val_loss2.207810163497925\n",
            "iteration 2471: train_loss:2.211212635040283 val_loss2.2073280811309814\n",
            "iteration 2472: train_loss:2.210752248764038 val_loss2.2068443298339844\n",
            "iteration 2473: train_loss:2.2102904319763184 val_loss2.20635986328125\n",
            "iteration 2474: train_loss:2.209827423095703 val_loss2.205873727798462\n",
            "iteration 2475: train_loss:2.2093632221221924 val_loss2.2053866386413574\n",
            "iteration 2476: train_loss:2.2088983058929443 val_loss2.2048981189727783\n",
            "iteration 2477: train_loss:2.2084317207336426 val_loss2.2044081687927246\n",
            "iteration 2478: train_loss:2.2079641819000244 val_loss2.2039170265197754\n",
            "iteration 2479: train_loss:2.20749568939209 val_loss2.203425168991089\n",
            "iteration 2480: train_loss:2.2070260047912598 val_loss2.2029318809509277\n",
            "iteration 2481: train_loss:2.206554889678955 val_loss2.20243763923645\n",
            "iteration 2482: train_loss:2.206083059310913 val_loss2.201941967010498\n",
            "iteration 2483: train_loss:2.2056100368499756 val_loss2.2014453411102295\n",
            "iteration 2484: train_loss:2.2051358222961426 val_loss2.2009479999542236\n",
            "iteration 2485: train_loss:2.204660654067993 val_loss2.2004494667053223\n",
            "iteration 2486: train_loss:2.2041845321655273 val_loss2.1999497413635254\n",
            "iteration 2487: train_loss:2.203707456588745 val_loss2.199449300765991\n",
            "iteration 2488: train_loss:2.2032289505004883 val_loss2.1989474296569824\n",
            "iteration 2489: train_loss:2.202749729156494 val_loss2.198444366455078\n",
            "iteration 2490: train_loss:2.2022697925567627 val_loss2.1979408264160156\n",
            "iteration 2491: train_loss:2.2017886638641357 val_loss2.1974360942840576\n",
            "iteration 2492: train_loss:2.2013068199157715 val_loss2.1969306468963623\n",
            "iteration 2493: train_loss:2.200824022293091 val_loss2.1964242458343506\n",
            "iteration 2494: train_loss:2.2003402709960938 val_loss2.1959168910980225\n",
            "iteration 2495: train_loss:2.199855327606201 val_loss2.195408582687378\n",
            "iteration 2496: train_loss:2.1993699073791504 val_loss2.194899559020996\n",
            "iteration 2497: train_loss:2.198883056640625 val_loss2.194389581680298\n",
            "iteration 2498: train_loss:2.1983962059020996 val_loss2.193878412246704\n",
            "iteration 2499: train_loss:2.1979074478149414 val_loss2.193366765975952\n",
            "iteration 2500: train_loss:2.197418212890625 val_loss2.192854166030884\n",
            "iteration 2501: train_loss:2.196928024291992 val_loss2.192340612411499\n",
            "iteration 2502: train_loss:2.196437120437622 val_loss2.191826105117798\n",
            "iteration 2503: train_loss:2.1959450244903564 val_loss2.1913113594055176\n",
            "iteration 2504: train_loss:2.1954522132873535 val_loss2.190795421600342\n",
            "iteration 2505: train_loss:2.194958448410034 val_loss2.190279006958008\n",
            "iteration 2506: train_loss:2.1944642066955566 val_loss2.1897616386413574\n",
            "iteration 2507: train_loss:2.1939690113067627 val_loss2.189243793487549\n",
            "iteration 2508: train_loss:2.1934731006622314 val_loss2.188724994659424\n",
            "iteration 2509: train_loss:2.192976474761963 val_loss2.1882054805755615\n",
            "iteration 2510: train_loss:2.192479133605957 val_loss2.187685251235962\n",
            "iteration 2511: train_loss:2.191981554031372 val_loss2.187164545059204\n",
            "iteration 2512: train_loss:2.1914830207824707 val_loss2.186643123626709\n",
            "iteration 2513: train_loss:2.1909844875335693 val_loss2.1861207485198975\n",
            "iteration 2514: train_loss:2.1904852390289307 val_loss2.1855978965759277\n",
            "iteration 2515: train_loss:2.1899850368499756 val_loss2.1850743293762207\n",
            "iteration 2516: train_loss:2.1894850730895996 val_loss2.1845502853393555\n",
            "iteration 2517: train_loss:2.188983917236328 val_loss2.184025287628174\n",
            "iteration 2518: train_loss:2.1884829998016357 val_loss2.183499813079834\n",
            "iteration 2519: train_loss:2.1879818439483643 val_loss2.182973861694336\n",
            "iteration 2520: train_loss:2.1874799728393555 val_loss2.1824471950531006\n",
            "iteration 2521: train_loss:2.1869771480560303 val_loss2.181920051574707\n",
            "iteration 2522: train_loss:2.186474084854126 val_loss2.1813924312591553\n",
            "iteration 2523: train_loss:2.1859703063964844 val_loss2.1808645725250244\n",
            "iteration 2524: train_loss:2.1854658126831055 val_loss2.1803359985351562\n",
            "iteration 2525: train_loss:2.1849606037139893 val_loss2.179807186126709\n",
            "iteration 2526: train_loss:2.184455394744873 val_loss2.1792778968811035\n",
            "iteration 2527: train_loss:2.183950424194336 val_loss2.178748369216919\n",
            "iteration 2528: train_loss:2.1834444999694824 val_loss2.178218364715576\n",
            "iteration 2529: train_loss:2.1829380989074707 val_loss2.177687883377075\n",
            "iteration 2530: train_loss:2.182431697845459 val_loss2.177157163619995\n",
            "iteration 2531: train_loss:2.181924819946289 val_loss2.176626205444336\n",
            "iteration 2532: train_loss:2.18141770362854 val_loss2.1760947704315186\n",
            "iteration 2533: train_loss:2.180910348892212 val_loss2.175563097000122\n",
            "iteration 2534: train_loss:2.1804027557373047 val_loss2.1750309467315674\n",
            "iteration 2535: train_loss:2.1798949241638184 val_loss2.174499034881592\n",
            "iteration 2536: train_loss:2.1793863773345947 val_loss2.173966646194458\n",
            "iteration 2537: train_loss:2.178877592086792 val_loss2.173434019088745\n",
            "iteration 2538: train_loss:2.1783688068389893 val_loss2.1729013919830322\n",
            "iteration 2539: train_loss:2.177859306335449 val_loss2.1723687648773193\n",
            "iteration 2540: train_loss:2.177349328994751 val_loss2.1718356609344482\n",
            "iteration 2541: train_loss:2.1768393516540527 val_loss2.1713027954101562\n",
            "iteration 2542: train_loss:2.1763291358947754 val_loss2.170769453048706\n",
            "iteration 2543: train_loss:2.175818681716919 val_loss2.170236349105835\n",
            "iteration 2544: train_loss:2.1753082275390625 val_loss2.1697030067443848\n",
            "iteration 2545: train_loss:2.174797296524048 val_loss2.1691701412200928\n",
            "iteration 2546: train_loss:2.1742868423461914 val_loss2.1686363220214844\n",
            "iteration 2547: train_loss:2.1737756729125977 val_loss2.168102979660034\n",
            "iteration 2548: train_loss:2.173264741897583 val_loss2.167569875717163\n",
            "iteration 2549: train_loss:2.1727535724639893 val_loss2.167036294937134\n",
            "iteration 2550: train_loss:2.1722424030303955 val_loss2.1665027141571045\n",
            "iteration 2551: train_loss:2.1717307567596436 val_loss2.165969133377075\n",
            "iteration 2552: train_loss:2.1712191104888916 val_loss2.165435314178467\n",
            "iteration 2553: train_loss:2.1707074642181396 val_loss2.1649014949798584\n",
            "iteration 2554: train_loss:2.1701953411102295 val_loss2.164368152618408\n",
            "iteration 2555: train_loss:2.1696832180023193 val_loss2.163834810256958\n",
            "iteration 2556: train_loss:2.16917085647583 val_loss2.163301467895508\n",
            "iteration 2557: train_loss:2.168658494949341 val_loss2.162768602371216\n",
            "iteration 2558: train_loss:2.1681461334228516 val_loss2.1622354984283447\n",
            "iteration 2559: train_loss:2.1676337718963623 val_loss2.1617023944854736\n",
            "iteration 2560: train_loss:2.167121410369873 val_loss2.1611695289611816\n",
            "iteration 2561: train_loss:2.166609287261963 val_loss2.1606366634368896\n",
            "iteration 2562: train_loss:2.1660962104797363 val_loss2.1601035594940186\n",
            "iteration 2563: train_loss:2.165583610534668 val_loss2.1595706939697266\n",
            "iteration 2564: train_loss:2.1650710105895996 val_loss2.1590375900268555\n",
            "iteration 2565: train_loss:2.164557933807373 val_loss2.1585049629211426\n",
            "iteration 2566: train_loss:2.1640453338623047 val_loss2.1579720973968506\n",
            "iteration 2567: train_loss:2.163532257080078 val_loss2.1574394702911377\n",
            "iteration 2568: train_loss:2.1630189418792725 val_loss2.156906843185425\n",
            "iteration 2569: train_loss:2.162505865097046 val_loss2.156374216079712\n",
            "iteration 2570: train_loss:2.161992311477661 val_loss2.155841827392578\n",
            "iteration 2571: train_loss:2.1614785194396973 val_loss2.1553099155426025\n",
            "iteration 2572: train_loss:2.1609647274017334 val_loss2.154777765274048\n",
            "iteration 2573: train_loss:2.1604506969451904 val_loss2.154245376586914\n",
            "iteration 2574: train_loss:2.1599366664886475 val_loss2.1537134647369385\n",
            "iteration 2575: train_loss:2.1594223976135254 val_loss2.153181791305542\n",
            "iteration 2576: train_loss:2.1589086055755615 val_loss2.1526503562927246\n",
            "iteration 2577: train_loss:2.1583948135375977 val_loss2.1521191596984863\n",
            "iteration 2578: train_loss:2.157881259918213 val_loss2.151587963104248\n",
            "iteration 2579: train_loss:2.15736722946167 val_loss2.1510567665100098\n",
            "iteration 2580: train_loss:2.156853437423706 val_loss2.1505258083343506\n",
            "iteration 2581: train_loss:2.156339168548584 val_loss2.1499950885772705\n",
            "iteration 2582: train_loss:2.15582537651062 val_loss2.1494648456573486\n",
            "iteration 2583: train_loss:2.1553115844726562 val_loss2.148934841156006\n",
            "iteration 2584: train_loss:2.1547975540161133 val_loss2.148404598236084\n",
            "iteration 2585: train_loss:2.1542837619781494 val_loss2.1478748321533203\n",
            "iteration 2586: train_loss:2.1537697315216064 val_loss2.1473448276519775\n",
            "iteration 2587: train_loss:2.1532554626464844 val_loss2.146815776824951\n",
            "iteration 2588: train_loss:2.1527419090270996 val_loss2.146286725997925\n",
            "iteration 2589: train_loss:2.1522281169891357 val_loss2.1457576751708984\n",
            "iteration 2590: train_loss:2.15171480178833 val_loss2.145228862762451\n",
            "iteration 2591: train_loss:2.151201009750366 val_loss2.144700050354004\n",
            "iteration 2592: train_loss:2.1506879329681396 val_loss2.144171714782715\n",
            "iteration 2593: train_loss:2.150174379348755 val_loss2.143643617630005\n",
            "iteration 2594: train_loss:2.149661064147949 val_loss2.143115758895874\n",
            "iteration 2595: train_loss:2.1491477489471436 val_loss2.1425883769989014\n",
            "iteration 2596: train_loss:2.148634672164917 val_loss2.142061233520508\n",
            "iteration 2597: train_loss:2.1481215953826904 val_loss2.1415345668792725\n",
            "iteration 2598: train_loss:2.147608757019043 val_loss2.141007661819458\n",
            "iteration 2599: train_loss:2.1470954418182373 val_loss2.140481472015381\n",
            "iteration 2600: train_loss:2.146582841873169 val_loss2.139955759048462\n",
            "iteration 2601: train_loss:2.1460697650909424 val_loss2.139429807662964\n",
            "iteration 2602: train_loss:2.145556688308716 val_loss2.138904094696045\n",
            "iteration 2603: train_loss:2.14504337310791 val_loss2.138378620147705\n",
            "iteration 2604: train_loss:2.1445305347442627 val_loss2.1378531455993652\n",
            "iteration 2605: train_loss:2.144017219543457 val_loss2.1373281478881836\n",
            "iteration 2606: train_loss:2.1435043811798096 val_loss2.136803150177002\n",
            "iteration 2607: train_loss:2.142991304397583 val_loss2.136277914047241\n",
            "iteration 2608: train_loss:2.1424782276153564 val_loss2.1357533931732178\n",
            "iteration 2609: train_loss:2.141965866088867 val_loss2.1352288722991943\n",
            "iteration 2610: train_loss:2.141453266143799 val_loss2.134704828262329\n",
            "iteration 2611: train_loss:2.1409406661987305 val_loss2.134180784225464\n",
            "iteration 2612: train_loss:2.140427827835083 val_loss2.133657217025757\n",
            "iteration 2613: train_loss:2.1399149894714355 val_loss2.133133888244629\n",
            "iteration 2614: train_loss:2.139401912689209 val_loss2.132610559463501\n",
            "iteration 2615: train_loss:2.1388890743255615 val_loss2.1320877075195312\n",
            "iteration 2616: train_loss:2.138375759124756 val_loss2.1315648555755615\n",
            "iteration 2617: train_loss:2.13786244392395 val_loss2.13104248046875\n",
            "iteration 2618: train_loss:2.1373488903045654 val_loss2.1305203437805176\n",
            "iteration 2619: train_loss:2.1368350982666016 val_loss2.1299984455108643\n",
            "iteration 2620: train_loss:2.1363213062286377 val_loss2.12947678565979\n",
            "iteration 2621: train_loss:2.135807514190674 val_loss2.128955125808716\n",
            "iteration 2622: train_loss:2.1352944374084473 val_loss2.1284337043762207\n",
            "iteration 2623: train_loss:2.1347808837890625 val_loss2.1279125213623047\n",
            "iteration 2624: train_loss:2.1342670917510986 val_loss2.1273915767669678\n",
            "iteration 2625: train_loss:2.1337532997131348 val_loss2.126871109008789\n",
            "iteration 2626: train_loss:2.13323974609375 val_loss2.1263508796691895\n",
            "iteration 2627: train_loss:2.132725954055786 val_loss2.125830888748169\n",
            "iteration 2628: train_loss:2.1322121620178223 val_loss2.1253106594085693\n",
            "iteration 2629: train_loss:2.1316978931427 val_loss2.124790668487549\n",
            "iteration 2630: train_loss:2.131183624267578 val_loss2.1242709159851074\n",
            "iteration 2631: train_loss:2.1306698322296143 val_loss2.123751401901245\n",
            "iteration 2632: train_loss:2.130155563354492 val_loss2.123231887817383\n",
            "iteration 2633: train_loss:2.129641056060791 val_loss2.1227126121520996\n",
            "iteration 2634: train_loss:2.12912654876709 val_loss2.1221938133239746\n",
            "iteration 2635: train_loss:2.1286120414733887 val_loss2.1216747760772705\n",
            "iteration 2636: train_loss:2.1280975341796875 val_loss2.1211557388305664\n",
            "iteration 2637: train_loss:2.1275830268859863 val_loss2.1206371784210205\n",
            "iteration 2638: train_loss:2.127068281173706 val_loss2.1201186180114746\n",
            "iteration 2639: train_loss:2.126553773880005 val_loss2.1196000576019287\n",
            "iteration 2640: train_loss:2.1260390281677246 val_loss2.119081974029541\n",
            "iteration 2641: train_loss:2.1255245208740234 val_loss2.1185638904571533\n",
            "iteration 2642: train_loss:2.125009536743164 val_loss2.1180458068847656\n",
            "iteration 2643: train_loss:2.1244943141937256 val_loss2.117527723312378\n",
            "iteration 2644: train_loss:2.123979091644287 val_loss2.1170096397399902\n",
            "iteration 2645: train_loss:2.1234636306762695 val_loss2.1164915561676025\n",
            "iteration 2646: train_loss:2.122947931289673 val_loss2.115973711013794\n",
            "iteration 2647: train_loss:2.122431993484497 val_loss2.1154558658599854\n",
            "iteration 2648: train_loss:2.121915817260742 val_loss2.1149375438690186\n",
            "iteration 2649: train_loss:2.1213996410369873 val_loss2.114420175552368\n",
            "iteration 2650: train_loss:2.120882987976074 val_loss2.1139025688171387\n",
            "iteration 2651: train_loss:2.1203665733337402 val_loss2.1133854389190674\n",
            "iteration 2652: train_loss:2.1198501586914062 val_loss2.112868547439575\n",
            "iteration 2653: train_loss:2.119333267211914 val_loss2.112351179122925\n",
            "iteration 2654: train_loss:2.11881685256958 val_loss2.1118342876434326\n",
            "iteration 2655: train_loss:2.118299961090088 val_loss2.1113169193267822\n",
            "iteration 2656: train_loss:2.1177830696105957 val_loss2.110799551010132\n",
            "iteration 2657: train_loss:2.1172661781311035 val_loss2.1102819442749023\n",
            "iteration 2658: train_loss:2.116748809814453 val_loss2.109764575958252\n",
            "iteration 2659: train_loss:2.1162314414978027 val_loss2.1092469692230225\n",
            "iteration 2660: train_loss:2.115713596343994 val_loss2.108729362487793\n",
            "iteration 2661: train_loss:2.1151962280273438 val_loss2.1082119941711426\n",
            "iteration 2662: train_loss:2.1146788597106934 val_loss2.107693910598755\n",
            "iteration 2663: train_loss:2.1141607761383057 val_loss2.107175827026367\n",
            "iteration 2664: train_loss:2.113642930984497 val_loss2.1066579818725586\n",
            "iteration 2665: train_loss:2.113124370574951 val_loss2.10614013671875\n",
            "iteration 2666: train_loss:2.1126060485839844 val_loss2.1056222915649414\n",
            "iteration 2667: train_loss:2.1120872497558594 val_loss2.1051042079925537\n",
            "iteration 2668: train_loss:2.1115684509277344 val_loss2.104586124420166\n",
            "iteration 2669: train_loss:2.1110498905181885 val_loss2.1040680408477783\n",
            "iteration 2670: train_loss:2.1105308532714844 val_loss2.1035499572753906\n",
            "iteration 2671: train_loss:2.110011339187622 val_loss2.103031635284424\n",
            "iteration 2672: train_loss:2.1094913482666016 val_loss2.102513074874878\n",
            "iteration 2673: train_loss:2.108971118927002 val_loss2.101994514465332\n",
            "iteration 2674: train_loss:2.1084506511688232 val_loss2.101475954055786\n",
            "iteration 2675: train_loss:2.1079297065734863 val_loss2.100957155227661\n",
            "iteration 2676: train_loss:2.107408285140991 val_loss2.100438356399536\n",
            "iteration 2677: train_loss:2.106886863708496 val_loss2.099919080734253\n",
            "iteration 2678: train_loss:2.1063649654388428 val_loss2.0993995666503906\n",
            "iteration 2679: train_loss:2.1058430671691895 val_loss2.098879814147949\n",
            "iteration 2680: train_loss:2.105320692062378 val_loss2.098360300064087\n",
            "iteration 2681: train_loss:2.1047980785369873 val_loss2.0978403091430664\n",
            "iteration 2682: train_loss:2.1042749881744385 val_loss2.097320318222046\n",
            "iteration 2683: train_loss:2.1037514209747314 val_loss2.0968008041381836\n",
            "iteration 2684: train_loss:2.103227138519287 val_loss2.096281051635742\n",
            "iteration 2685: train_loss:2.1027028560638428 val_loss2.0957608222961426\n",
            "iteration 2686: train_loss:2.1021780967712402 val_loss2.095240592956543\n",
            "iteration 2687: train_loss:2.1016530990600586 val_loss2.0947201251983643\n",
            "iteration 2688: train_loss:2.1011273860931396 val_loss2.0941991806030273\n",
            "iteration 2689: train_loss:2.1006011962890625 val_loss2.0936779975891113\n",
            "iteration 2690: train_loss:2.1000747680664062 val_loss2.0931568145751953\n",
            "iteration 2691: train_loss:2.099548101425171 val_loss2.092634916305542\n",
            "iteration 2692: train_loss:2.0990207195281982 val_loss2.0921130180358887\n",
            "iteration 2693: train_loss:2.0984928607940674 val_loss2.091590642929077\n",
            "iteration 2694: train_loss:2.097964286804199 val_loss2.0910680294036865\n",
            "iteration 2695: train_loss:2.0974349975585938 val_loss2.090545177459717\n",
            "iteration 2696: train_loss:2.09690523147583 val_loss2.090022087097168\n",
            "iteration 2697: train_loss:2.096374750137329 val_loss2.089498996734619\n",
            "iteration 2698: train_loss:2.095844030380249 val_loss2.088975667953491\n",
            "iteration 2699: train_loss:2.0953123569488525 val_loss2.0884511470794678\n",
            "iteration 2700: train_loss:2.094780206680298 val_loss2.0879271030426025\n",
            "iteration 2701: train_loss:2.0942468643188477 val_loss2.08740234375\n",
            "iteration 2702: train_loss:2.0937135219573975 val_loss2.0868771076202393\n",
            "iteration 2703: train_loss:2.093179702758789 val_loss2.086350917816162\n",
            "iteration 2704: train_loss:2.0926449298858643 val_loss2.085824489593506\n",
            "iteration 2705: train_loss:2.092109441757202 val_loss2.0852978229522705\n",
            "iteration 2706: train_loss:2.091573476791382 val_loss2.084770679473877\n",
            "iteration 2707: train_loss:2.091036558151245 val_loss2.084243059158325\n",
            "iteration 2708: train_loss:2.090498924255371 val_loss2.0837149620056152\n",
            "iteration 2709: train_loss:2.0899600982666016 val_loss2.083186149597168\n",
            "iteration 2710: train_loss:2.089420795440674 val_loss2.0826570987701416\n",
            "iteration 2711: train_loss:2.088881254196167 val_loss2.082127332687378\n",
            "iteration 2712: train_loss:2.0883402824401855 val_loss2.081596612930298\n",
            "iteration 2713: train_loss:2.087798595428467 val_loss2.0810658931732178\n",
            "iteration 2714: train_loss:2.0872561931610107 val_loss2.0805346965789795\n",
            "iteration 2715: train_loss:2.086712598800659 val_loss2.080002546310425\n",
            "iteration 2716: train_loss:2.0861682891845703 val_loss2.079469680786133\n",
            "iteration 2717: train_loss:2.085623264312744 val_loss2.0789363384246826\n",
            "iteration 2718: train_loss:2.0850770473480225 val_loss2.078402519226074\n",
            "iteration 2719: train_loss:2.0845301151275635 val_loss2.077868700027466\n",
            "iteration 2720: train_loss:2.083981990814209 val_loss2.077333688735962\n",
            "iteration 2721: train_loss:2.0834333896636963 val_loss2.0767979621887207\n",
            "iteration 2722: train_loss:2.082883596420288 val_loss2.0762617588043213\n",
            "iteration 2723: train_loss:2.0823330879211426 val_loss2.0757246017456055\n",
            "iteration 2724: train_loss:2.0817813873291016 val_loss2.0751869678497314\n",
            "iteration 2725: train_loss:2.081228494644165 val_loss2.07464861869812\n",
            "iteration 2726: train_loss:2.080674886703491 val_loss2.0741097927093506\n",
            "iteration 2727: train_loss:2.0801198482513428 val_loss2.0735700130462646\n",
            "iteration 2728: train_loss:2.079564094543457 val_loss2.0730292797088623\n",
            "iteration 2729: train_loss:2.079007148742676 val_loss2.0724878311157227\n",
            "iteration 2730: train_loss:2.078449010848999 val_loss2.0719456672668457\n",
            "iteration 2731: train_loss:2.077889919281006 val_loss2.0714027881622314\n",
            "iteration 2732: train_loss:2.077329635620117 val_loss2.070858955383301\n",
            "iteration 2733: train_loss:2.076768398284912 val_loss2.070314645767212\n",
            "iteration 2734: train_loss:2.0762057304382324 val_loss2.0697691440582275\n",
            "iteration 2735: train_loss:2.075641632080078 val_loss2.069223165512085\n",
            "iteration 2736: train_loss:2.0750763416290283 val_loss2.068676471710205\n",
            "iteration 2737: train_loss:2.074509859085083 val_loss2.068128824234009\n",
            "iteration 2738: train_loss:2.073942184448242 val_loss2.067579984664917\n",
            "iteration 2739: train_loss:2.073373317718506 val_loss2.067030191421509\n",
            "iteration 2740: train_loss:2.072803020477295 val_loss2.066479444503784\n",
            "iteration 2741: train_loss:2.0722310543060303 val_loss2.0659279823303223\n",
            "iteration 2742: train_loss:2.07165789604187 val_loss2.0653750896453857\n",
            "iteration 2743: train_loss:2.0710833072662354 val_loss2.06482195854187\n",
            "iteration 2744: train_loss:2.070507287979126 val_loss2.064267635345459\n",
            "iteration 2745: train_loss:2.069929838180542 val_loss2.0637123584747314\n",
            "iteration 2746: train_loss:2.0693511962890625 val_loss2.0631561279296875\n",
            "iteration 2747: train_loss:2.0687708854675293 val_loss2.062598705291748\n",
            "iteration 2748: train_loss:2.0681893825531006 val_loss2.062039852142334\n",
            "iteration 2749: train_loss:2.0676069259643555 val_loss2.0614800453186035\n",
            "iteration 2750: train_loss:2.0670230388641357 val_loss2.0609188079833984\n",
            "iteration 2751: train_loss:2.0664374828338623 val_loss2.060356616973877\n",
            "iteration 2752: train_loss:2.065850019454956 val_loss2.059793710708618\n",
            "iteration 2753: train_loss:2.0652616024017334 val_loss2.0592291355133057\n",
            "iteration 2754: train_loss:2.064671516418457 val_loss2.0586636066436768\n",
            "iteration 2755: train_loss:2.064080238342285 val_loss2.058096408843994\n",
            "iteration 2756: train_loss:2.0634872913360596 val_loss2.057528257369995\n",
            "iteration 2757: train_loss:2.062892198562622 val_loss2.0569589138031006\n",
            "iteration 2758: train_loss:2.062296152114868 val_loss2.0563881397247314\n",
            "iteration 2759: train_loss:2.0616979598999023 val_loss2.055816411972046\n",
            "iteration 2760: train_loss:2.061098098754883 val_loss2.055243492126465\n",
            "iteration 2761: train_loss:2.0604965686798096 val_loss2.054669141769409\n",
            "iteration 2762: train_loss:2.0598931312561035 val_loss2.0540931224823\n",
            "iteration 2763: train_loss:2.059288501739502 val_loss2.053516149520874\n",
            "iteration 2764: train_loss:2.0586817264556885 val_loss2.0529377460479736\n",
            "iteration 2765: train_loss:2.0580732822418213 val_loss2.0523574352264404\n",
            "iteration 2766: train_loss:2.0574631690979004 val_loss2.051776170730591\n",
            "iteration 2767: train_loss:2.0568511486053467 val_loss2.0511927604675293\n",
            "iteration 2768: train_loss:2.05623722076416 val_loss2.0506083965301514\n",
            "iteration 2769: train_loss:2.055621862411499 val_loss2.0500221252441406\n",
            "iteration 2770: train_loss:2.055004119873047 val_loss2.0494346618652344\n",
            "iteration 2771: train_loss:2.054384708404541 val_loss2.0488457679748535\n",
            "iteration 2772: train_loss:2.053762912750244 val_loss2.048255443572998\n",
            "iteration 2773: train_loss:2.0531394481658936 val_loss2.0476632118225098\n",
            "iteration 2774: train_loss:2.052513837814331 val_loss2.047070026397705\n",
            "iteration 2775: train_loss:2.051886558532715 val_loss2.0464749336242676\n",
            "iteration 2776: train_loss:2.051257371902466 val_loss2.0458781719207764\n",
            "iteration 2777: train_loss:2.0506253242492676 val_loss2.0452797412872314\n",
            "iteration 2778: train_loss:2.049992322921753 val_loss2.044679641723633\n",
            "iteration 2779: train_loss:2.0493571758270264 val_loss2.0440773963928223\n",
            "iteration 2780: train_loss:2.048719644546509 val_loss2.043473482131958\n",
            "iteration 2781: train_loss:2.0480797290802 val_loss2.042868137359619\n",
            "iteration 2782: train_loss:2.047438144683838 val_loss2.0422606468200684\n",
            "iteration 2783: train_loss:2.046794891357422 val_loss2.041651487350464\n",
            "iteration 2784: train_loss:2.0461490154266357 val_loss2.0410406589508057\n",
            "iteration 2785: train_loss:2.045501470565796 val_loss2.0404276847839355\n",
            "iteration 2786: train_loss:2.044851779937744 val_loss2.0398130416870117\n",
            "iteration 2787: train_loss:2.0441997051239014 val_loss2.039196252822876\n",
            "iteration 2788: train_loss:2.043545722961426 val_loss2.0385777950286865\n",
            "iteration 2789: train_loss:2.042889356613159 val_loss2.0379574298858643\n",
            "iteration 2790: train_loss:2.0422306060791016 val_loss2.0373353958129883\n",
            "iteration 2791: train_loss:2.041569471359253 val_loss2.0367114543914795\n",
            "iteration 2792: train_loss:2.0409064292907715 val_loss2.036085605621338\n",
            "iteration 2793: train_loss:2.040240526199341 val_loss2.0354573726654053\n",
            "iteration 2794: train_loss:2.0395727157592773 val_loss2.034827470779419\n",
            "iteration 2795: train_loss:2.038902521133423 val_loss2.0341951847076416\n",
            "iteration 2796: train_loss:2.038229465484619 val_loss2.0335609912872314\n",
            "iteration 2797: train_loss:2.0375545024871826 val_loss2.0329248905181885\n",
            "iteration 2798: train_loss:2.036877155303955 val_loss2.0322864055633545\n",
            "iteration 2799: train_loss:2.0361974239349365 val_loss2.0316457748413086\n",
            "iteration 2800: train_loss:2.0355148315429688 val_loss2.03100323677063\n",
            "iteration 2801: train_loss:2.034830093383789 val_loss2.030358076095581\n",
            "iteration 2802: train_loss:2.0341429710388184 val_loss2.0297110080718994\n",
            "iteration 2803: train_loss:2.0334529876708984 val_loss2.029061794281006\n",
            "iteration 2804: train_loss:2.0327606201171875 val_loss2.028409957885742\n",
            "iteration 2805: train_loss:2.0320661067962646 val_loss2.0277554988861084\n",
            "iteration 2806: train_loss:2.0313689708709717 val_loss2.027099132537842\n",
            "iteration 2807: train_loss:2.0306692123413086 val_loss2.026440143585205\n",
            "iteration 2808: train_loss:2.0299668312072754 val_loss2.0257792472839355\n",
            "iteration 2809: train_loss:2.029261589050293 val_loss2.025115966796875\n",
            "iteration 2810: train_loss:2.0285537242889404 val_loss2.0244505405426025\n",
            "iteration 2811: train_loss:2.0278432369232178 val_loss2.023782253265381\n",
            "iteration 2812: train_loss:2.027129888534546 val_loss2.0231120586395264\n",
            "iteration 2813: train_loss:2.026414155960083 val_loss2.022439479827881\n",
            "iteration 2814: train_loss:2.0256950855255127 val_loss2.0217645168304443\n",
            "iteration 2815: train_loss:2.0249736309051514 val_loss2.021087169647217\n",
            "iteration 2816: train_loss:2.0242490768432617 val_loss2.02040696144104\n",
            "iteration 2817: train_loss:2.023521661758423 val_loss2.0197246074676514\n",
            "iteration 2818: train_loss:2.022791862487793 val_loss2.0190398693084717\n",
            "iteration 2819: train_loss:2.0220584869384766 val_loss2.018352508544922\n",
            "iteration 2820: train_loss:2.021322727203369 val_loss2.017662763595581\n",
            "iteration 2821: train_loss:2.0205838680267334 val_loss2.01697039604187\n",
            "iteration 2822: train_loss:2.0198419094085693 val_loss2.016275644302368\n",
            "iteration 2823: train_loss:2.019096851348877 val_loss2.015578269958496\n",
            "iteration 2824: train_loss:2.0183486938476562 val_loss2.014878511428833\n",
            "iteration 2825: train_loss:2.0175976753234863 val_loss2.0141761302948\n",
            "iteration 2826: train_loss:2.016843557357788 val_loss2.0134706497192383\n",
            "iteration 2827: train_loss:2.0160861015319824 val_loss2.012763023376465\n",
            "iteration 2828: train_loss:2.0153257846832275 val_loss2.012052536010742\n",
            "iteration 2829: train_loss:2.014561891555786 val_loss2.0113391876220703\n",
            "iteration 2830: train_loss:2.0137951374053955 val_loss2.0106234550476074\n",
            "iteration 2831: train_loss:2.0130252838134766 val_loss2.0099048614501953\n",
            "iteration 2832: train_loss:2.012251615524292 val_loss2.009183645248413\n",
            "iteration 2833: train_loss:2.011474847793579 val_loss2.0084598064422607\n",
            "iteration 2834: train_loss:2.010695219039917 val_loss2.007733106613159\n",
            "iteration 2835: train_loss:2.0099120140075684 val_loss2.0070033073425293\n",
            "iteration 2836: train_loss:2.0091257095336914 val_loss2.00627064704895\n",
            "iteration 2837: train_loss:2.008336067199707 val_loss2.005535125732422\n",
            "iteration 2838: train_loss:2.0075430870056152 val_loss2.0047972202301025\n",
            "iteration 2839: train_loss:2.006746768951416 val_loss2.004056215286255\n",
            "iteration 2840: train_loss:2.0059468746185303 val_loss2.003312110900879\n",
            "iteration 2841: train_loss:2.005143880844116 val_loss2.0025649070739746\n",
            "iteration 2842: train_loss:2.004337787628174 val_loss2.001814842224121\n",
            "iteration 2843: train_loss:2.003527879714966 val_loss2.0010619163513184\n",
            "iteration 2844: train_loss:2.0027143955230713 val_loss2.0003058910369873\n",
            "iteration 2845: train_loss:2.0018980503082275 val_loss1.9995468854904175\n",
            "iteration 2846: train_loss:2.001077890396118 val_loss1.9987846612930298\n",
            "iteration 2847: train_loss:2.0002543926239014 val_loss1.998019814491272\n",
            "iteration 2848: train_loss:1.9994274377822876 val_loss1.9972516298294067\n",
            "iteration 2849: train_loss:1.9985966682434082 val_loss1.9964808225631714\n",
            "iteration 2850: train_loss:1.9977625608444214 val_loss1.9957064390182495\n",
            "iteration 2851: train_loss:1.9969252347946167 val_loss1.994929313659668\n",
            "iteration 2852: train_loss:1.9960839748382568 val_loss1.9941487312316895\n",
            "iteration 2853: train_loss:1.9952393770217896 val_loss1.9933650493621826\n",
            "iteration 2854: train_loss:1.9943912029266357 val_loss1.9925779104232788\n",
            "iteration 2855: train_loss:1.9935396909713745 val_loss1.9917880296707153\n",
            "iteration 2856: train_loss:1.992684245109558 val_loss1.990994930267334\n",
            "iteration 2857: train_loss:1.991824984550476 val_loss1.9901983737945557\n",
            "iteration 2858: train_loss:1.9909625053405762 val_loss1.989398717880249\n",
            "iteration 2859: train_loss:1.9900959730148315 val_loss1.9885960817337036\n",
            "iteration 2860: train_loss:1.9892261028289795 val_loss1.9877899885177612\n",
            "iteration 2861: train_loss:1.9883522987365723 val_loss1.9869807958602905\n",
            "iteration 2862: train_loss:1.9874749183654785 val_loss1.986168622970581\n",
            "iteration 2863: train_loss:1.98659348487854 val_loss1.9853532314300537\n",
            "iteration 2864: train_loss:1.9857087135314941 val_loss1.984534502029419\n",
            "iteration 2865: train_loss:1.984820008277893 val_loss1.9837126731872559\n",
            "iteration 2866: train_loss:1.9839274883270264 val_loss1.9828873872756958\n",
            "iteration 2867: train_loss:1.9830313920974731 val_loss1.9820592403411865\n",
            "iteration 2868: train_loss:1.9821313619613647 val_loss1.9812273979187012\n",
            "iteration 2869: train_loss:1.9812273979187012 val_loss1.980392575263977\n",
            "iteration 2870: train_loss:1.9803199768066406 val_loss1.979554295539856\n",
            "iteration 2871: train_loss:1.979408621788025 val_loss1.9787131547927856\n",
            "iteration 2872: train_loss:1.978493332862854 val_loss1.9778683185577393\n",
            "iteration 2873: train_loss:1.977574348449707 val_loss1.9770203828811646\n",
            "iteration 2874: train_loss:1.9766514301300049 val_loss1.976169228553772\n",
            "iteration 2875: train_loss:1.975724697113037 val_loss1.9753150939941406\n",
            "iteration 2876: train_loss:1.9747943878173828 val_loss1.9744575023651123\n",
            "iteration 2877: train_loss:1.973860502243042 val_loss1.9735966920852661\n",
            "iteration 2878: train_loss:1.9729225635528564 val_loss1.9727325439453125\n",
            "iteration 2879: train_loss:1.9719808101654053 val_loss1.971865177154541\n",
            "iteration 2880: train_loss:1.971035361289978 val_loss1.9709941148757935\n",
            "iteration 2881: train_loss:1.9700859785079956 val_loss1.970119833946228\n",
            "iteration 2882: train_loss:1.9691330194473267 val_loss1.969241976737976\n",
            "iteration 2883: train_loss:1.9681758880615234 val_loss1.9683610200881958\n",
            "iteration 2884: train_loss:1.9672151803970337 val_loss1.967476725578308\n",
            "iteration 2885: train_loss:1.9662506580352783 val_loss1.9665888547897339\n",
            "iteration 2886: train_loss:1.9652823209762573 val_loss1.9656976461410522\n",
            "iteration 2887: train_loss:1.9643100500106812 val_loss1.9648033380508423\n",
            "iteration 2888: train_loss:1.9633342027664185 val_loss1.9639055728912354\n",
            "iteration 2889: train_loss:1.9623546600341797 val_loss1.9630053043365479\n",
            "iteration 2890: train_loss:1.9613713026046753 val_loss1.9621013402938843\n",
            "iteration 2891: train_loss:1.9603841304779053 val_loss1.9611936807632446\n",
            "iteration 2892: train_loss:1.95939302444458 val_loss1.9602829217910767\n",
            "iteration 2893: train_loss:1.958398461341858 val_loss1.9593689441680908\n",
            "iteration 2894: train_loss:1.9574000835418701 val_loss1.958451747894287\n",
            "iteration 2895: train_loss:1.9563980102539062 val_loss1.957531213760376\n",
            "iteration 2896: train_loss:1.9553920030593872 val_loss1.956607460975647\n",
            "iteration 2897: train_loss:1.954382300376892 val_loss1.955680012702942\n",
            "iteration 2898: train_loss:1.953368902206421 val_loss1.954749345779419\n",
            "iteration 2899: train_loss:1.952351450920105 val_loss1.9538153409957886\n",
            "iteration 2900: train_loss:1.9513306617736816 val_loss1.9528778791427612\n",
            "iteration 2901: train_loss:1.9503059387207031 val_loss1.951936960220337\n",
            "iteration 2902: train_loss:1.9492775201797485 val_loss1.950993299484253\n",
            "iteration 2903: train_loss:1.948245644569397 val_loss1.950046420097351\n",
            "iteration 2904: train_loss:1.9472099542617798 val_loss1.9490959644317627\n",
            "iteration 2905: train_loss:1.946170449256897 val_loss1.9481422901153564\n",
            "iteration 2906: train_loss:1.945127010345459 val_loss1.9471852779388428\n",
            "iteration 2907: train_loss:1.9440804719924927 val_loss1.9462252855300903\n",
            "iteration 2908: train_loss:1.9430301189422607 val_loss1.9452619552612305\n",
            "iteration 2909: train_loss:1.9419763088226318 val_loss1.9442957639694214\n",
            "iteration 2910: train_loss:1.9409188032150269 val_loss1.9433258771896362\n",
            "iteration 2911: train_loss:1.939857840538025 val_loss1.9423532485961914\n",
            "iteration 2912: train_loss:1.9387935400009155 val_loss1.9413774013519287\n",
            "iteration 2913: train_loss:1.9377259016036987 val_loss1.940398931503296\n",
            "iteration 2914: train_loss:1.9366545677185059 val_loss1.9394173622131348\n",
            "iteration 2915: train_loss:1.9355798959732056 val_loss1.9384325742721558\n",
            "iteration 2916: train_loss:1.93450129032135 val_loss1.937445044517517\n",
            "iteration 2917: train_loss:1.9334198236465454 val_loss1.9364546537399292\n",
            "iteration 2918: train_loss:1.9323346614837646 val_loss1.935461401939392\n",
            "iteration 2919: train_loss:1.931246280670166 val_loss1.9344650506973267\n",
            "iteration 2920: train_loss:1.9301546812057495 val_loss1.933465838432312\n",
            "iteration 2921: train_loss:1.9290595054626465 val_loss1.932463526725769\n",
            "iteration 2922: train_loss:1.927960753440857 val_loss1.9314581155776978\n",
            "iteration 2923: train_loss:1.9268590211868286 val_loss1.9304498434066772\n",
            "iteration 2924: train_loss:1.925754189491272 val_loss1.9294381141662598\n",
            "iteration 2925: train_loss:1.9246459007263184 val_loss1.9284240007400513\n",
            "iteration 2926: train_loss:1.923534631729126 val_loss1.9274064302444458\n",
            "iteration 2927: train_loss:1.9224203824996948 val_loss1.926386833190918\n",
            "iteration 2928: train_loss:1.9213032722473145 val_loss1.9253640174865723\n",
            "iteration 2929: train_loss:1.9201828241348267 val_loss1.924338698387146\n",
            "iteration 2930: train_loss:1.9190596342086792 val_loss1.9233100414276123\n",
            "iteration 2931: train_loss:1.917933464050293 val_loss1.9222798347473145\n",
            "iteration 2932: train_loss:1.9168044328689575 val_loss1.9212467670440674\n",
            "iteration 2933: train_loss:1.9156724214553833 val_loss1.9202111959457397\n",
            "iteration 2934: train_loss:1.9145373106002808 val_loss1.9191722869873047\n",
            "iteration 2935: train_loss:1.9133994579315186 val_loss1.91813063621521\n",
            "iteration 2936: train_loss:1.9122588634490967 val_loss1.9170864820480347\n",
            "iteration 2937: train_loss:1.9111154079437256 val_loss1.9160395860671997\n",
            "iteration 2938: train_loss:1.9099692106246948 val_loss1.9149905443191528\n",
            "iteration 2939: train_loss:1.9088205099105835 val_loss1.913939356803894\n",
            "iteration 2940: train_loss:1.9076693058013916 val_loss1.912886142730713\n",
            "iteration 2941: train_loss:1.9065154790878296 val_loss1.9118304252624512\n",
            "iteration 2942: train_loss:1.905358910560608 val_loss1.9107720851898193\n",
            "iteration 2943: train_loss:1.9041997194290161 val_loss1.909711480140686\n",
            "iteration 2944: train_loss:1.9030381441116333 val_loss1.9086483716964722\n",
            "iteration 2945: train_loss:1.9018739461898804 val_loss1.9075828790664673\n",
            "iteration 2946: train_loss:1.900707483291626 val_loss1.9065154790878296\n",
            "iteration 2947: train_loss:1.899538516998291 val_loss1.9054456949234009\n",
            "iteration 2948: train_loss:1.8983674049377441 val_loss1.9043740034103394\n",
            "iteration 2949: train_loss:1.8971940279006958 val_loss1.903300166130066\n",
            "iteration 2950: train_loss:1.8960189819335938 val_loss1.9022246599197388\n",
            "iteration 2951: train_loss:1.894841194152832 val_loss1.9011470079421997\n",
            "iteration 2952: train_loss:1.8936612606048584 val_loss1.9000670909881592\n",
            "iteration 2953: train_loss:1.892479419708252 val_loss1.8989852666854858\n",
            "iteration 2954: train_loss:1.891295313835144 val_loss1.897901177406311\n",
            "iteration 2955: train_loss:1.8901091814041138 val_loss1.896815299987793\n",
            "iteration 2956: train_loss:1.8889211416244507 val_loss1.8957271575927734\n",
            "iteration 2957: train_loss:1.8877310752868652 val_loss1.8946375846862793\n",
            "iteration 2958: train_loss:1.886539340019226 val_loss1.8935467004776\n",
            "iteration 2959: train_loss:1.885345458984375 val_loss1.8924534320831299\n",
            "iteration 2960: train_loss:1.8841499090194702 val_loss1.8913582563400269\n",
            "iteration 2961: train_loss:1.8829524517059326 val_loss1.8902617692947388\n",
            "iteration 2962: train_loss:1.8817533254623413 val_loss1.8891637325286865\n",
            "iteration 2963: train_loss:1.8805525302886963 val_loss1.8880631923675537\n",
            "iteration 2964: train_loss:1.8793494701385498 val_loss1.886960744857788\n",
            "iteration 2965: train_loss:1.8781452178955078 val_loss1.8858563899993896\n",
            "iteration 2966: train_loss:1.8769396543502808 val_loss1.8847512006759644\n",
            "iteration 2967: train_loss:1.875732183456421 val_loss1.8836437463760376\n",
            "iteration 2968: train_loss:1.8745228052139282 val_loss1.8825350999832153\n",
            "iteration 2969: train_loss:1.873311996459961 val_loss1.8814247846603394\n",
            "iteration 2970: train_loss:1.8721001148223877 val_loss1.8803141117095947\n",
            "iteration 2971: train_loss:1.870885968208313 val_loss1.8792011737823486\n",
            "iteration 2972: train_loss:1.8696708679199219 val_loss1.878087043762207\n",
            "iteration 2973: train_loss:1.8684537410736084 val_loss1.8769713640213013\n",
            "iteration 2974: train_loss:1.8672356605529785 val_loss1.875854253768921\n",
            "iteration 2975: train_loss:1.8660156726837158 val_loss1.8747358322143555\n",
            "iteration 2976: train_loss:1.8647947311401367 val_loss1.8736166954040527\n",
            "iteration 2977: train_loss:1.8635728359222412 val_loss1.8724963665008545\n",
            "iteration 2978: train_loss:1.8623491525650024 val_loss1.8713740110397339\n",
            "iteration 2979: train_loss:1.8611245155334473 val_loss1.8702505826950073\n",
            "iteration 2980: train_loss:1.8598991632461548 val_loss1.8691262006759644\n",
            "iteration 2981: train_loss:1.8586719036102295 val_loss1.8680002689361572\n",
            "iteration 2982: train_loss:1.857444405555725 val_loss1.8668733835220337\n",
            "iteration 2983: train_loss:1.8562153577804565 val_loss1.8657455444335938\n",
            "iteration 2984: train_loss:1.8549857139587402 val_loss1.8646169900894165\n",
            "iteration 2985: train_loss:1.8537553548812866 val_loss1.8634884357452393\n",
            "iteration 2986: train_loss:1.8525240421295166 val_loss1.862358808517456\n",
            "iteration 2987: train_loss:1.8512910604476929 val_loss1.86122727394104\n",
            "iteration 2988: train_loss:1.8500573635101318 val_loss1.8600940704345703\n",
            "iteration 2989: train_loss:1.8488225936889648 val_loss1.8589600324630737\n",
            "iteration 2990: train_loss:1.8475871086120605 val_loss1.8578259944915771\n",
            "iteration 2991: train_loss:1.8463505506515503 val_loss1.8566910028457642\n",
            "iteration 2992: train_loss:1.8451136350631714 val_loss1.8555554151535034\n",
            "iteration 2993: train_loss:1.8438752889633179 val_loss1.8544191122055054\n",
            "iteration 2994: train_loss:1.8426364660263062 val_loss1.8532822132110596\n",
            "iteration 2995: train_loss:1.8413974046707153 val_loss1.852144479751587\n",
            "iteration 2996: train_loss:1.840157389640808 val_loss1.8510055541992188\n",
            "iteration 2997: train_loss:1.8389172554016113 val_loss1.8498667478561401\n",
            "iteration 2998: train_loss:1.8376765251159668 val_loss1.848728060722351\n",
            "iteration 2999: train_loss:1.836435079574585 val_loss1.847588062286377\n",
            "iteration 3000: train_loss:1.835193157196045 val_loss1.8464471101760864\n",
            "iteration 3001: train_loss:1.8339508771896362 val_loss1.8453060388565063\n",
            "iteration 3002: train_loss:1.832708477973938 val_loss1.8441648483276367\n",
            "iteration 3003: train_loss:1.8314656019210815 val_loss1.8430231809616089\n",
            "iteration 3004: train_loss:1.8302215337753296 val_loss1.8418800830841064\n",
            "iteration 3005: train_loss:1.8289779424667358 val_loss1.8407366275787354\n",
            "iteration 3006: train_loss:1.8277342319488525 val_loss1.8395931720733643\n",
            "iteration 3007: train_loss:1.8264901638031006 val_loss1.8384493589401245\n",
            "iteration 3008: train_loss:1.825245976448059 val_loss1.8373050689697266\n",
            "iteration 3009: train_loss:1.8240010738372803 val_loss1.8361603021621704\n",
            "iteration 3010: train_loss:1.8227561712265015 val_loss1.8350151777267456\n",
            "iteration 3011: train_loss:1.821511149406433 val_loss1.8338699340820312\n",
            "iteration 3012: train_loss:1.8202654123306274 val_loss1.8327242136001587\n",
            "iteration 3013: train_loss:1.8190197944641113 val_loss1.831578016281128\n",
            "iteration 3014: train_loss:1.8177739381790161 val_loss1.830431342124939\n",
            "iteration 3015: train_loss:1.8165276050567627 val_loss1.8292840719223022\n",
            "iteration 3016: train_loss:1.8152812719345093 val_loss1.8281365633010864\n",
            "iteration 3017: train_loss:1.8140344619750977 val_loss1.8269891738891602\n",
            "iteration 3018: train_loss:1.8127878904342651 val_loss1.8258413076400757\n",
            "iteration 3019: train_loss:1.811541199684143 val_loss1.8246939182281494\n",
            "iteration 3020: train_loss:1.810294270515442 val_loss1.8235455751419067\n",
            "iteration 3021: train_loss:1.809046983718872 val_loss1.8223968744277954\n",
            "iteration 3022: train_loss:1.8077998161315918 val_loss1.821248173713684\n",
            "iteration 3023: train_loss:1.8065522909164429 val_loss1.8200989961624146\n",
            "iteration 3024: train_loss:1.8053051233291626 val_loss1.8189500570297241\n",
            "iteration 3025: train_loss:1.8040579557418823 val_loss1.817801594734192\n",
            "iteration 3026: train_loss:1.8028110265731812 val_loss1.8166532516479492\n",
            "iteration 3027: train_loss:1.801564335823059 val_loss1.8155051469802856\n",
            "iteration 3028: train_loss:1.800317406654358 val_loss1.814356803894043\n",
            "iteration 3029: train_loss:1.7990705966949463 val_loss1.8132084608078003\n",
            "iteration 3030: train_loss:1.7978235483169556 val_loss1.8120594024658203\n",
            "iteration 3031: train_loss:1.7965770959854126 val_loss1.810910701751709\n",
            "iteration 3032: train_loss:1.79533052444458 val_loss1.809761643409729\n",
            "iteration 3033: train_loss:1.794084072113037 val_loss1.8086124658584595\n",
            "iteration 3034: train_loss:1.7928376197814941 val_loss1.8074629306793213\n",
            "iteration 3035: train_loss:1.7915910482406616 val_loss1.8063132762908936\n",
            "iteration 3036: train_loss:1.7903447151184082 val_loss1.8051637411117554\n",
            "iteration 3037: train_loss:1.7890981435775757 val_loss1.804013729095459\n",
            "iteration 3038: train_loss:1.7878514528274536 val_loss1.802863359451294\n",
            "iteration 3039: train_loss:1.7866053581237793 val_loss1.8017141819000244\n",
            "iteration 3040: train_loss:1.785359263420105 val_loss1.8005645275115967\n",
            "iteration 3041: train_loss:1.7841137647628784 val_loss1.799414873123169\n",
            "iteration 3042: train_loss:1.7828683853149414 val_loss1.7982654571533203\n",
            "iteration 3043: train_loss:1.7816228866577148 val_loss1.7971166372299194\n",
            "iteration 3044: train_loss:1.7803778648376465 val_loss1.79596745967865\n",
            "iteration 3045: train_loss:1.7791332006454468 val_loss1.794818639755249\n",
            "iteration 3046: train_loss:1.7778890132904053 val_loss1.7936702966690063\n",
            "iteration 3047: train_loss:1.7766444683074951 val_loss1.7925208806991577\n",
            "iteration 3048: train_loss:1.7754000425338745 val_loss1.7913720607757568\n",
            "iteration 3049: train_loss:1.7741559743881226 val_loss1.790223240852356\n",
            "iteration 3050: train_loss:1.7729123830795288 val_loss1.7890739440917969\n",
            "iteration 3051: train_loss:1.7716686725616455 val_loss1.7879256010055542\n",
            "iteration 3052: train_loss:1.770425796508789 val_loss1.7867767810821533\n",
            "iteration 3053: train_loss:1.7691829204559326 val_loss1.785628318786621\n",
            "iteration 3054: train_loss:1.7679401636123657 val_loss1.7844798564910889\n",
            "iteration 3055: train_loss:1.7666980028152466 val_loss1.7833316326141357\n",
            "iteration 3056: train_loss:1.7654558420181274 val_loss1.782183051109314\n",
            "iteration 3057: train_loss:1.7642143964767456 val_loss1.7810341119766235\n",
            "iteration 3058: train_loss:1.7629731893539429 val_loss1.77988600730896\n",
            "iteration 3059: train_loss:1.7617329359054565 val_loss1.7787389755249023\n",
            "iteration 3060: train_loss:1.7604929208755493 val_loss1.7775917053222656\n",
            "iteration 3061: train_loss:1.7592530250549316 val_loss1.7764441967010498\n",
            "iteration 3062: train_loss:1.758013367652893 val_loss1.7752965688705444\n",
            "iteration 3063: train_loss:1.7567737102508545 val_loss1.7741485834121704\n",
            "iteration 3064: train_loss:1.755534291267395 val_loss1.773000717163086\n",
            "iteration 3065: train_loss:1.7542952299118042 val_loss1.7718524932861328\n",
            "iteration 3066: train_loss:1.753056287765503 val_loss1.770704746246338\n",
            "iteration 3067: train_loss:1.7518181800842285 val_loss1.769557237625122\n",
            "iteration 3068: train_loss:1.7505799531936646 val_loss1.7684099674224854\n",
            "iteration 3069: train_loss:1.7493417263031006 val_loss1.7672619819641113\n",
            "iteration 3070: train_loss:1.7481036186218262 val_loss1.7661141157150269\n",
            "iteration 3071: train_loss:1.74686598777771 val_loss1.764966607093811\n",
            "iteration 3072: train_loss:1.7456284761428833 val_loss1.7638193368911743\n",
            "iteration 3073: train_loss:1.7443912029266357 val_loss1.762671709060669\n",
            "iteration 3074: train_loss:1.743153691291809 val_loss1.7615243196487427\n",
            "iteration 3075: train_loss:1.7419161796569824 val_loss1.760375738143921\n",
            "iteration 3076: train_loss:1.7406786680221558 val_loss1.7592270374298096\n",
            "iteration 3077: train_loss:1.7394416332244873 val_loss1.7580784559249878\n",
            "iteration 3078: train_loss:1.7382049560546875 val_loss1.7569299936294556\n",
            "iteration 3079: train_loss:1.7369685173034668 val_loss1.7557812929153442\n",
            "iteration 3080: train_loss:1.7357319593429565 val_loss1.7546321153640747\n",
            "iteration 3081: train_loss:1.7344961166381836 val_loss1.753483772277832\n",
            "iteration 3082: train_loss:1.7332606315612793 val_loss1.7523359060287476\n",
            "iteration 3083: train_loss:1.732025146484375 val_loss1.7511874437332153\n",
            "iteration 3084: train_loss:1.7307898998260498 val_loss1.750038743019104\n",
            "iteration 3085: train_loss:1.7295548915863037 val_loss1.74889075756073\n",
            "iteration 3086: train_loss:1.7283198833465576 val_loss1.7477424144744873\n",
            "iteration 3087: train_loss:1.7270853519439697 val_loss1.7465933561325073\n",
            "iteration 3088: train_loss:1.7258505821228027 val_loss1.7454445362091064\n",
            "iteration 3089: train_loss:1.724616289138794 val_loss1.7442961931228638\n",
            "iteration 3090: train_loss:1.7233818769454956 val_loss1.743147373199463\n",
            "iteration 3091: train_loss:1.7221475839614868 val_loss1.741998314857483\n",
            "iteration 3092: train_loss:1.7209131717681885 val_loss1.7408497333526611\n",
            "iteration 3093: train_loss:1.7196788787841797 val_loss1.739700198173523\n",
            "iteration 3094: train_loss:1.7184442281723022 val_loss1.7385506629943848\n",
            "iteration 3095: train_loss:1.717208743095398 val_loss1.7373987436294556\n",
            "iteration 3096: train_loss:1.715973138809204 val_loss1.7362464666366577\n",
            "iteration 3097: train_loss:1.7147374153137207 val_loss1.7350940704345703\n",
            "iteration 3098: train_loss:1.7135015726089478 val_loss1.7339415550231934\n",
            "iteration 3099: train_loss:1.7122650146484375 val_loss1.7327889204025269\n",
            "iteration 3100: train_loss:1.7110285758972168 val_loss1.7316358089447021\n",
            "iteration 3101: train_loss:1.7097917795181274 val_loss1.730483055114746\n",
            "iteration 3102: train_loss:1.7085548639297485 val_loss1.729329228401184\n",
            "iteration 3103: train_loss:1.7073173522949219 val_loss1.7281743288040161\n",
            "iteration 3104: train_loss:1.7060798406600952 val_loss1.7270199060440063\n",
            "iteration 3105: train_loss:1.7048417329788208 val_loss1.725864052772522\n",
            "iteration 3106: train_loss:1.703603744506836 val_loss1.7247087955474854\n",
            "iteration 3107: train_loss:1.7023652791976929 val_loss1.7235538959503174\n",
            "iteration 3108: train_loss:1.7011268138885498 val_loss1.7223986387252808\n",
            "iteration 3109: train_loss:1.6998881101608276 val_loss1.721242904663086\n",
            "iteration 3110: train_loss:1.6986494064331055 val_loss1.7200872898101807\n",
            "iteration 3111: train_loss:1.6974107027053833 val_loss1.7189290523529053\n",
            "iteration 3112: train_loss:1.6961712837219238 val_loss1.7177704572677612\n",
            "iteration 3113: train_loss:1.6949318647384644 val_loss1.716611385345459\n",
            "iteration 3114: train_loss:1.6936920881271362 val_loss1.715451955795288\n",
            "iteration 3115: train_loss:1.6924521923065186 val_loss1.7142926454544067\n",
            "iteration 3116: train_loss:1.6912115812301636 val_loss1.7131327390670776\n",
            "iteration 3117: train_loss:1.6899709701538086 val_loss1.7119723558425903\n",
            "iteration 3118: train_loss:1.688728928565979 val_loss1.7108107805252075\n",
            "iteration 3119: train_loss:1.6874868869781494 val_loss1.709648609161377\n",
            "iteration 3120: train_loss:1.6862443685531616 val_loss1.708486557006836\n",
            "iteration 3121: train_loss:1.6850014925003052 val_loss1.7073240280151367\n",
            "iteration 3122: train_loss:1.6837581396102905 val_loss1.706160306930542\n",
            "iteration 3123: train_loss:1.6825138330459595 val_loss1.704996109008789\n",
            "iteration 3124: train_loss:1.6812694072723389 val_loss1.703831672668457\n",
            "iteration 3125: train_loss:1.6800248622894287 val_loss1.7026675939559937\n",
            "iteration 3126: train_loss:1.6787797212600708 val_loss1.7015036344528198\n",
            "iteration 3127: train_loss:1.6775343418121338 val_loss1.70033860206604\n",
            "iteration 3128: train_loss:1.6762882471084595 val_loss1.6991735696792603\n",
            "iteration 3129: train_loss:1.675041675567627 val_loss1.6980071067810059\n",
            "iteration 3130: train_loss:1.6737945079803467 val_loss1.6968402862548828\n",
            "iteration 3131: train_loss:1.6725469827651978 val_loss1.6956733465194702\n",
            "iteration 3132: train_loss:1.6712989807128906 val_loss1.6945058107376099\n",
            "iteration 3133: train_loss:1.6700501441955566 val_loss1.6933379173278809\n",
            "iteration 3134: train_loss:1.6688010692596436 val_loss1.6921696662902832\n",
            "iteration 3135: train_loss:1.6675512790679932 val_loss1.6910008192062378\n",
            "iteration 3136: train_loss:1.6663011312484741 val_loss1.6898313760757446\n",
            "iteration 3137: train_loss:1.6650501489639282 val_loss1.6886610984802246\n",
            "iteration 3138: train_loss:1.6637980937957764 val_loss1.6874897480010986\n",
            "iteration 3139: train_loss:1.6625455617904663 val_loss1.6863175630569458\n",
            "iteration 3140: train_loss:1.6612920761108398 val_loss1.6851445436477661\n",
            "iteration 3141: train_loss:1.6600385904312134 val_loss1.6839709281921387\n",
            "iteration 3142: train_loss:1.6587841510772705 val_loss1.682796597480774\n",
            "iteration 3143: train_loss:1.6575287580490112 val_loss1.6816214323043823\n",
            "iteration 3144: train_loss:1.6562731266021729 val_loss1.6804462671279907\n",
            "iteration 3145: train_loss:1.6550166606903076 val_loss1.6792707443237305\n",
            "iteration 3146: train_loss:1.6537595987319946 val_loss1.6780941486358643\n",
            "iteration 3147: train_loss:1.6525014638900757 val_loss1.6769163608551025\n",
            "iteration 3148: train_loss:1.651242733001709 val_loss1.675737738609314\n",
            "iteration 3149: train_loss:1.6499831676483154 val_loss1.674558401107788\n",
            "iteration 3150: train_loss:1.6487226486206055 val_loss1.6733781099319458\n",
            "iteration 3151: train_loss:1.647460699081421 val_loss1.6721961498260498\n",
            "iteration 3152: train_loss:1.6461979150772095 val_loss1.6710132360458374\n",
            "iteration 3153: train_loss:1.6449342966079712 val_loss1.6698297262191772\n",
            "iteration 3154: train_loss:1.6436697244644165 val_loss1.668644905090332\n",
            "iteration 3155: train_loss:1.6424040794372559 val_loss1.6674593687057495\n",
            "iteration 3156: train_loss:1.6411375999450684 val_loss1.6662728786468506\n",
            "iteration 3157: train_loss:1.6398699283599854 val_loss1.665086030960083\n",
            "iteration 3158: train_loss:1.6386010646820068 val_loss1.6638983488082886\n",
            "iteration 3159: train_loss:1.6373310089111328 val_loss1.6627081632614136\n",
            "iteration 3160: train_loss:1.6360599994659424 val_loss1.6615172624588013\n",
            "iteration 3161: train_loss:1.6347875595092773 val_loss1.6603251695632935\n",
            "iteration 3162: train_loss:1.6335139274597168 val_loss1.659132480621338\n",
            "iteration 3163: train_loss:1.6322391033172607 val_loss1.6579378843307495\n",
            "iteration 3164: train_loss:1.6309632062911987 val_loss1.656742811203003\n",
            "iteration 3165: train_loss:1.6296865940093994 val_loss1.65554678440094\n",
            "iteration 3166: train_loss:1.6284083127975464 val_loss1.6543500423431396\n",
            "iteration 3167: train_loss:1.6271287202835083 val_loss1.653151273727417\n",
            "iteration 3168: train_loss:1.6258481740951538 val_loss1.6519521474838257\n",
            "iteration 3169: train_loss:1.624566674232483 val_loss1.6507518291473389\n",
            "iteration 3170: train_loss:1.6232839822769165 val_loss1.649550437927246\n",
            "iteration 3171: train_loss:1.6220003366470337 val_loss1.6483490467071533\n",
            "iteration 3172: train_loss:1.6207154989242554 val_loss1.6471469402313232\n",
            "iteration 3173: train_loss:1.6194294691085815 val_loss1.6459439992904663\n",
            "iteration 3174: train_loss:1.6181414127349854 val_loss1.6447372436523438\n",
            "iteration 3175: train_loss:1.6168521642684937 val_loss1.6435291767120361\n",
            "iteration 3176: train_loss:1.6155617237091064 val_loss1.642320990562439\n",
            "iteration 3177: train_loss:1.6142706871032715 val_loss1.6411123275756836\n",
            "iteration 3178: train_loss:1.6129781007766724 val_loss1.6399030685424805\n",
            "iteration 3179: train_loss:1.6116844415664673 val_loss1.638692855834961\n",
            "iteration 3180: train_loss:1.6103893518447876 val_loss1.6374812126159668\n",
            "iteration 3181: train_loss:1.6090930700302124 val_loss1.6362682580947876\n",
            "iteration 3182: train_loss:1.607795238494873 val_loss1.6350539922714233\n",
            "iteration 3183: train_loss:1.6064963340759277 val_loss1.6338386535644531\n",
            "iteration 3184: train_loss:1.605196475982666 val_loss1.6326223611831665\n",
            "iteration 3185: train_loss:1.6038950681686401 val_loss1.6314053535461426\n",
            "iteration 3186: train_loss:1.6025928258895874 val_loss1.630187749862671\n",
            "iteration 3187: train_loss:1.6012893915176392 val_loss1.6289693117141724\n",
            "iteration 3188: train_loss:1.5999850034713745 val_loss1.6277501583099365\n",
            "iteration 3189: train_loss:1.5986794233322144 val_loss1.626530408859253\n",
            "iteration 3190: train_loss:1.5973727703094482 val_loss1.6253095865249634\n",
            "iteration 3191: train_loss:1.5960651636123657 val_loss1.624087929725647\n",
            "iteration 3192: train_loss:1.5947566032409668 val_loss1.6228654384613037\n",
            "iteration 3193: train_loss:1.5934464931488037 val_loss1.6216416358947754\n",
            "iteration 3194: train_loss:1.5921350717544556 val_loss1.6204164028167725\n",
            "iteration 3195: train_loss:1.5908228158950806 val_loss1.6191904544830322\n",
            "iteration 3196: train_loss:1.5895092487335205 val_loss1.617963194847107\n",
            "iteration 3197: train_loss:1.5881946086883545 val_loss1.6167354583740234\n",
            "iteration 3198: train_loss:1.5868788957595825 val_loss1.6155065298080444\n",
            "iteration 3199: train_loss:1.585561990737915 val_loss1.61427640914917\n",
            "iteration 3200: train_loss:1.5842442512512207 val_loss1.6130456924438477\n",
            "iteration 3201: train_loss:1.582924723625183 val_loss1.611812949180603\n",
            "iteration 3202: train_loss:1.5816038846969604 val_loss1.6105796098709106\n",
            "iteration 3203: train_loss:1.580282211303711 val_loss1.6093449592590332\n",
            "iteration 3204: train_loss:1.5789592266082764 val_loss1.6081092357635498\n",
            "iteration 3205: train_loss:1.5776350498199463 val_loss1.6068719625473022\n",
            "iteration 3206: train_loss:1.5763099193572998 val_loss1.6056348085403442\n",
            "iteration 3207: train_loss:1.5749843120574951 val_loss1.604397177696228\n",
            "iteration 3208: train_loss:1.5736572742462158 val_loss1.6031584739685059\n",
            "iteration 3209: train_loss:1.5723294019699097 val_loss1.6019188165664673\n",
            "iteration 3210: train_loss:1.5710002183914185 val_loss1.6006786823272705\n",
            "iteration 3211: train_loss:1.56967031955719 val_loss1.5994375944137573\n",
            "iteration 3212: train_loss:1.568339228630066 val_loss1.5981955528259277\n",
            "iteration 3213: train_loss:1.5670071840286255 val_loss1.5969523191452026\n",
            "iteration 3214: train_loss:1.565674066543579 val_loss1.5957083702087402\n",
            "iteration 3215: train_loss:1.5643404722213745 val_loss1.5944640636444092\n",
            "iteration 3216: train_loss:1.5630056858062744 val_loss1.5932190418243408\n",
            "iteration 3217: train_loss:1.5616700649261475 val_loss1.5919734239578247\n",
            "iteration 3218: train_loss:1.5603340864181519 val_loss1.59072744846344\n",
            "iteration 3219: train_loss:1.5589970350265503 val_loss1.5894808769226074\n",
            "iteration 3220: train_loss:1.5576591491699219 val_loss1.588233470916748\n",
            "iteration 3221: train_loss:1.556320309638977 val_loss1.5869851112365723\n",
            "iteration 3222: train_loss:1.5549806356430054 val_loss1.5857359170913696\n",
            "iteration 3223: train_loss:1.553640365600586 val_loss1.5844857692718506\n",
            "iteration 3224: train_loss:1.5522987842559814 val_loss1.5832350254058838\n",
            "iteration 3225: train_loss:1.55095636844635 val_loss1.5819834470748901\n",
            "iteration 3226: train_loss:1.549613356590271 val_loss1.5807312726974487\n",
            "iteration 3227: train_loss:1.5482697486877441 val_loss1.57947838306427\n",
            "iteration 3228: train_loss:1.54692542552948 val_loss1.5782256126403809\n",
            "iteration 3229: train_loss:1.545580267906189 val_loss1.5769709348678589\n",
            "iteration 3230: train_loss:1.5442341566085815 val_loss1.5757155418395996\n",
            "iteration 3231: train_loss:1.5428876876831055 val_loss1.5744595527648926\n",
            "iteration 3232: train_loss:1.541540265083313 val_loss1.5732035636901855\n",
            "iteration 3233: train_loss:1.5401922464370728 val_loss1.571946620941162\n",
            "iteration 3234: train_loss:1.5388437509536743 val_loss1.5706901550292969\n",
            "iteration 3235: train_loss:1.5374946594238281 val_loss1.5694330930709839\n",
            "iteration 3236: train_loss:1.5361449718475342 val_loss1.5681753158569336\n",
            "iteration 3237: train_loss:1.534794569015503 val_loss1.5669173002243042\n",
            "iteration 3238: train_loss:1.533443808555603 val_loss1.5656588077545166\n",
            "iteration 3239: train_loss:1.5320923328399658 val_loss1.564400553703308\n",
            "iteration 3240: train_loss:1.5307400226593018 val_loss1.5631413459777832\n",
            "iteration 3241: train_loss:1.5293872356414795 val_loss1.5618810653686523\n",
            "iteration 3242: train_loss:1.52803373336792 val_loss1.560620665550232\n",
            "iteration 3243: train_loss:1.5266798734664917 val_loss1.5593605041503906\n",
            "iteration 3244: train_loss:1.5253257751464844 val_loss1.5581001043319702\n",
            "iteration 3245: train_loss:1.523971438407898 val_loss1.556839942932129\n",
            "iteration 3246: train_loss:1.5226165056228638 val_loss1.5555799007415771\n",
            "iteration 3247: train_loss:1.521261215209961 val_loss1.5543193817138672\n",
            "iteration 3248: train_loss:1.5199054479599 val_loss1.5530588626861572\n",
            "iteration 3249: train_loss:1.5185496807098389 val_loss1.55179762840271\n",
            "iteration 3250: train_loss:1.5171934366226196 val_loss1.5505365133285522\n",
            "iteration 3251: train_loss:1.5158365964889526 val_loss1.5492750406265259\n",
            "iteration 3252: train_loss:1.514479160308838 val_loss1.5480122566223145\n",
            "iteration 3253: train_loss:1.5131216049194336 val_loss1.5467491149902344\n",
            "iteration 3254: train_loss:1.5117641687393188 val_loss1.5454864501953125\n",
            "iteration 3255: train_loss:1.510406494140625 val_loss1.5442240238189697\n",
            "iteration 3256: train_loss:1.5090487003326416 val_loss1.542961835861206\n",
            "iteration 3257: train_loss:1.5076903104782104 val_loss1.541700005531311\n",
            "iteration 3258: train_loss:1.5063321590423584 val_loss1.540437936782837\n",
            "iteration 3259: train_loss:1.5049740076065063 val_loss1.5391764640808105\n",
            "iteration 3260: train_loss:1.5036160945892334 val_loss1.5379149913787842\n",
            "iteration 3261: train_loss:1.5022579431533813 val_loss1.5366535186767578\n",
            "iteration 3262: train_loss:1.500900149345398 val_loss1.535393238067627\n",
            "iteration 3263: train_loss:1.4995421171188354 val_loss1.5341330766677856\n",
            "iteration 3264: train_loss:1.498184084892273 val_loss1.5328729152679443\n",
            "iteration 3265: train_loss:1.4968267679214478 val_loss1.5316132307052612\n",
            "iteration 3266: train_loss:1.495469331741333 val_loss1.5303540229797363\n",
            "iteration 3267: train_loss:1.4941118955612183 val_loss1.52909517288208\n",
            "iteration 3268: train_loss:1.4927549362182617 val_loss1.527836799621582\n",
            "iteration 3269: train_loss:1.4913980960845947 val_loss1.5265785455703735\n",
            "iteration 3270: train_loss:1.4900414943695068 val_loss1.5253210067749023\n",
            "iteration 3271: train_loss:1.4886853694915771 val_loss1.5240644216537476\n",
            "iteration 3272: train_loss:1.4873292446136475 val_loss1.522808313369751\n",
            "iteration 3273: train_loss:1.4859733581542969 val_loss1.5215524435043335\n",
            "iteration 3274: train_loss:1.4846181869506836 val_loss1.5202974081039429\n",
            "iteration 3275: train_loss:1.4832631349563599 val_loss1.519043207168579\n",
            "iteration 3276: train_loss:1.4819090366363525 val_loss1.5177897214889526\n",
            "iteration 3277: train_loss:1.4805551767349243 val_loss1.5165371894836426\n",
            "iteration 3278: train_loss:1.479202389717102 val_loss1.5152854919433594\n",
            "iteration 3279: train_loss:1.4778501987457275 val_loss1.5140347480773926\n",
            "iteration 3280: train_loss:1.4764986038208008 val_loss1.5127853155136108\n",
            "iteration 3281: train_loss:1.4751474857330322 val_loss1.5115365982055664\n",
            "iteration 3282: train_loss:1.4737972021102905 val_loss1.5102884769439697\n",
            "iteration 3283: train_loss:1.4724478721618652 val_loss1.5090417861938477\n",
            "iteration 3284: train_loss:1.4710991382598877 val_loss1.507796287536621\n",
            "iteration 3285: train_loss:1.4697514772415161 val_loss1.506551742553711\n",
            "iteration 3286: train_loss:1.468404769897461 val_loss1.5053082704544067\n",
            "iteration 3287: train_loss:1.467058777809143 val_loss1.5040661096572876\n",
            "iteration 3288: train_loss:1.4657135009765625 val_loss1.5028246641159058\n",
            "iteration 3289: train_loss:1.464369297027588 val_loss1.5015844106674194\n",
            "iteration 3290: train_loss:1.4630260467529297 val_loss1.5003457069396973\n",
            "iteration 3291: train_loss:1.461683750152588 val_loss1.499107837677002\n",
            "iteration 3292: train_loss:1.4603424072265625 val_loss1.497870922088623\n",
            "iteration 3293: train_loss:1.4590027332305908 val_loss1.4966365098953247\n",
            "iteration 3294: train_loss:1.457663893699646 val_loss1.4954031705856323\n",
            "iteration 3295: train_loss:1.4563263654708862 val_loss1.4941715002059937\n",
            "iteration 3296: train_loss:1.4549897909164429 val_loss1.4929403066635132\n",
            "iteration 3297: train_loss:1.4536545276641846 val_loss1.4917105436325073\n",
            "iteration 3298: train_loss:1.4523202180862427 val_loss1.4904814958572388\n",
            "iteration 3299: train_loss:1.4509871006011963 val_loss1.4892535209655762\n",
            "iteration 3300: train_loss:1.4496550559997559 val_loss1.4880272150039673\n",
            "iteration 3301: train_loss:1.4483245611190796 val_loss1.4868029356002808\n",
            "iteration 3302: train_loss:1.4469952583312988 val_loss1.485580325126648\n",
            "iteration 3303: train_loss:1.4456671476364136 val_loss1.4843592643737793\n",
            "iteration 3304: train_loss:1.444340705871582 val_loss1.4831395149230957\n",
            "iteration 3305: train_loss:1.443015456199646 val_loss1.4819211959838867\n",
            "iteration 3306: train_loss:1.4416918754577637 val_loss1.4807043075561523\n",
            "iteration 3307: train_loss:1.4403698444366455 val_loss1.479488492012024\n",
            "iteration 3308: train_loss:1.4390493631362915 val_loss1.478275179862976\n",
            "iteration 3309: train_loss:1.4377301931381226 val_loss1.477063536643982\n",
            "iteration 3310: train_loss:1.4364122152328491 val_loss1.4758527278900146\n",
            "iteration 3311: train_loss:1.4350956678390503 val_loss1.474644422531128\n",
            "iteration 3312: train_loss:1.4337809085845947 val_loss1.4734387397766113\n",
            "iteration 3313: train_loss:1.4324672222137451 val_loss1.472233772277832\n",
            "iteration 3314: train_loss:1.4311555624008179 val_loss1.471030354499817\n",
            "iteration 3315: train_loss:1.4298453330993652 val_loss1.4698293209075928\n",
            "iteration 3316: train_loss:1.4285365343093872 val_loss1.4686309099197388\n",
            "iteration 3317: train_loss:1.427229642868042 val_loss1.4674339294433594\n",
            "iteration 3318: train_loss:1.4259244203567505 val_loss1.4662389755249023\n",
            "iteration 3319: train_loss:1.4246214628219604 val_loss1.465047001838684\n",
            "iteration 3320: train_loss:1.4233202934265137 val_loss1.4638571739196777\n",
            "iteration 3321: train_loss:1.4220213890075684 val_loss1.4626693725585938\n",
            "iteration 3322: train_loss:1.4207240343093872 val_loss1.4614834785461426\n",
            "iteration 3323: train_loss:1.4194293022155762 val_loss1.4603008031845093\n",
            "iteration 3324: train_loss:1.418136715888977 val_loss1.4591199159622192\n",
            "iteration 3325: train_loss:1.416845679283142 val_loss1.4579404592514038\n",
            "iteration 3326: train_loss:1.4155572652816772 val_loss1.4567632675170898\n",
            "iteration 3327: train_loss:1.4142699241638184 val_loss1.4555879831314087\n",
            "iteration 3328: train_loss:1.41298508644104 val_loss1.45441472530365\n",
            "iteration 3329: train_loss:1.4117025136947632 val_loss1.4532443284988403\n",
            "iteration 3330: train_loss:1.4104217290878296 val_loss1.4520761966705322\n",
            "iteration 3331: train_loss:1.4091435670852661 val_loss1.4509103298187256\n",
            "iteration 3332: train_loss:1.4078669548034668 val_loss1.4497466087341309\n",
            "iteration 3333: train_loss:1.4065932035446167 val_loss1.4485851526260376\n",
            "iteration 3334: train_loss:1.4053211212158203 val_loss1.4474263191223145\n",
            "iteration 3335: train_loss:1.4040515422821045 val_loss1.4462696313858032\n",
            "iteration 3336: train_loss:1.4027841091156006 val_loss1.4451152086257935\n",
            "iteration 3337: train_loss:1.4015189409255981 val_loss1.4439635276794434\n",
            "iteration 3338: train_loss:1.4002556800842285 val_loss1.442813754081726\n",
            "iteration 3339: train_loss:1.398995041847229 val_loss1.4416669607162476\n",
            "iteration 3340: train_loss:1.39773690700531 val_loss1.44052255153656\n",
            "iteration 3341: train_loss:1.396480917930603 val_loss1.4393811225891113\n",
            "iteration 3342: train_loss:1.3952277898788452 val_loss1.4382414817810059\n",
            "iteration 3343: train_loss:1.3939770460128784 val_loss1.4371049404144287\n",
            "iteration 3344: train_loss:1.3927288055419922 val_loss1.4359711408615112\n",
            "iteration 3345: train_loss:1.3914833068847656 val_loss1.4348397254943848\n",
            "iteration 3346: train_loss:1.3902403116226196 val_loss1.4337114095687866\n",
            "iteration 3347: train_loss:1.3889998197555542 val_loss1.43258535861969\n",
            "iteration 3348: train_loss:1.3877620697021484 val_loss1.431462049484253\n",
            "iteration 3349: train_loss:1.3865267038345337 val_loss1.430341124534607\n",
            "iteration 3350: train_loss:1.385293960571289 val_loss1.4292230606079102\n",
            "iteration 3351: train_loss:1.384063720703125 val_loss1.4281073808670044\n",
            "iteration 3352: train_loss:1.3828359842300415 val_loss1.4269946813583374\n",
            "iteration 3353: train_loss:1.3816107511520386 val_loss1.4258840084075928\n",
            "iteration 3354: train_loss:1.3803883790969849 val_loss1.424776554107666\n",
            "iteration 3355: train_loss:1.3791686296463013 val_loss1.4236717224121094\n",
            "iteration 3356: train_loss:1.377951741218567 val_loss1.422569990158081\n",
            "iteration 3357: train_loss:1.376737356185913 val_loss1.421471357345581\n",
            "iteration 3358: train_loss:1.375525951385498 val_loss1.4203753471374512\n",
            "iteration 3359: train_loss:1.3743174076080322 val_loss1.4192821979522705\n",
            "iteration 3360: train_loss:1.373111605644226 val_loss1.4181920289993286\n",
            "iteration 3361: train_loss:1.3719086647033691 val_loss1.417104721069336\n",
            "iteration 3362: train_loss:1.3707085847854614 val_loss1.4160209894180298\n",
            "iteration 3363: train_loss:1.3695111274719238 val_loss1.414940357208252\n",
            "iteration 3364: train_loss:1.368316650390625 val_loss1.4138622283935547\n",
            "iteration 3365: train_loss:1.367125153541565 val_loss1.4127875566482544\n",
            "iteration 3366: train_loss:1.3659367561340332 val_loss1.4117159843444824\n",
            "iteration 3367: train_loss:1.3647512197494507 val_loss1.4106471538543701\n",
            "iteration 3368: train_loss:1.3635681867599487 val_loss1.4095814228057861\n",
            "iteration 3369: train_loss:1.3623884916305542 val_loss1.4085181951522827\n",
            "iteration 3370: train_loss:1.361210823059082 val_loss1.4074573516845703\n",
            "iteration 3371: train_loss:1.3600366115570068 val_loss1.4063997268676758\n",
            "iteration 3372: train_loss:1.3588651418685913 val_loss1.4053457975387573\n",
            "iteration 3373: train_loss:1.3576966524124146 val_loss1.404294729232788\n",
            "iteration 3374: train_loss:1.356531023979187 val_loss1.4032466411590576\n",
            "iteration 3375: train_loss:1.3553682565689087 val_loss1.4022012948989868\n",
            "iteration 3376: train_loss:1.3542088270187378 val_loss1.4011598825454712\n",
            "iteration 3377: train_loss:1.3530524969100952 val_loss1.400121808052063\n",
            "iteration 3378: train_loss:1.3518993854522705 val_loss1.3990867137908936\n",
            "iteration 3379: train_loss:1.3507492542266846 val_loss1.398055076599121\n",
            "iteration 3380: train_loss:1.349602222442627 val_loss1.397026777267456\n",
            "iteration 3381: train_loss:1.3484580516815186 val_loss1.3960011005401611\n",
            "iteration 3382: train_loss:1.3473167419433594 val_loss1.3949785232543945\n",
            "iteration 3383: train_loss:1.3461787700653076 val_loss1.393958568572998\n",
            "iteration 3384: train_loss:1.3450435400009155 val_loss1.392942190170288\n",
            "iteration 3385: train_loss:1.3439115285873413 val_loss1.3919285535812378\n",
            "iteration 3386: train_loss:1.3427822589874268 val_loss1.3909180164337158\n",
            "iteration 3387: train_loss:1.34165620803833 val_loss1.3899105787277222\n",
            "iteration 3388: train_loss:1.3405331373214722 val_loss1.3889057636260986\n",
            "iteration 3389: train_loss:1.3394131660461426 val_loss1.387904167175293\n",
            "iteration 3390: train_loss:1.338295578956604 val_loss1.3869050741195679\n",
            "iteration 3391: train_loss:1.3371810913085938 val_loss1.385908603668213\n",
            "iteration 3392: train_loss:1.3360698223114014 val_loss1.384915828704834\n",
            "iteration 3393: train_loss:1.3349616527557373 val_loss1.3839260339736938\n",
            "iteration 3394: train_loss:1.3338569402694702 val_loss1.3829396963119507\n",
            "iteration 3395: train_loss:1.3327546119689941 val_loss1.3819564580917358\n",
            "iteration 3396: train_loss:1.331655502319336 val_loss1.3809763193130493\n",
            "iteration 3397: train_loss:1.3305591344833374 val_loss1.3799996376037598\n",
            "iteration 3398: train_loss:1.3294659852981567 val_loss1.379025936126709\n",
            "iteration 3399: train_loss:1.328376054763794 val_loss1.3780558109283447\n",
            "iteration 3400: train_loss:1.32728910446167 val_loss1.3770886659622192\n",
            "iteration 3401: train_loss:1.3262054920196533 val_loss1.376124620437622\n",
            "iteration 3402: train_loss:1.3251248598098755 val_loss1.3751639127731323\n",
            "iteration 3403: train_loss:1.3240469694137573 val_loss1.3742058277130127\n",
            "iteration 3404: train_loss:1.3229718208312988 val_loss1.3732503652572632\n",
            "iteration 3405: train_loss:1.321899652481079 val_loss1.3722983598709106\n",
            "iteration 3406: train_loss:1.3208308219909668 val_loss1.3713493347167969\n",
            "iteration 3407: train_loss:1.3197650909423828 val_loss1.3704036474227905\n",
            "iteration 3408: train_loss:1.3187024593353271 val_loss1.369460940361023\n",
            "iteration 3409: train_loss:1.3176426887512207 val_loss1.3685210943222046\n",
            "iteration 3410: train_loss:1.3165862560272217 val_loss1.367584466934204\n",
            "iteration 3411: train_loss:1.3155325651168823 val_loss1.3666504621505737\n",
            "iteration 3412: train_loss:1.3144819736480713 val_loss1.3657201528549194\n",
            "iteration 3413: train_loss:1.313434362411499 val_loss1.364792823791504\n",
            "iteration 3414: train_loss:1.312389850616455 val_loss1.363867998123169\n",
            "iteration 3415: train_loss:1.31134831905365 val_loss1.3629467487335205\n",
            "iteration 3416: train_loss:1.3103100061416626 val_loss1.3620275259017944\n",
            "iteration 3417: train_loss:1.3092745542526245 val_loss1.3611115217208862\n",
            "iteration 3418: train_loss:1.3082419633865356 val_loss1.3601986169815063\n",
            "iteration 3419: train_loss:1.3072125911712646 val_loss1.3592886924743652\n",
            "iteration 3420: train_loss:1.306186318397522 val_loss1.3583815097808838\n",
            "iteration 3421: train_loss:1.3051631450653076 val_loss1.357477068901062\n",
            "iteration 3422: train_loss:1.3041428327560425 val_loss1.356575846672058\n",
            "iteration 3423: train_loss:1.3031253814697266 val_loss1.3556772470474243\n",
            "iteration 3424: train_loss:1.3021116256713867 val_loss1.354782223701477\n",
            "iteration 3425: train_loss:1.3011003732681274 val_loss1.3538898229599\n",
            "iteration 3426: train_loss:1.3000924587249756 val_loss1.3530008792877197\n",
            "iteration 3427: train_loss:1.2990878820419312 val_loss1.3521150350570679\n",
            "iteration 3428: train_loss:1.298086166381836 val_loss1.3512322902679443\n",
            "iteration 3429: train_loss:1.2970876693725586 val_loss1.3503530025482178\n",
            "iteration 3430: train_loss:1.2960925102233887 val_loss1.349476933479309\n",
            "iteration 3431: train_loss:1.2951003313064575 val_loss1.34860360622406\n",
            "iteration 3432: train_loss:1.2941110134124756 val_loss1.3477329015731812\n",
            "iteration 3433: train_loss:1.293124794960022 val_loss1.3468654155731201\n",
            "iteration 3434: train_loss:1.2921416759490967 val_loss1.3460005521774292\n",
            "iteration 3435: train_loss:1.2911615371704102 val_loss1.3451390266418457\n",
            "iteration 3436: train_loss:1.2901842594146729 val_loss1.344280481338501\n",
            "iteration 3437: train_loss:1.2892102003097534 val_loss1.343424677848816\n",
            "iteration 3438: train_loss:1.2882390022277832 val_loss1.3425720930099487\n",
            "iteration 3439: train_loss:1.287271499633789 val_loss1.3417227268218994\n",
            "iteration 3440: train_loss:1.2863068580627441 val_loss1.340876817703247\n",
            "iteration 3441: train_loss:1.2853453159332275 val_loss1.340033769607544\n",
            "iteration 3442: train_loss:1.2843869924545288 val_loss1.3391934633255005\n",
            "iteration 3443: train_loss:1.2834316492080688 val_loss1.3383562564849854\n",
            "iteration 3444: train_loss:1.282479166984558 val_loss1.3375216722488403\n",
            "iteration 3445: train_loss:1.281529426574707 val_loss1.3366895914077759\n",
            "iteration 3446: train_loss:1.2805832624435425 val_loss1.3358606100082397\n",
            "iteration 3447: train_loss:1.2796400785446167 val_loss1.3350344896316528\n",
            "iteration 3448: train_loss:1.2786996364593506 val_loss1.3342114686965942\n",
            "iteration 3449: train_loss:1.2777624130249023 val_loss1.3333911895751953\n",
            "iteration 3450: train_loss:1.2768282890319824 val_loss1.3325743675231934\n",
            "iteration 3451: train_loss:1.2758967876434326 val_loss1.3317599296569824\n",
            "iteration 3452: train_loss:1.2749685049057007 val_loss1.330948829650879\n",
            "iteration 3453: train_loss:1.274043321609497 val_loss1.3301405906677246\n",
            "iteration 3454: train_loss:1.2731207609176636 val_loss1.32933509349823\n",
            "iteration 3455: train_loss:1.2722012996673584 val_loss1.3285324573516846\n",
            "iteration 3456: train_loss:1.2712849378585815 val_loss1.3277331590652466\n",
            "iteration 3457: train_loss:1.2703717947006226 val_loss1.3269370794296265\n",
            "iteration 3458: train_loss:1.2694618701934814 val_loss1.326143741607666\n",
            "iteration 3459: train_loss:1.2685548067092896 val_loss1.3253536224365234\n",
            "iteration 3460: train_loss:1.267650842666626 val_loss1.32456636428833\n",
            "iteration 3461: train_loss:1.2667499780654907 val_loss1.3237820863723755\n",
            "iteration 3462: train_loss:1.2658518552780151 val_loss1.3230005502700806\n",
            "iteration 3463: train_loss:1.2649568319320679 val_loss1.3222217559814453\n",
            "iteration 3464: train_loss:1.2640645503997803 val_loss1.3214454650878906\n",
            "iteration 3465: train_loss:1.2631750106811523 val_loss1.3206719160079956\n",
            "iteration 3466: train_loss:1.2622886896133423 val_loss1.319901704788208\n",
            "iteration 3467: train_loss:1.2614048719406128 val_loss1.3191338777542114\n",
            "iteration 3468: train_loss:1.2605241537094116 val_loss1.3183691501617432\n",
            "iteration 3469: train_loss:1.2596461772918701 val_loss1.3176076412200928\n",
            "iteration 3470: train_loss:1.2587711811065674 val_loss1.3168485164642334\n",
            "iteration 3471: train_loss:1.2578994035720825 val_loss1.3160922527313232\n",
            "iteration 3472: train_loss:1.2570301294326782 val_loss1.3153387308120728\n",
            "iteration 3473: train_loss:1.2561640739440918 val_loss1.3145878314971924\n",
            "iteration 3474: train_loss:1.2553009986877441 val_loss1.3138395547866821\n",
            "iteration 3475: train_loss:1.2544403076171875 val_loss1.313094139099121\n",
            "iteration 3476: train_loss:1.2535827159881592 val_loss1.3123513460159302\n",
            "iteration 3477: train_loss:1.2527275085449219 val_loss1.3116109371185303\n",
            "iteration 3478: train_loss:1.2518752813339233 val_loss1.3108733892440796\n",
            "iteration 3479: train_loss:1.2510263919830322 val_loss1.3101389408111572\n",
            "iteration 3480: train_loss:1.2501802444458008 val_loss1.309407353401184\n",
            "iteration 3481: train_loss:1.2493371963500977 val_loss1.3086787462234497\n",
            "iteration 3482: train_loss:1.248496651649475 val_loss1.3079533576965332\n",
            "iteration 3483: train_loss:1.2476590871810913 val_loss1.3072305917739868\n",
            "iteration 3484: train_loss:1.2468245029449463 val_loss1.3065109252929688\n",
            "iteration 3485: train_loss:1.245992660522461 val_loss1.3057938814163208\n",
            "iteration 3486: train_loss:1.2451635599136353 val_loss1.305079698562622\n",
            "iteration 3487: train_loss:1.2443368434906006 val_loss1.3043674230575562\n",
            "iteration 3488: train_loss:1.2435134649276733 val_loss1.3036584854125977\n",
            "iteration 3489: train_loss:1.2426927089691162 val_loss1.3029519319534302\n",
            "iteration 3490: train_loss:1.2418750524520874 val_loss1.302248477935791\n",
            "iteration 3491: train_loss:1.2410598993301392 val_loss1.3015472888946533\n",
            "iteration 3492: train_loss:1.2402477264404297 val_loss1.3008486032485962\n",
            "iteration 3493: train_loss:1.2394384145736694 val_loss1.3001528978347778\n",
            "iteration 3494: train_loss:1.2386314868927002 val_loss1.29945969581604\n",
            "iteration 3495: train_loss:1.2378273010253906 val_loss1.2987688779830933\n",
            "iteration 3496: train_loss:1.2370262145996094 val_loss1.298080563545227\n",
            "iteration 3497: train_loss:1.23622727394104 val_loss1.297394871711731\n",
            "iteration 3498: train_loss:1.235431432723999 val_loss1.2967115640640259\n",
            "iteration 3499: train_loss:1.2346382141113281 val_loss1.29603111743927\n",
            "iteration 3500: train_loss:1.2338474988937378 val_loss1.2953529357910156\n",
            "iteration 3501: train_loss:1.233059287071228 val_loss1.2946776151657104\n",
            "iteration 3502: train_loss:1.2322734594345093 val_loss1.294004201889038\n",
            "iteration 3503: train_loss:1.2314902544021606 val_loss1.2933332920074463\n",
            "iteration 3504: train_loss:1.2307099103927612 val_loss1.2926652431488037\n",
            "iteration 3505: train_loss:1.2299323081970215 val_loss1.2919994592666626\n",
            "iteration 3506: train_loss:1.2291570901870728 val_loss1.2913364171981812\n",
            "iteration 3507: train_loss:1.2283846139907837 val_loss1.2906757593154907\n",
            "iteration 3508: train_loss:1.2276146411895752 val_loss1.2900176048278809\n",
            "iteration 3509: train_loss:1.2268471717834473 val_loss1.2893620729446411\n",
            "iteration 3510: train_loss:1.226082682609558 val_loss1.2887094020843506\n",
            "iteration 3511: train_loss:1.2253204584121704 val_loss1.2880592346191406\n",
            "iteration 3512: train_loss:1.2245609760284424 val_loss1.2874115705490112\n",
            "iteration 3513: train_loss:1.223803997039795 val_loss1.2867662906646729\n",
            "iteration 3514: train_loss:1.2230494022369385 val_loss1.2861236333847046\n",
            "iteration 3515: train_loss:1.2222976684570312 val_loss1.2854830026626587\n",
            "iteration 3516: train_loss:1.221548080444336 val_loss1.284844994544983\n",
            "iteration 3517: train_loss:1.2208008766174316 val_loss1.2842094898223877\n",
            "iteration 3518: train_loss:1.220056414604187 val_loss1.2835761308670044\n",
            "iteration 3519: train_loss:1.2193140983581543 val_loss1.282945156097412\n",
            "iteration 3520: train_loss:1.2185744047164917 val_loss1.2823169231414795\n",
            "iteration 3521: train_loss:1.2178374528884888 val_loss1.2816911935806274\n",
            "iteration 3522: train_loss:1.2171026468276978 val_loss1.281067967414856\n",
            "iteration 3523: train_loss:1.2163708209991455 val_loss1.2804471254348755\n",
            "iteration 3524: train_loss:1.2156411409378052 val_loss1.2798287868499756\n",
            "iteration 3525: train_loss:1.2149142026901245 val_loss1.2792130708694458\n",
            "iteration 3526: train_loss:1.2141900062561035 val_loss1.2785996198654175\n",
            "iteration 3527: train_loss:1.2134678363800049 val_loss1.2779887914657593\n",
            "iteration 3528: train_loss:1.212748408317566 val_loss1.2773802280426025\n",
            "iteration 3529: train_loss:1.212031364440918 val_loss1.2767741680145264\n",
            "iteration 3530: train_loss:1.211316704750061 val_loss1.276170253753662\n",
            "iteration 3531: train_loss:1.2106045484542847 val_loss1.2755684852600098\n",
            "iteration 3532: train_loss:1.2098945379257202 val_loss1.2749688625335693\n",
            "iteration 3533: train_loss:1.209187388420105 val_loss1.274371862411499\n",
            "iteration 3534: train_loss:1.2084821462631226 val_loss1.2737773656845093\n",
            "iteration 3535: train_loss:1.2077796459197998 val_loss1.273184895515442\n",
            "iteration 3536: train_loss:1.2070794105529785 val_loss1.2725950479507446\n",
            "iteration 3537: train_loss:1.2063815593719482 val_loss1.2720072269439697\n",
            "iteration 3538: train_loss:1.2056858539581299 val_loss1.2714216709136963\n",
            "iteration 3539: train_loss:1.204992651939392 val_loss1.2708388566970825\n",
            "iteration 3540: train_loss:1.2043018341064453 val_loss1.2702583074569702\n",
            "iteration 3541: train_loss:1.2036131620407104 val_loss1.2696799039840698\n",
            "iteration 3542: train_loss:1.202926754951477 val_loss1.2691034078598022\n",
            "iteration 3543: train_loss:1.2022430896759033 val_loss1.2685294151306152\n",
            "iteration 3544: train_loss:1.2015615701675415 val_loss1.2679579257965088\n",
            "iteration 3545: train_loss:1.2008823156356812 val_loss1.2673888206481934\n",
            "iteration 3546: train_loss:1.2002052068710327 val_loss1.2668216228485107\n",
            "iteration 3547: train_loss:1.1995306015014648 val_loss1.2662568092346191\n",
            "iteration 3548: train_loss:1.1988581418991089 val_loss1.265694260597229\n",
            "iteration 3549: train_loss:1.198188066482544 val_loss1.2651338577270508\n",
            "iteration 3550: train_loss:1.1975202560424805 val_loss1.2645756006240845\n",
            "iteration 3551: train_loss:1.196854829788208 val_loss1.26401948928833\n",
            "iteration 3552: train_loss:1.1961910724639893 val_loss1.2634655237197876\n",
            "iteration 3553: train_loss:1.195529580116272 val_loss1.262913703918457\n",
            "iteration 3554: train_loss:1.1948699951171875 val_loss1.2623635530471802\n",
            "iteration 3555: train_loss:1.1942124366760254 val_loss1.2618159055709839\n",
            "iteration 3556: train_loss:1.1935575008392334 val_loss1.2612704038619995\n",
            "iteration 3557: train_loss:1.1929044723510742 val_loss1.2607269287109375\n",
            "iteration 3558: train_loss:1.1922537088394165 val_loss1.2601855993270874\n",
            "iteration 3559: train_loss:1.1916050910949707 val_loss1.2596465349197388\n",
            "iteration 3560: train_loss:1.1909587383270264 val_loss1.2591099739074707\n",
            "iteration 3561: train_loss:1.190314531326294 val_loss1.258575201034546\n",
            "iteration 3562: train_loss:1.1896727085113525 val_loss1.2580426931381226\n",
            "iteration 3563: train_loss:1.189032793045044 val_loss1.2575123310089111\n",
            "iteration 3564: train_loss:1.1883949041366577 val_loss1.2569838762283325\n",
            "iteration 3565: train_loss:1.187759280204773 val_loss1.2564572095870972\n",
            "iteration 3566: train_loss:1.1871258020401 val_loss1.2559329271316528\n",
            "iteration 3567: train_loss:1.1864943504333496 val_loss1.2554107904434204\n",
            "iteration 3568: train_loss:1.185865044593811 val_loss1.2548904418945312\n",
            "iteration 3569: train_loss:1.185238003730774 val_loss1.2543723583221436\n",
            "iteration 3570: train_loss:1.18461275100708 val_loss1.2538560628890991\n",
            "iteration 3571: train_loss:1.1839896440505981 val_loss1.2533419132232666\n",
            "iteration 3572: train_loss:1.1833686828613281 val_loss1.2528294324874878\n",
            "iteration 3573: train_loss:1.182749629020691 val_loss1.252319097518921\n",
            "iteration 3574: train_loss:1.182132363319397 val_loss1.251810908317566\n",
            "iteration 3575: train_loss:1.1815173625946045 val_loss1.2513045072555542\n",
            "iteration 3576: train_loss:1.1809037923812866 val_loss1.2507996559143066\n",
            "iteration 3577: train_loss:1.180292010307312 val_loss1.250296950340271\n",
            "iteration 3578: train_loss:1.1796820163726807 val_loss1.24979567527771\n",
            "iteration 3579: train_loss:1.1790738105773926 val_loss1.2492965459823608\n",
            "iteration 3580: train_loss:1.1784679889678955 val_loss1.248799443244934\n",
            "iteration 3581: train_loss:1.1778641939163208 val_loss1.2483044862747192\n",
            "iteration 3582: train_loss:1.1772621870040894 val_loss1.2478115558624268\n",
            "iteration 3583: train_loss:1.176662802696228 val_loss1.2473206520080566\n",
            "iteration 3584: train_loss:1.1760647296905518 val_loss1.2468316555023193\n",
            "iteration 3585: train_loss:1.1754686832427979 val_loss1.2463446855545044\n",
            "iteration 3586: train_loss:1.1748746633529663 val_loss1.245859146118164\n",
            "iteration 3587: train_loss:1.174282431602478 val_loss1.2453759908676147\n",
            "iteration 3588: train_loss:1.1736921072006226 val_loss1.2448943853378296\n",
            "iteration 3589: train_loss:1.1731038093566895 val_loss1.2444148063659668\n",
            "iteration 3590: train_loss:1.1725175380706787 val_loss1.2439370155334473\n",
            "iteration 3591: train_loss:1.1719331741333008 val_loss1.2434611320495605\n",
            "iteration 3592: train_loss:1.171350359916687 val_loss1.2429869174957275\n",
            "iteration 3593: train_loss:1.1707695722579956 val_loss1.2425142526626587\n",
            "iteration 3594: train_loss:1.1701905727386475 val_loss1.2420432567596436\n",
            "iteration 3595: train_loss:1.1696133613586426 val_loss1.2415744066238403\n",
            "iteration 3596: train_loss:1.1690375804901123 val_loss1.2411068677902222\n",
            "iteration 3597: train_loss:1.168463945388794 val_loss1.2406412363052368\n",
            "iteration 3598: train_loss:1.1678916215896606 val_loss1.2401773929595947\n",
            "iteration 3599: train_loss:1.1673215627670288 val_loss1.239715337753296\n",
            "iteration 3600: train_loss:1.1667530536651611 val_loss1.2392551898956299\n",
            "iteration 3601: train_loss:1.1661865711212158 val_loss1.2387969493865967\n",
            "iteration 3602: train_loss:1.1656218767166138 val_loss1.2383403778076172\n",
            "iteration 3603: train_loss:1.1650587320327759 val_loss1.237885594367981\n",
            "iteration 3604: train_loss:1.1644978523254395 val_loss1.237432599067688\n",
            "iteration 3605: train_loss:1.1639384031295776 val_loss1.2369811534881592\n",
            "iteration 3606: train_loss:1.1633808612823486 val_loss1.2365314960479736\n",
            "iteration 3607: train_loss:1.162825107574463 val_loss1.236083745956421\n",
            "iteration 3608: train_loss:1.1622709035873413 val_loss1.2356373071670532\n",
            "iteration 3609: train_loss:1.161718487739563 val_loss1.2351926565170288\n",
            "iteration 3610: train_loss:1.161168098449707 val_loss1.234749674797058\n",
            "iteration 3611: train_loss:1.1606192588806152 val_loss1.2343083620071411\n",
            "iteration 3612: train_loss:1.1600720882415771 val_loss1.2338687181472778\n",
            "iteration 3613: train_loss:1.1595264673233032 val_loss1.2334306240081787\n",
            "iteration 3614: train_loss:1.1589828729629517 val_loss1.2329940795898438\n",
            "iteration 3615: train_loss:1.1584410667419434 val_loss1.2325592041015625\n",
            "iteration 3616: train_loss:1.1579008102416992 val_loss1.2321263551712036\n",
            "iteration 3617: train_loss:1.1573621034622192 val_loss1.2316946983337402\n",
            "iteration 3618: train_loss:1.156825065612793 val_loss1.2312647104263306\n",
            "iteration 3619: train_loss:1.1562893390655518 val_loss1.230836033821106\n",
            "iteration 3620: train_loss:1.1557552814483643 val_loss1.2304089069366455\n",
            "iteration 3621: train_loss:1.1552226543426514 val_loss1.2299833297729492\n",
            "iteration 3622: train_loss:1.1546918153762817 val_loss1.2295594215393066\n",
            "iteration 3623: train_loss:1.1541625261306763 val_loss1.2291373014450073\n",
            "iteration 3624: train_loss:1.153634786605835 val_loss1.228716492652893\n",
            "iteration 3625: train_loss:1.1531085968017578 val_loss1.2282973527908325\n",
            "iteration 3626: train_loss:1.152584195137024 val_loss1.2278798818588257\n",
            "iteration 3627: train_loss:1.1520612239837646 val_loss1.227463960647583\n",
            "iteration 3628: train_loss:1.151539921760559 val_loss1.2270493507385254\n",
            "iteration 3629: train_loss:1.1510201692581177 val_loss1.2266367673873901\n",
            "iteration 3630: train_loss:1.1505018472671509 val_loss1.22622549533844\n",
            "iteration 3631: train_loss:1.149985432624817 val_loss1.225816011428833\n",
            "iteration 3632: train_loss:1.149470329284668 val_loss1.2254079580307007\n",
            "iteration 3633: train_loss:1.1489567756652832 val_loss1.2250012159347534\n",
            "iteration 3634: train_loss:1.1484451293945312 val_loss1.224596381187439\n",
            "iteration 3635: train_loss:1.1479347944259644 val_loss1.2241933345794678\n",
            "iteration 3636: train_loss:1.1474260091781616 val_loss1.2237917184829712\n",
            "iteration 3637: train_loss:1.1469190120697021 val_loss1.2233912944793701\n",
            "iteration 3638: train_loss:1.1464130878448486 val_loss1.2229924201965332\n",
            "iteration 3639: train_loss:1.1459089517593384 val_loss1.2225950956344604\n",
            "iteration 3640: train_loss:1.1454064846038818 val_loss1.2221994400024414\n",
            "iteration 3641: train_loss:1.1449048519134521 val_loss1.2218047380447388\n",
            "iteration 3642: train_loss:1.1444051265716553 val_loss1.2214118242263794\n",
            "iteration 3643: train_loss:1.143906831741333 val_loss1.2210205793380737\n",
            "iteration 3644: train_loss:1.1434098482131958 val_loss1.2206306457519531\n",
            "iteration 3645: train_loss:1.1429144144058228 val_loss1.2202421426773071\n",
            "iteration 3646: train_loss:1.1424206495285034 val_loss1.2198550701141357\n",
            "iteration 3647: train_loss:1.14192795753479 val_loss1.219469428062439\n",
            "iteration 3648: train_loss:1.1414369344711304 val_loss1.2190852165222168\n",
            "iteration 3649: train_loss:1.1409473419189453 val_loss1.2187020778656006\n",
            "iteration 3650: train_loss:1.1404591798782349 val_loss1.2183208465576172\n",
            "iteration 3651: train_loss:1.1399725675582886 val_loss1.2179405689239502\n",
            "iteration 3652: train_loss:1.139487385749817 val_loss1.2175617218017578\n",
            "iteration 3653: train_loss:1.1390035152435303 val_loss1.2171841859817505\n",
            "iteration 3654: train_loss:1.1385210752487183 val_loss1.2168081998825073\n",
            "iteration 3655: train_loss:1.1380400657653809 val_loss1.2164332866668701\n",
            "iteration 3656: train_loss:1.137560486793518 val_loss1.2160600423812866\n",
            "iteration 3657: train_loss:1.1370822191238403 val_loss1.2156882286071777\n",
            "iteration 3658: train_loss:1.1366052627563477 val_loss1.2153173685073853\n",
            "iteration 3659: train_loss:1.13612961769104 val_loss1.214948058128357\n",
            "iteration 3660: train_loss:1.1356555223464966 val_loss1.2145802974700928\n",
            "iteration 3661: train_loss:1.1351827383041382 val_loss1.2142138481140137\n",
            "iteration 3662: train_loss:1.1347112655639648 val_loss1.21384859085083\n",
            "iteration 3663: train_loss:1.1342412233352661 val_loss1.213484525680542\n",
            "iteration 3664: train_loss:1.133772611618042 val_loss1.2131221294403076\n",
            "iteration 3665: train_loss:1.1333054304122925 val_loss1.2127609252929688\n",
            "iteration 3666: train_loss:1.1328394412994385 val_loss1.2124011516571045\n",
            "iteration 3667: train_loss:1.132374882698059 val_loss1.2120429277420044\n",
            "iteration 3668: train_loss:1.1319115161895752 val_loss1.2116856575012207\n",
            "iteration 3669: train_loss:1.1314494609832764 val_loss1.211329698562622\n",
            "iteration 3670: train_loss:1.1309884786605835 val_loss1.210974931716919\n",
            "iteration 3671: train_loss:1.1305289268493652 val_loss1.2106214761734009\n",
            "iteration 3672: train_loss:1.1300708055496216 val_loss1.2102694511413574\n",
            "iteration 3673: train_loss:1.129613995552063 val_loss1.2099186182022095\n",
            "iteration 3674: train_loss:1.1291584968566895 val_loss1.2095692157745361\n",
            "iteration 3675: train_loss:1.1287041902542114 val_loss1.2092208862304688\n",
            "iteration 3676: train_loss:1.128251314163208 val_loss1.2088741064071655\n",
            "iteration 3677: train_loss:1.1277997493743896 val_loss1.2085286378860474\n",
            "iteration 3678: train_loss:1.1273492574691772 val_loss1.2081841230392456\n",
            "iteration 3679: train_loss:1.1268998384475708 val_loss1.2078412771224976\n",
            "iteration 3680: train_loss:1.126451849937439 val_loss1.2074995040893555\n",
            "iteration 3681: train_loss:1.1260050535202026 val_loss1.2071588039398193\n",
            "iteration 3682: train_loss:1.1255595684051514 val_loss1.2068196535110474\n",
            "iteration 3683: train_loss:1.1251150369644165 val_loss1.2064812183380127\n",
            "iteration 3684: train_loss:1.1246718168258667 val_loss1.2061439752578735\n",
            "iteration 3685: train_loss:1.124229907989502 val_loss1.2058078050613403\n",
            "iteration 3686: train_loss:1.1237893104553223 val_loss1.2054729461669922\n",
            "iteration 3687: train_loss:1.123349905014038 val_loss1.20513916015625\n",
            "iteration 3688: train_loss:1.1229116916656494 val_loss1.2048065662384033\n",
            "iteration 3689: train_loss:1.1224746704101562 val_loss1.204474925994873\n",
            "iteration 3690: train_loss:1.1220386028289795 val_loss1.2041447162628174\n",
            "iteration 3691: train_loss:1.121604084968567 val_loss1.2038155794143677\n",
            "iteration 3692: train_loss:1.1211705207824707 val_loss1.203487515449524\n",
            "iteration 3693: train_loss:1.12073814868927 val_loss1.2031606435775757\n",
            "iteration 3694: train_loss:1.1203070878982544 val_loss1.2028348445892334\n",
            "iteration 3695: train_loss:1.1198770999908447 val_loss1.2025099992752075\n",
            "iteration 3696: train_loss:1.1194483041763306 val_loss1.2021857500076294\n",
            "iteration 3697: train_loss:1.1190208196640015 val_loss1.2018626928329468\n",
            "iteration 3698: train_loss:1.1185944080352783 val_loss1.2015407085418701\n",
            "iteration 3699: train_loss:1.1181693077087402 val_loss1.201219916343689\n",
            "iteration 3700: train_loss:1.117745280265808 val_loss1.2009001970291138\n",
            "iteration 3701: train_loss:1.1173224449157715 val_loss1.2005815505981445\n",
            "iteration 3702: train_loss:1.1169006824493408 val_loss1.2002639770507812\n",
            "iteration 3703: train_loss:1.1164799928665161 val_loss1.1999473571777344\n",
            "iteration 3704: train_loss:1.1160606145858765 val_loss1.199631690979004\n",
            "iteration 3705: train_loss:1.1156423091888428 val_loss1.1993168592453003\n",
            "iteration 3706: train_loss:1.115225076675415 val_loss1.1990032196044922\n",
            "iteration 3707: train_loss:1.1148089170455933 val_loss1.1986907720565796\n",
            "iteration 3708: train_loss:1.1143938302993774 val_loss1.198379397392273\n",
            "iteration 3709: train_loss:1.1139800548553467 val_loss1.1980688571929932\n",
            "iteration 3710: train_loss:1.1135672330856323 val_loss1.1977591514587402\n",
            "iteration 3711: train_loss:1.1131556034088135 val_loss1.1974502801895142\n",
            "iteration 3712: train_loss:1.1127448081970215 val_loss1.1971423625946045\n",
            "iteration 3713: train_loss:1.1123350858688354 val_loss1.1968351602554321\n",
            "iteration 3714: train_loss:1.1119264364242554 val_loss1.1965289115905762\n",
            "iteration 3715: train_loss:1.1115186214447021 val_loss1.1962238550186157\n",
            "iteration 3716: train_loss:1.111112117767334 val_loss1.1959199905395508\n",
            "iteration 3717: train_loss:1.1107065677642822 val_loss1.1956168413162231\n",
            "iteration 3718: train_loss:1.1103020906448364 val_loss1.1953145265579224\n",
            "iteration 3719: train_loss:1.109898567199707 val_loss1.195013403892517\n",
            "iteration 3720: train_loss:1.1094961166381836 val_loss1.1947129964828491\n",
            "iteration 3721: train_loss:1.109094500541687 val_loss1.1944137811660767\n",
            "iteration 3722: train_loss:1.108694076538086 val_loss1.1941155195236206\n",
            "iteration 3723: train_loss:1.1082946062088013 val_loss1.193818211555481\n",
            "iteration 3724: train_loss:1.1078962087631226 val_loss1.1935219764709473\n",
            "iteration 3725: train_loss:1.1074988842010498 val_loss1.1932264566421509\n",
            "iteration 3726: train_loss:1.107102394104004 val_loss1.1929320096969604\n",
            "iteration 3727: train_loss:1.1067068576812744 val_loss1.1926383972167969\n",
            "iteration 3728: train_loss:1.10631263256073 val_loss1.1923459768295288\n",
            "iteration 3729: train_loss:1.105919361114502 val_loss1.1920546293258667\n",
            "iteration 3730: train_loss:1.1055271625518799 val_loss1.1917643547058105\n",
            "iteration 3731: train_loss:1.1051360368728638 val_loss1.1914751529693604\n",
            "iteration 3732: train_loss:1.104745864868164 val_loss1.1911869049072266\n",
            "iteration 3733: train_loss:1.1043566465377808 val_loss1.1908994913101196\n",
            "iteration 3734: train_loss:1.1039683818817139 val_loss1.1906129121780396\n",
            "iteration 3735: train_loss:1.1035813093185425 val_loss1.1903272867202759\n",
            "iteration 3736: train_loss:1.1031949520111084 val_loss1.1900428533554077\n",
            "iteration 3737: train_loss:1.1028097867965698 val_loss1.1897591352462769\n",
            "iteration 3738: train_loss:1.1024250984191895 val_loss1.1894762516021729\n",
            "iteration 3739: train_loss:1.1020417213439941 val_loss1.1891942024230957\n",
            "iteration 3740: train_loss:1.1016589403152466 val_loss1.1889132261276245\n",
            "iteration 3741: train_loss:1.101277232170105 val_loss1.188632845878601\n",
            "iteration 3742: train_loss:1.1008962392807007 val_loss1.1883535385131836\n",
            "iteration 3743: train_loss:1.1005163192749023 val_loss1.1880749464035034\n",
            "iteration 3744: train_loss:1.1001371145248413 val_loss1.18779718875885\n",
            "iteration 3745: train_loss:1.0997589826583862 val_loss1.1875202655792236\n",
            "iteration 3746: train_loss:1.099381685256958 val_loss1.1872444152832031\n",
            "iteration 3747: train_loss:1.0990054607391357 val_loss1.186969518661499\n",
            "iteration 3748: train_loss:1.0986300706863403 val_loss1.1866953372955322\n",
            "iteration 3749: train_loss:1.0982556343078613 val_loss1.1864219903945923\n",
            "iteration 3750: train_loss:1.0978820323944092 val_loss1.1861494779586792\n",
            "iteration 3751: train_loss:1.0975092649459839 val_loss1.185878038406372\n",
            "iteration 3752: train_loss:1.0971373319625854 val_loss1.1856073141098022\n",
            "iteration 3753: train_loss:1.0967663526535034 val_loss1.1853374242782593\n",
            "iteration 3754: train_loss:1.0963963270187378 val_loss1.1850682497024536\n",
            "iteration 3755: train_loss:1.096027135848999 val_loss1.184800148010254\n",
            "iteration 3756: train_loss:1.095658540725708 val_loss1.184532642364502\n",
            "iteration 3757: train_loss:1.0952908992767334 val_loss1.1842659711837769\n",
            "iteration 3758: train_loss:1.0949240922927856 val_loss1.1840003728866577\n",
            "iteration 3759: train_loss:1.0945581197738647 val_loss1.1837353706359863\n",
            "iteration 3760: train_loss:1.0941929817199707 val_loss1.1834709644317627\n",
            "iteration 3761: train_loss:1.093828797340393 val_loss1.1832071542739868\n",
            "iteration 3762: train_loss:1.0934653282165527 val_loss1.1829442977905273\n",
            "iteration 3763: train_loss:1.0931026935577393 val_loss1.182681918144226\n",
            "iteration 3764: train_loss:1.0927410125732422 val_loss1.1824207305908203\n",
            "iteration 3765: train_loss:1.0923799276351929 val_loss1.1821599006652832\n",
            "iteration 3766: train_loss:1.0920196771621704 val_loss1.1818997859954834\n",
            "iteration 3767: train_loss:1.0916603803634644 val_loss1.181640386581421\n",
            "iteration 3768: train_loss:1.091301679611206 val_loss1.1813818216323853\n",
            "iteration 3769: train_loss:1.090943694114685 val_loss1.1811238527297974\n",
            "iteration 3770: train_loss:1.090586543083191 val_loss1.1808667182922363\n",
            "iteration 3771: train_loss:1.0902299880981445 val_loss1.1806100606918335\n",
            "iteration 3772: train_loss:1.0898743867874146 val_loss1.180354356765747\n",
            "iteration 3773: train_loss:1.0895195007324219 val_loss1.1800994873046875\n",
            "iteration 3774: train_loss:1.0891653299331665 val_loss1.1798450946807861\n",
            "iteration 3775: train_loss:1.088811993598938 val_loss1.1795916557312012\n",
            "iteration 3776: train_loss:1.0884593725204468 val_loss1.179338812828064\n",
            "iteration 3777: train_loss:1.0881075859069824 val_loss1.1790868043899536\n",
            "iteration 3778: train_loss:1.0877565145492554 val_loss1.1788355112075806\n",
            "iteration 3779: train_loss:1.0874062776565552 val_loss1.1785850524902344\n",
            "iteration 3780: train_loss:1.0870568752288818 val_loss1.178335189819336\n",
            "iteration 3781: train_loss:1.0867081880569458 val_loss1.1780864000320435\n",
            "iteration 3782: train_loss:1.086360216140747 val_loss1.1778377294540405\n",
            "iteration 3783: train_loss:1.0860131978988647 val_loss1.177590012550354\n",
            "iteration 3784: train_loss:1.0856667757034302 val_loss1.1773431301116943\n",
            "iteration 3785: train_loss:1.085321068763733 val_loss1.1770967245101929\n",
            "iteration 3786: train_loss:1.084976077079773 val_loss1.1768512725830078\n",
            "iteration 3787: train_loss:1.0846320390701294 val_loss1.1766059398651123\n",
            "iteration 3788: train_loss:1.0842883586883545 val_loss1.1763615608215332\n",
            "iteration 3789: train_loss:1.0839455127716064 val_loss1.1761176586151123\n",
            "iteration 3790: train_loss:1.0836032629013062 val_loss1.1758744716644287\n",
            "iteration 3791: train_loss:1.0832618474960327 val_loss1.1756317615509033\n",
            "iteration 3792: train_loss:1.082920789718628 val_loss1.1753895282745361\n",
            "iteration 3793: train_loss:1.08258056640625 val_loss1.1751478910446167\n",
            "iteration 3794: train_loss:1.0822407007217407 val_loss1.1749067306518555\n",
            "iteration 3795: train_loss:1.0819016695022583 val_loss1.174666404724121\n",
            "iteration 3796: train_loss:1.0815632343292236 val_loss1.1744263172149658\n",
            "iteration 3797: train_loss:1.0812256336212158 val_loss1.1741873025894165\n",
            "iteration 3798: train_loss:1.0808885097503662 val_loss1.1739487648010254\n",
            "iteration 3799: train_loss:1.0805519819259644 val_loss1.1737111806869507\n",
            "iteration 3800: train_loss:1.0802162885665894 val_loss1.1734737157821655\n",
            "iteration 3801: train_loss:1.0798810720443726 val_loss1.1732372045516968\n",
            "iteration 3802: train_loss:1.079546570777893 val_loss1.1730012893676758\n",
            "iteration 3803: train_loss:1.0792127847671509 val_loss1.1727659702301025\n",
            "iteration 3804: train_loss:1.078879714012146 val_loss1.1725313663482666\n",
            "iteration 3805: train_loss:1.0785472393035889 val_loss1.1722971200942993\n",
            "iteration 3806: train_loss:1.078215479850769 val_loss1.1720637083053589\n",
            "iteration 3807: train_loss:1.077884316444397 val_loss1.171830654144287\n",
            "iteration 3808: train_loss:1.0775537490844727 val_loss1.171597957611084\n",
            "iteration 3809: train_loss:1.0772238969802856 val_loss1.1713660955429077\n",
            "iteration 3810: train_loss:1.0768944025039673 val_loss1.1711347103118896\n",
            "iteration 3811: train_loss:1.0765656232833862 val_loss1.1709038019180298\n",
            "iteration 3812: train_loss:1.0762375593185425 val_loss1.1706734895706177\n",
            "iteration 3813: train_loss:1.0759100914001465 val_loss1.1704438924789429\n",
            "iteration 3814: train_loss:1.0755831003189087 val_loss1.1702144145965576\n",
            "iteration 3815: train_loss:1.0752567052841187 val_loss1.1699857711791992\n",
            "iteration 3816: train_loss:1.0749309062957764 val_loss1.169757604598999\n",
            "iteration 3817: train_loss:1.0746057033538818 val_loss1.1695301532745361\n",
            "iteration 3818: train_loss:1.0742812156677246 val_loss1.169303297996521\n",
            "iteration 3819: train_loss:1.0739572048187256 val_loss1.1690765619277954\n",
            "iteration 3820: train_loss:1.0736337900161743 val_loss1.1688507795333862\n",
            "iteration 3821: train_loss:1.0733109712600708 val_loss1.1686253547668457\n",
            "iteration 3822: train_loss:1.072988748550415 val_loss1.1684004068374634\n",
            "iteration 3823: train_loss:1.072667121887207 val_loss1.168176293373108\n",
            "iteration 3824: train_loss:1.0723462104797363 val_loss1.1679521799087524\n",
            "iteration 3825: train_loss:1.0720257759094238 val_loss1.1677290201187134\n",
            "iteration 3826: train_loss:1.0717058181762695 val_loss1.1675060987472534\n",
            "iteration 3827: train_loss:1.071386456489563 val_loss1.1672837734222412\n",
            "iteration 3828: train_loss:1.071067452430725 val_loss1.1670619249343872\n",
            "iteration 3829: train_loss:1.070749282836914 val_loss1.1668405532836914\n",
            "iteration 3830: train_loss:1.0704314708709717 val_loss1.1666197776794434\n",
            "iteration 3831: train_loss:1.070114254951477 val_loss1.166399359703064\n",
            "iteration 3832: train_loss:1.0697975158691406 val_loss1.1661792993545532\n",
            "iteration 3833: train_loss:1.069481372833252 val_loss1.1659598350524902\n",
            "iteration 3834: train_loss:1.0691657066345215 val_loss1.1657402515411377\n",
            "iteration 3835: train_loss:1.0688506364822388 val_loss1.1655213832855225\n",
            "iteration 3836: train_loss:1.0685361623764038 val_loss1.1653029918670654\n",
            "iteration 3837: train_loss:1.068222165107727 val_loss1.1650850772857666\n",
            "iteration 3838: train_loss:1.067908763885498 val_loss1.1648677587509155\n",
            "iteration 3839: train_loss:1.0675960779190063 val_loss1.1646509170532227\n",
            "iteration 3840: train_loss:1.0672837495803833 val_loss1.1644346714019775\n",
            "iteration 3841: train_loss:1.0669721364974976 val_loss1.164218544960022\n",
            "iteration 3842: train_loss:1.066660761833191 val_loss1.1640032529830933\n",
            "iteration 3843: train_loss:1.0663501024246216 val_loss1.163788080215454\n",
            "iteration 3844: train_loss:1.0660399198532104 val_loss1.1635733842849731\n",
            "iteration 3845: train_loss:1.065730333328247 val_loss1.1633594036102295\n",
            "iteration 3846: train_loss:1.065421223640442 val_loss1.163145899772644\n",
            "iteration 3847: train_loss:1.065112590789795 val_loss1.1629327535629272\n",
            "iteration 3848: train_loss:1.0648045539855957 val_loss1.162719964981079\n",
            "iteration 3849: train_loss:1.0644969940185547 val_loss1.1625077724456787\n",
            "iteration 3850: train_loss:1.0641899108886719 val_loss1.162296175956726\n",
            "iteration 3851: train_loss:1.0638833045959473 val_loss1.162084698677063\n",
            "iteration 3852: train_loss:1.0635770559310913 val_loss1.1618738174438477\n",
            "iteration 3853: train_loss:1.0632710456848145 val_loss1.1616626977920532\n",
            "iteration 3854: train_loss:1.0629655122756958 val_loss1.161452054977417\n",
            "iteration 3855: train_loss:1.062660574913025 val_loss1.161241888999939\n",
            "iteration 3856: train_loss:1.062355875968933 val_loss1.1610323190689087\n",
            "iteration 3857: train_loss:1.0620520114898682 val_loss1.1608234643936157\n",
            "iteration 3858: train_loss:1.0617483854293823 val_loss1.1606147289276123\n",
            "iteration 3859: train_loss:1.0614451169967651 val_loss1.160406470298767\n",
            "iteration 3860: train_loss:1.0611424446105957 val_loss1.1601988077163696\n",
            "iteration 3861: train_loss:1.0608400106430054 val_loss1.1599913835525513\n",
            "iteration 3862: train_loss:1.0605381727218628 val_loss1.159784197807312\n",
            "iteration 3863: train_loss:1.0602365732192993 val_loss1.1595776081085205\n",
            "iteration 3864: train_loss:1.059935450553894 val_loss1.1593713760375977\n",
            "iteration 3865: train_loss:1.0596346855163574 val_loss1.159165620803833\n",
            "iteration 3866: train_loss:1.059334397315979 val_loss1.158960223197937\n",
            "iteration 3867: train_loss:1.0590343475341797 val_loss1.1587550640106201\n",
            "iteration 3868: train_loss:1.0587350130081177 val_loss1.1585503816604614\n",
            "iteration 3869: train_loss:1.0584360361099243 val_loss1.158346176147461\n",
            "iteration 3870: train_loss:1.0581376552581787 val_loss1.158142328262329\n",
            "iteration 3871: train_loss:1.0578397512435913 val_loss1.1579389572143555\n",
            "iteration 3872: train_loss:1.0575422048568726 val_loss1.1577361822128296\n",
            "iteration 3873: train_loss:1.0572454929351807 val_loss1.1575337648391724\n",
            "iteration 3874: train_loss:1.0569490194320679 val_loss1.1573318243026733\n",
            "iteration 3875: train_loss:1.0566530227661133 val_loss1.1571300029754639\n",
            "iteration 3876: train_loss:1.0563571453094482 val_loss1.1569286584854126\n",
            "iteration 3877: train_loss:1.0560617446899414 val_loss1.1567270755767822\n",
            "iteration 3878: train_loss:1.0557667016983032 val_loss1.1565263271331787\n",
            "iteration 3879: train_loss:1.0554718971252441 val_loss1.1563258171081543\n",
            "iteration 3880: train_loss:1.0551773309707642 val_loss1.156125545501709\n",
            "iteration 3881: train_loss:1.054883360862732 val_loss1.1559253931045532\n",
            "iteration 3882: train_loss:1.0545896291732788 val_loss1.1557258367538452\n",
            "iteration 3883: train_loss:1.0542963743209839 val_loss1.1555265188217163\n",
            "iteration 3884: train_loss:1.0540035963058472 val_loss1.155327558517456\n",
            "iteration 3885: train_loss:1.0537110567092896 val_loss1.1551289558410645\n",
            "iteration 3886: train_loss:1.0534189939498901 val_loss1.1549307107925415\n",
            "iteration 3887: train_loss:1.0531272888183594 val_loss1.1547328233718872\n",
            "iteration 3888: train_loss:1.0528360605239868 val_loss1.154535174369812\n",
            "iteration 3889: train_loss:1.052545189857483 val_loss1.1543378829956055\n",
            "iteration 3890: train_loss:1.0522546768188477 val_loss1.154140830039978\n",
            "iteration 3891: train_loss:1.051964521408081 val_loss1.1539441347122192\n",
            "iteration 3892: train_loss:1.051674723625183 val_loss1.1537476778030396\n",
            "iteration 3893: train_loss:1.0513849258422852 val_loss1.153551459312439\n",
            "iteration 3894: train_loss:1.0510954856872559 val_loss1.1533551216125488\n",
            "iteration 3895: train_loss:1.0508064031600952 val_loss1.1531591415405273\n",
            "iteration 3896: train_loss:1.0505176782608032 val_loss1.1529637575149536\n",
            "iteration 3897: train_loss:1.0502294301986694 val_loss1.1527684926986694\n",
            "iteration 3898: train_loss:1.0499414205551147 val_loss1.1525734663009644\n",
            "iteration 3899: train_loss:1.0496537685394287 val_loss1.152378797531128\n",
            "iteration 3900: train_loss:1.0493664741516113 val_loss1.1521843671798706\n",
            "iteration 3901: train_loss:1.0490795373916626 val_loss1.1519904136657715\n",
            "iteration 3902: train_loss:1.048792839050293 val_loss1.1517969369888306\n",
            "iteration 3903: train_loss:1.0485066175460815 val_loss1.1516035795211792\n",
            "iteration 3904: train_loss:1.0482208728790283 val_loss1.1514105796813965\n",
            "iteration 3905: train_loss:1.0479353666305542 val_loss1.1512178182601929\n",
            "iteration 3906: train_loss:1.0476502180099487 val_loss1.1510255336761475\n",
            "iteration 3907: train_loss:1.0473655462265015 val_loss1.1508336067199707\n",
            "iteration 3908: train_loss:1.0470811128616333 val_loss1.150641918182373\n",
            "iteration 3909: train_loss:1.0467970371246338 val_loss1.150450587272644\n",
            "iteration 3910: train_loss:1.046513319015503 val_loss1.1502594947814941\n",
            "iteration 3911: train_loss:1.0462299585342407 val_loss1.1500686407089233\n",
            "iteration 3912: train_loss:1.0459468364715576 val_loss1.1498780250549316\n",
            "iteration 3913: train_loss:1.0456640720367432 val_loss1.1496875286102295\n",
            "iteration 3914: train_loss:1.0453814268112183 val_loss1.149497151374817\n",
            "iteration 3915: train_loss:1.0450993776321411 val_loss1.1493068933486938\n",
            "iteration 3916: train_loss:1.044817566871643 val_loss1.1491169929504395\n",
            "iteration 3917: train_loss:1.0445358753204346 val_loss1.1489273309707642\n",
            "iteration 3918: train_loss:1.0442545413970947 val_loss1.148737907409668\n",
            "iteration 3919: train_loss:1.0439735651016235 val_loss1.1485487222671509\n",
            "iteration 3920: train_loss:1.043692708015442 val_loss1.148360013961792\n",
            "iteration 3921: train_loss:1.043412446975708 val_loss1.1481715440750122\n",
            "iteration 3922: train_loss:1.0431324243545532 val_loss1.1479830741882324\n",
            "iteration 3923: train_loss:1.0428526401519775 val_loss1.1477949619293213\n",
            "iteration 3924: train_loss:1.0425732135772705 val_loss1.1476069688796997\n",
            "iteration 3925: train_loss:1.0422940254211426 val_loss1.1474192142486572\n",
            "iteration 3926: train_loss:1.0420151948928833 val_loss1.1472316980361938\n",
            "iteration 3927: train_loss:1.0417366027832031 val_loss1.1470444202423096\n",
            "iteration 3928: train_loss:1.041458249092102 val_loss1.1468573808670044\n",
            "iteration 3929: train_loss:1.04118013381958 val_loss1.1466705799102783\n",
            "iteration 3930: train_loss:1.0409022569656372 val_loss1.1464837789535522\n",
            "iteration 3931: train_loss:1.0406246185302734 val_loss1.1462974548339844\n",
            "iteration 3932: train_loss:1.0403474569320679 val_loss1.1461108922958374\n",
            "iteration 3933: train_loss:1.0400702953338623 val_loss1.1459248065948486\n",
            "iteration 3934: train_loss:1.0397934913635254 val_loss1.1457388401031494\n",
            "iteration 3935: train_loss:1.0395170450210571 val_loss1.1455531120300293\n",
            "iteration 3936: train_loss:1.039240837097168 val_loss1.1453677415847778\n",
            "iteration 3937: train_loss:1.0389647483825684 val_loss1.145182490348816\n",
            "iteration 3938: train_loss:1.038689136505127 val_loss1.144997239112854\n",
            "iteration 3939: train_loss:1.038413405418396 val_loss1.1448123455047607\n",
            "iteration 3940: train_loss:1.0381375551223755 val_loss1.1446269750595093\n",
            "iteration 3941: train_loss:1.0378621816635132 val_loss1.144441843032837\n",
            "iteration 3942: train_loss:1.0375865697860718 val_loss1.1442569494247437\n",
            "iteration 3943: train_loss:1.0373114347457886 val_loss1.1440720558166504\n",
            "iteration 3944: train_loss:1.037036418914795 val_loss1.1438871622085571\n",
            "iteration 3945: train_loss:1.0367616415023804 val_loss1.1437026262283325\n",
            "iteration 3946: train_loss:1.036487340927124 val_loss1.1435178518295288\n",
            "iteration 3947: train_loss:1.0362130403518677 val_loss1.1433337926864624\n",
            "iteration 3948: train_loss:1.03593909740448 val_loss1.1431496143341064\n",
            "iteration 3949: train_loss:1.0356652736663818 val_loss1.1429656744003296\n",
            "iteration 3950: train_loss:1.0353914499282837 val_loss1.1427817344665527\n",
            "iteration 3951: train_loss:1.0351179838180542 val_loss1.1425979137420654\n",
            "iteration 3952: train_loss:1.0348447561264038 val_loss1.1424145698547363\n",
            "iteration 3953: train_loss:1.034571647644043 val_loss1.1422309875488281\n",
            "iteration 3954: train_loss:1.0342990159988403 val_loss1.1420475244522095\n",
            "iteration 3955: train_loss:1.0340262651443481 val_loss1.1418639421463013\n",
            "iteration 3956: train_loss:1.0337539911270142 val_loss1.1416805982589722\n",
            "iteration 3957: train_loss:1.0334818363189697 val_loss1.141497254371643\n",
            "iteration 3958: train_loss:1.0332099199295044 val_loss1.141313910484314\n",
            "iteration 3959: train_loss:1.0329383611679077 val_loss1.141130805015564\n",
            "iteration 3960: train_loss:1.0326666831970215 val_loss1.1409478187561035\n",
            "iteration 3961: train_loss:1.0323952436447144 val_loss1.140764832496643\n",
            "iteration 3962: train_loss:1.0321242809295654 val_loss1.1405822038650513\n",
            "iteration 3963: train_loss:1.031853437423706 val_loss1.1403999328613281\n",
            "iteration 3964: train_loss:1.0315829515457153 val_loss1.1402175426483154\n",
            "iteration 3965: train_loss:1.0313125848770142 val_loss1.140035629272461\n",
            "iteration 3966: train_loss:1.031042456626892 val_loss1.139853835105896\n",
            "iteration 3967: train_loss:1.0307726860046387 val_loss1.1396722793579102\n",
            "iteration 3968: train_loss:1.0305031538009644 val_loss1.1394909620285034\n",
            "iteration 3969: train_loss:1.0302337408065796 val_loss1.1393097639083862\n",
            "iteration 3970: train_loss:1.0299646854400635 val_loss1.1391289234161377\n",
            "iteration 3971: train_loss:1.029695749282837 val_loss1.1389480829238892\n",
            "iteration 3972: train_loss:1.0294270515441895 val_loss1.1387673616409302\n",
            "iteration 3973: train_loss:1.029158353805542 val_loss1.1385868787765503\n",
            "iteration 3974: train_loss:1.0288898944854736 val_loss1.1384066343307495\n",
            "iteration 3975: train_loss:1.0286214351654053 val_loss1.1382262706756592\n",
            "iteration 3976: train_loss:1.0283534526824951 val_loss1.1380460262298584\n",
            "iteration 3977: train_loss:1.0280853509902954 val_loss1.137865662574768\n",
            "iteration 3978: train_loss:1.0278174877166748 val_loss1.1376854181289673\n",
            "iteration 3979: train_loss:1.0275497436523438 val_loss1.1375054121017456\n",
            "iteration 3980: train_loss:1.0272823572158813 val_loss1.1373252868652344\n",
            "iteration 3981: train_loss:1.027014970779419 val_loss1.1371455192565918\n",
            "iteration 3982: train_loss:1.0267479419708252 val_loss1.1369656324386597\n",
            "iteration 3983: train_loss:1.026481032371521 val_loss1.1367857456207275\n",
            "iteration 3984: train_loss:1.026214361190796 val_loss1.136606216430664\n",
            "iteration 3985: train_loss:1.02594792842865 val_loss1.1364268064498901\n",
            "iteration 3986: train_loss:1.0256816148757935 val_loss1.1362473964691162\n",
            "iteration 3987: train_loss:1.0254154205322266 val_loss1.1360678672790527\n",
            "iteration 3988: train_loss:1.0251493453979492 val_loss1.1358883380889893\n",
            "iteration 3989: train_loss:1.024883508682251 val_loss1.1357088088989258\n",
            "iteration 3990: train_loss:1.0246176719665527 val_loss1.1355293989181519\n",
            "iteration 3991: train_loss:1.0243520736694336 val_loss1.1353498697280884\n",
            "iteration 3992: train_loss:1.0240864753723145 val_loss1.135170578956604\n",
            "iteration 3993: train_loss:1.0238208770751953 val_loss1.1349914073944092\n",
            "iteration 3994: train_loss:1.0235555171966553 val_loss1.1348119974136353\n",
            "iteration 3995: train_loss:1.0232900381088257 val_loss1.1346325874328613\n",
            "iteration 3996: train_loss:1.0230250358581543 val_loss1.134453296661377\n",
            "iteration 3997: train_loss:1.022760033607483 val_loss1.1342741250991821\n",
            "iteration 3998: train_loss:1.0224952697753906 val_loss1.1340948343276978\n",
            "iteration 3999: train_loss:1.0222302675247192 val_loss1.1339154243469238\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting the learning curves:\n",
        "\n",
        "**Training Loss:**  It tends to decrease as the model learns to better fit the training data. With mini-batch gradient descent, we observe little fluctuations in the training loss due to the use of random mini-batches in each iteration.\n",
        "\n",
        "**Validation Loss:**  It typically follows the training loss trend but might show signs of overfitting after few epochs the gap is increasing.\n",
        "\n",
        "**Comparison with the model without mini-batch gradient descent:**\n",
        "\n",
        "When using mini-batch training, the training and validation loss curves initially appear close for a few epochs. However, as training progresses, the gap between them starts to increase. This is because the mini-batch updates can lead to overfitting on the training data, while the validation loss reflects the model's performance on unseen data. In contrast, training without mini-batches may not show a significant gap between training and validation loss, but this might indicate underfitting (the model is not learning enough from the data).\n",
        "\n"
      ],
      "metadata": {
        "id": "VhQjNy4CWRLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, model accuracy has increased with mini batch gradient descent model.\n",
        "\n"
      ],
      "metadata": {
        "id": "cReGfZy2WZjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Modify your update_parameters method to update parameters using Nesterov Momentum (5pt) .**\n",
        "\n",
        "The updae rule for nesterov Momentum is as follows:\n",
        "\n",
        "= \n",
        "\n",
        "=+\n",
        "\n",
        "Where  is the gradient of loss with respect to the      (..,  ) and [0,1] is momentum decay rate ( it should be provided as input to the update_parameter method).\n",
        " is initially zero.\n",
        "\n",
        "In your initialize_parameters method, you should define a  variable for each parameter (i.e., weights and biases) with the same shape as the parameter and set it to zero.\n",
        "Try re-training your neural network with different values of  ( e.g., 0.9, 0.95, and 0.99) and interpret the learning curves."
      ],
      "metadata": {
        "id": "sXrhSPfYfcrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(nx, nh1, nh2, ny):\n",
        "    tf.random.set_seed(20)\n",
        "    W1 = tf.Variable(tf.random.normal(shape=(nh1, nx), stddev=0.01), name=\"W1\")\n",
        "    b1 = tf.Variable(tf.zeros(shape=(nh1, 1), name=\"b1\"))\n",
        "    W2 = tf.Variable(tf.random.normal(shape=(nh2, nh1), stddev=0.01), name=\"W2\")\n",
        "    b2 = tf.Variable(tf.zeros(shape=(nh2, 1), name=\"b2\"))\n",
        "    W3 = tf.Variable(tf.random.normal(shape=(ny, nh2), stddev=0.01), name=\"W3\")\n",
        "    b3 = tf.Variable(tf.zeros(shape=(ny, 1), name=\"b3\"))\n",
        "\n",
        "    # Initialize velocity for each parameter\n",
        "    v_W1 = tf.Variable(tf.zeros_like(W1))\n",
        "    v_b1 = tf.Variable(tf.zeros_like(b1))\n",
        "    v_W2 = tf.Variable(tf.zeros_like(W2))\n",
        "    v_b2 = tf.Variable(tf.zeros_like(b2))\n",
        "    v_W3 = tf.Variable(tf.zeros_like(W3))\n",
        "    v_b3 = tf.Variable(tf.zeros_like(b3))\n",
        "\n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
        "    velocities = {\"v_W1\": v_W1, \"v_b1\": v_b1, \"v_W2\": v_W2, \"v_b2\": v_b2, \"v_W3\": v_W3, \"v_b3\": v_b3}\n",
        "\n",
        "    return parameters, velocities\n",
        "\n",
        "# Modify update_parameters to use Nesterov Momentum\n",
        "def update_parameters(parameters, velocities, gradients, learning_rate, mu):\n",
        "    for param, grad, vel in zip(parameters.values(), gradients, velocities.values()):\n",
        "        # Update velocity\n",
        "        vel.assign(mu * vel - learning_rate * grad)\n",
        "        # Update parameter using velocity\n",
        "        param.assign_add(mu * vel - learning_rate * grad)\n",
        "\n",
        "    return parameters\n",
        "\n",
        "#Forward Pass:\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "\n",
        "    Z1 = tf.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.matmul(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.matmul(parameters[\"W3\"], A2) + parameters[\"b3\"]\n",
        "\n",
        "    # Apply softmax activation for multi-class classification\n",
        "    Yhat = tf.nn.softmax(Z3, axis=0)\n",
        "\n",
        "    return Yhat\n",
        "\n",
        "#Loss calculations\n",
        "def compute_loss(Y, Yhat):\n",
        "    # Cross-entropy loss\n",
        "    loss = -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(Yhat + 1e-10), axis=0))\n",
        "    return loss\n",
        "\n",
        "#Backward Pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters.values())\n",
        "    return gradients\n",
        "\n",
        "# Create a new create_nn_model function to utilize the modified update_parameters\n",
        "def create_nn_model_mu(train_X, train_Y, val_X, val_Y, num_iterations, learning_rate, nh1, nh2, batch_size, mu):\n",
        "    nx, m = train_X.shape\n",
        "    ny = train_Y.shape[0]\n",
        "\n",
        "    parameters, velocities = initialize_parameters(nx, nh1, nh2, ny)\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    # Adjusted calculation for number of batches\n",
        "    num_batches = (m + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        epoch_train_loss = 0  # Initialize epoch training loss\n",
        "        train_loss = []   # Initialize train_loss\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = min(start + batch_size, m)\n",
        "            X_batch = train_X[:, start:end]\n",
        "            Y_batch = train_Y[:, start:end]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                train_Yhat = forward_pass(parameters, tf.convert_to_tensor(X_batch, dtype=tf.float32))\n",
        "                train_loss = compute_loss(tf.convert_to_tensor(Y_batch, dtype=tf.float32), train_Yhat)\n",
        "\n",
        "            gradients = backward_pass(parameters, train_loss, tape)\n",
        "            parameters = update_parameters(parameters, velocities, gradients, learning_rate, mu)\n",
        "\n",
        "            epoch_train_loss = epoch_train_loss + train_loss.numpy()  # Accumulate loss for the current batch\n",
        "\n",
        "\n",
        "        epoch_train_loss = epoch_train_loss/num_batches\n",
        "        train_losses.append(epoch_train_loss)  # Append epoch loss to train_losses\n",
        "\n",
        "\n",
        "        # Calculate validation loss after each epoch\n",
        "        val_Yhat = forward_pass(parameters, tf.convert_to_tensor(val_X, dtype=tf.float32))\n",
        "        val_loss = compute_loss(tf.convert_to_tensor(val_Y, dtype=tf.float32), val_Yhat)\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        print(\"epoch {}: train_loss:{} val_loss{}\".format(i, epoch_train_loss, val_loss.numpy()))\n",
        "\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return parameters, history\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_iterations = 30\n",
        "nh1 = 128\n",
        "nh2 = 64\n",
        "batch_size = 32\n",
        "# Different values of momentum\n",
        "mu_list = [0.9, 0.95, 0.99]\n",
        "\n",
        "# Train the model with different values of mu\n",
        "for mu in mu_list:\n",
        "    print(f\"Training with mu={mu}\")\n",
        "    parameters_mu, history = create_nn_model_mu(train_X, train_Y, test_X, test_Y, num_iterations, learning_rate, nh1, nh2, batch_size, mu)\n",
        "    plt.figure()\n",
        "    plt.plot(history['train_loss'], 'b', label=f'Training Loss (mu={mu})')\n",
        "    plt.plot(history['val_loss'], 'r', label=f'Validation Loss (mu={mu})')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Loss vs Iterations (mu={mu})')\n",
        "\n",
        "    # Use the trained parameters to make predictions on the test set\n",
        "    test_predictions_mu = predict(parameters_mu, tf.convert_to_tensor(test_X, dtype=tf.float32))\n",
        "\n",
        "    # Compare predicted labels with true labels to calculate test accuracy\n",
        "    correct_predictions = np.sum(test_predictions_mu == np.argmax(test_Y, axis=0))\n",
        "    total_samples = test_Y.shape[1]\n",
        "    test_accuracy = correct_predictions / total_samples * 100\n",
        "    print(f\"Test Accuracy (mu={mu}): {test_accuracy:.1f}%\")\n",
        "\n",
        "    # Plot a few images in the test set along with their predicted labels\n",
        "    num_images_to_plot = 5\n",
        "    random_indices = np.random.choice(test_X.shape[1], num_images_to_plot, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    for i, idx in enumerate(random_indices, 1):\n",
        "        plt.subplot(1, num_images_to_plot, i)\n",
        "        plt.imshow(test_X[:, idx].reshape(40, 40), cmap='gray')\n",
        "        plt.title(\"True Label: {}\\nPredicted Label: {}\".format(test_Y[:, idx].argmax(), test_predictions_mu[idx]))\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QCqF94v3ft9T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "718237c6-7607-4d41-d8d7-9367fd8f8a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with mu=0.9\n",
            "epoch 0: train_loss:2.3026795559101276 val_loss2.302518844604492\n",
            "epoch 1: train_loss:2.302526684494706 val_loss2.302403211593628\n",
            "epoch 2: train_loss:2.3023893446535677 val_loss2.3022961616516113\n",
            "epoch 3: train_loss:2.302260751122827 val_loss2.3021910190582275\n",
            "epoch 4: train_loss:2.3021346728006997 val_loss2.3020846843719482\n",
            "epoch 5: train_loss:2.302006283321896 val_loss2.3019726276397705\n",
            "epoch 6: train_loss:2.30187041480262 val_loss2.3018503189086914\n",
            "epoch 7: train_loss:2.3017218198862164 val_loss2.301710367202759\n",
            "epoch 8: train_loss:2.3015548645913064 val_loss2.301548480987549\n",
            "epoch 9: train_loss:2.301362254598119 val_loss2.3013556003570557\n",
            "epoch 10: train_loss:2.301133714280687 val_loss2.3011207580566406\n",
            "epoch 11: train_loss:2.3008573785558477 val_loss2.3008313179016113\n",
            "epoch 12: train_loss:2.300517110137252 val_loss2.3004679679870605\n",
            "epoch 13: train_loss:2.300088802973429 val_loss2.300001621246338\n",
            "epoch 14: train_loss:2.299538940996737 val_loss2.299393653869629\n",
            "epoch 15: train_loss:2.298817860113608 val_loss2.298581600189209\n",
            "epoch 16: train_loss:2.2978463065516843 val_loss2.29746675491333\n",
            "epoch 17: train_loss:2.296501829817488 val_loss2.2958927154541016\n",
            "epoch 18: train_loss:2.294579450074617 val_loss2.2935872077941895\n",
            "epoch 19: train_loss:2.29171288120854 val_loss2.2900474071502686\n",
            "epoch 20: train_loss:2.2872102518339417 val_loss2.284292221069336\n",
            "epoch 21: train_loss:2.2796667288015553 val_loss2.274247407913208\n",
            "epoch 22: train_loss:2.26601724796467 val_loss2.255236864089966\n",
            "epoch 23: train_loss:2.239334084966161 val_loss2.2168524265289307\n",
            "epoch 24: train_loss:2.1860620266682393 val_loss2.1431922912597656\n",
            "epoch 25: train_loss:2.0935512759664037 val_loss2.032331943511963\n",
            "epoch 26: train_loss:1.9628361431328025 val_loss1.8891485929489136\n",
            "epoch 27: train_loss:1.7892098652349937 val_loss1.7060655355453491\n",
            "epoch 28: train_loss:1.5849501055640143 val_loss1.5197904109954834\n",
            "epoch 29: train_loss:1.4032861284307532 val_loss1.3796051740646362\n",
            "Test Accuracy (mu=0.9): 55.2%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with mu=0.95\n",
            "epoch 0: train_loss:2.302724883363054 val_loss2.302469491958618\n",
            "epoch 1: train_loss:2.3024246284553596 val_loss2.302248001098633\n",
            "epoch 2: train_loss:2.3021751459654385 val_loss2.302031993865967\n",
            "epoch 3: train_loss:2.3019173811147877 val_loss2.301785945892334\n",
            "epoch 4: train_loss:2.301610308724481 val_loss2.3014652729034424\n",
            "epoch 5: train_loss:2.301203349689106 val_loss2.301008939743042\n",
            "epoch 6: train_loss:2.300615375106399 val_loss2.300316095352173\n",
            "epoch 7: train_loss:2.2997018195487358 val_loss2.299186944961548\n",
            "epoch 8: train_loss:2.2981674456381582 val_loss2.297192335128784\n",
            "epoch 9: train_loss:2.2953388669469335 val_loss2.2932870388031006\n",
            "epoch 10: train_loss:2.2894195887419553 val_loss2.284393787384033\n",
            "epoch 11: train_loss:2.274462049071853 val_loss2.259186267852783\n",
            "epoch 12: train_loss:2.226381997804384 val_loss2.170745611190796\n",
            "epoch 13: train_loss:2.0760833057197363 val_loss1.9537230730056763\n",
            "epoch 14: train_loss:1.7811890572041005 val_loss1.612715482711792\n",
            "epoch 15: train_loss:1.4176683984361254 val_loss1.3326449394226074\n",
            "epoch 16: train_loss:1.1914556155333649 val_loss1.1956312656402588\n",
            "epoch 17: train_loss:1.0561277941540554 val_loss1.095666766166687\n",
            "epoch 18: train_loss:0.9316650682741457 val_loss0.9975029230117798\n",
            "epoch 19: train_loss:0.8332257901763057 val_loss0.9369544982910156\n",
            "epoch 20: train_loss:0.7744751728332795 val_loss0.9000354409217834\n",
            "epoch 21: train_loss:0.731254364187653 val_loss0.8691776394844055\n",
            "epoch 22: train_loss:0.6918082776907328 val_loss0.8377237319946289\n",
            "epoch 23: train_loss:0.6525270568357932 val_loss0.8035087585449219\n",
            "epoch 24: train_loss:0.6129779796879571 val_loss0.7677139639854431\n",
            "epoch 25: train_loss:0.5744651964118889 val_loss0.7327864170074463\n",
            "epoch 26: train_loss:0.5388986414617246 val_loss0.7014002203941345\n",
            "epoch 27: train_loss:0.5066791135180104 val_loss0.6746854186058044\n",
            "epoch 28: train_loss:0.477491301056501 val_loss0.653570830821991\n",
            "epoch 29: train_loss:0.45098372864293623 val_loss0.6363073587417603\n",
            "Test Accuracy (mu=0.95): 81.3%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with mu=0.99\n",
            "epoch 0: train_loss:2.302907045897063 val_loss2.3025550842285156\n",
            "epoch 1: train_loss:2.3018904591465854 val_loss2.301683187484741\n",
            "epoch 2: train_loss:2.300365291200243 val_loss2.2991039752960205\n",
            "epoch 3: train_loss:2.2942876923191653 val_loss2.2855381965637207\n",
            "epoch 4: train_loss:2.2376161566725723 val_loss2.1114134788513184\n",
            "epoch 5: train_loss:1.7090922755164069 val_loss1.3281313180923462\n",
            "epoch 6: train_loss:1.0657375100496653 val_loss1.0251846313476562\n",
            "epoch 7: train_loss:0.7654432350987787 val_loss0.8161756992340088\n",
            "epoch 8: train_loss:0.5695443596388843 val_loss0.6682978272438049\n",
            "epoch 9: train_loss:0.4389359471765724 val_loss0.5811755657196045\n",
            "epoch 10: train_loss:0.352166335056494 val_loss0.5320858359336853\n",
            "epoch 11: train_loss:0.28633439248880826 val_loss0.49862104654312134\n",
            "epoch 12: train_loss:0.22527770587318652 val_loss0.488855242729187\n",
            "epoch 13: train_loss:0.1769489114181147 val_loss0.4809756875038147\n",
            "epoch 14: train_loss:0.14837018608509958 val_loss0.532305121421814\n",
            "epoch 15: train_loss:0.12907105673373015 val_loss0.5373703837394714\n",
            "epoch 16: train_loss:0.08771970246389911 val_loss0.4504185914993286\n",
            "epoch 17: train_loss:0.06494099852368906 val_loss0.47819992899894714\n",
            "epoch 18: train_loss:0.06730537908151746 val_loss0.5643793940544128\n",
            "epoch 19: train_loss:0.05346069015512193 val_loss0.594215989112854\n",
            "epoch 20: train_loss:0.062200464056553065 val_loss0.5804070830345154\n",
            "epoch 21: train_loss:0.04920644845041606 val_loss0.613701343536377\n",
            "epoch 22: train_loss:0.037913968658645276 val_loss0.618679940700531\n",
            "epoch 23: train_loss:0.040173262508737075 val_loss0.5367556810379028\n",
            "epoch 24: train_loss:0.04127259377960686 val_loss0.5540316700935364\n",
            "epoch 25: train_loss:0.04549258924089372 val_loss0.6036466956138611\n",
            "epoch 26: train_loss:0.05777050775836583 val_loss0.7169942855834961\n",
            "epoch 27: train_loss:0.05010753198577141 val_loss0.7061678767204285\n",
            "epoch 28: train_loss:0.04169822506908629 val_loss0.6339550018310547\n",
            "epoch 29: train_loss:0.0345809190756398 val_loss0.5420540571212769\n",
            "Test Accuracy (mu=0.99): 88.6%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding momentum, we observed some improvements in the model performance.\n",
        "\n",
        "For mu = 0.9: The learning curve remains relatively flat for a few epochs before gradually decreasing. There isn't a significant gap between the training and validation loss, but this doesn't translate to high accuracy with the test data.\n",
        "\n",
        "For mu = 0.95: Increasing the momentum further enhances the influence of previous updates. Here, we observe a similar trend with a relatively flat learning curve for the initial epochs followed by a decrease. The accuracy is better compared to the momentum with mu = 0.9.\n",
        "\n",
        "For mu = 0.99: A very high value of mu, such as 0.99, makes the optimization process even smoother. With mu = 0.99, we notice more fluctuations in the learning curve across epochs compared to other values of mu, but this is accompanied by higher accuracy.\n",
        "\n",
        "I experimented with different combinations of learning rate and momentum. Interestingly, I found that both higher learning rates with lower momentum and lower learning rates with higher momentum tend to yield better accuracy in this model. In this case, I opted for a lower learning rate with higher momentum, which resulted in better accuracy. Despite some variations in the validation loss compared to other momentums, the model performs well overall."
      ],
      "metadata": {
        "id": "6d7Eje3jJn80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reduce Learning Rate on Plateau(5pt) :**\n",
        "\n",
        "Reduce the learning rate by half if no improvement is seen in validation loss for 10 epochs. You should monitor and keep track of the best validation loss and the parameters for the model with that loss. If no improvement is seen in validation loss for 10 epochs, then reduce the learning rate by 0.5 and continue the training with that learning rate. Stop training if the learning rate is reduced to a\n",
        "minimum value ( e.g., 1e-4) and return the parameters with the lowest validation loss."
      ],
      "metadata": {
        "id": "qKCMiKQMKy1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Initialization\n",
        "def initialize_parameters(nx, nh1, nh2, ny):\n",
        "    tf.random.set_seed(21)\n",
        "    W1 = tf.Variable(tf.random.normal(shape=(nh1, nx), stddev=0.01), name=\"W1\")\n",
        "    b1 = tf.Variable(tf.zeros(shape=(nh1, 1), name=\"b1\"))\n",
        "    W2 = tf.Variable(tf.random.normal(shape=(nh2, nh1), stddev=0.01), name=\"W2\")\n",
        "    b2 = tf.Variable(tf.zeros(shape=(nh2, 1), name=\"b2\"))\n",
        "    W3 = tf.Variable(tf.random.normal(shape=(ny, nh2), stddev=0.01), name=\"W3\")\n",
        "    b3 = tf.Variable(tf.zeros(shape=(ny, 1), name=\"b3\"))\n",
        "\n",
        "    # Initialize velocity for each parameter\n",
        "    v_W1 = tf.Variable(tf.zeros_like(W1), trainable=False)\n",
        "    v_b1 = tf.Variable(tf.zeros_like(b1), trainable=False)\n",
        "    v_W2 = tf.Variable(tf.zeros_like(W2), trainable=False)\n",
        "    v_b2 = tf.Variable(tf.zeros_like(b2), trainable=False)\n",
        "    v_W3 = tf.Variable(tf.zeros_like(W3), trainable=False)\n",
        "    v_b3 = tf.Variable(tf.zeros_like(b3), trainable=False)\n",
        "\n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
        "    velocities = {\"v_W1\": v_W1, \"v_b1\": v_b1, \"v_W2\": v_W2, \"v_b2\": v_b2, \"v_W3\": v_W3, \"v_b3\": v_b3}\n",
        "\n",
        "    return parameters, velocities\n",
        "\n",
        "# Forward Pass\n",
        "def forward_pass(parameters, X):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "\n",
        "    Z1 = tf.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.matmul(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    Z3 = tf.matmul(parameters[\"W3\"], A2) + parameters[\"b3\"]\n",
        "\n",
        "    # Apply softmax activation for multi-class classification\n",
        "    Yhat = tf.nn.softmax(Z3, axis=0)\n",
        "\n",
        "    return Yhat\n",
        "\n",
        "# Loss calculations\n",
        "def compute_loss(Y, Yhat):\n",
        "    loss = -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(Yhat + 1e-10), axis=0))\n",
        "    return loss\n",
        "\n",
        "# Backward Pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters.values())\n",
        "    return gradients\n",
        "\n",
        "# Modify update_parameters to use Nesterov Momentum\n",
        "def update_parameters(parameters, velocities, gradients, learning_rate, mu):\n",
        "    for param, grad, vel in zip(parameters.values(), gradients, velocities.values()):\n",
        "        # Update velocity\n",
        "        vel.assign(mu * vel - learning_rate * grad)\n",
        "        # Update parameter using velocity\n",
        "        param.assign_add(mu * vel - learning_rate * grad)\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Create a new create_nn_model function to utilize the modified update_parameters\n",
        "def create_nn_model_ReduceLROnPlateau(train_X, train_Y, val_X, val_Y, num_iterations, learning_rate, nh1, nh2, batch_size, mu):\n",
        "    nx, m = train_X.shape\n",
        "    ny = train_Y.shape[0]\n",
        "\n",
        "    parameters, velocities = initialize_parameters(nx, nh1, nh2, ny)\n",
        "\n",
        "    best_val_loss = True # Generated AI help code\n",
        "    best_parameters = None # Generated AI help code\n",
        "    no_improvement_count = 0\n",
        "    min_learning_rate = 0.0001\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    num_batches = (m + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        epoch_train_loss = 0\n",
        "        train_loss = []\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = min(start + batch_size, m)\n",
        "            X_batch = train_X[:, start:end]\n",
        "            Y_batch = train_Y[:, start:end]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                train_Yhat = forward_pass(parameters, tf.convert_to_tensor(X_batch, dtype=tf.float32))\n",
        "                train_loss = compute_loss(tf.convert_to_tensor(Y_batch, dtype=tf.float32), train_Yhat)\n",
        "\n",
        "            gradients = backward_pass(parameters, train_loss, tape)\n",
        "            parameters = update_parameters(parameters, velocities, gradients, learning_rate, mu)\n",
        "\n",
        "            epoch_train_loss = epoch_train_loss + train_loss.numpy()  # Accumulate loss for the current batch\n",
        "\n",
        "\n",
        "        epoch_train_loss = epoch_train_loss/num_batches\n",
        "        train_losses.append(epoch_train_loss)  # Append epoch loss to train_losses\n",
        "\n",
        "\n",
        "        # Calculate validation loss after each epoch\n",
        "        val_Yhat = forward_pass(parameters, tf.convert_to_tensor(val_X, dtype=tf.float32))\n",
        "        val_loss = compute_loss(tf.convert_to_tensor(val_Y, dtype=tf.float32), val_Yhat)\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        print(\"epoch {}: train_loss:{} val_loss{}\".format(i, epoch_train_loss, val_loss.numpy()))\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss: # Generated AI help code\n",
        "            best_val_loss = val_loss # Generates AI help code\n",
        "            best_parameters = parameters # Generates AI help code\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            #print(\"no_improvement_count\", no_improvement_count)\n",
        "\n",
        "        # If no improvement for 10 epochs, reduce learning rate by half\n",
        "        if no_improvement_count >= 10:\n",
        "            if learning_rate > min_learning_rate:\n",
        "                learning_rate = learning_rate * 0.5\n",
        "                no_improvement_count = 0\n",
        "                print(f\"Reducing learning rate to {learning_rate}\")\n",
        "\n",
        "    print(\"Best val_loss :\" , best_val_loss)\n",
        "\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return best_parameters, history\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"German_digits.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract features (X) and labels (Y)\n",
        "X = df.iloc[:, :-1].values / 255.0\n",
        "Y = df.iloc[:, -1].values\n",
        "\n",
        "# Split the dataset into train and test\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=12)\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 10\n",
        "train_Y = pd.get_dummies(train_Y).values\n",
        "test_Y = pd.get_dummies(test_Y).values\n",
        "\n",
        "# Transpose the datasets\n",
        "train_X = train_X.T\n",
        "train_Y = train_Y.T\n",
        "test_X = test_X.T\n",
        "test_Y = test_Y.T\n",
        "\n",
        "# print(\"train_X\", train_X.shape)\n",
        "# print(\"train_Y\", train_Y.shape)\n",
        "# print(\"test_X\", test_X.shape)\n",
        "# print(\"test_Y\", test_Y.shape)\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_iterations = 100\n",
        "nh1 = 128\n",
        "nh2 = 64\n",
        "batch_size = 32\n",
        "mu = 0.99\n",
        "\n",
        "# Train the model\n",
        "best_parameters, history = create_nn_model_ReduceLROnPlateau(train_X, train_Y, test_X, test_Y, num_iterations, learning_rate, nh1, nh2, batch_size, mu)\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.plot(history['train_loss'], label='Training Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\")\n",
        "#print(best_parameters.items())\n",
        "for item, value in best_parameters.items():\n",
        "    print(f\"{item}:\")\n",
        "    print(value.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XUmI1pgfoqK9",
        "outputId": "9f947b08-f8bb-4163-b41b-fccf787d958d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: train_loss:2.302825390755593 val_loss2.3024215698242188\n",
            "epoch 1: train_loss:2.301624298095703 val_loss2.301241636276245\n",
            "epoch 2: train_loss:2.299386617299673 val_loss2.297261953353882\n",
            "epoch 3: train_loss:2.288591410662677 val_loss2.2708468437194824\n",
            "epoch 4: train_loss:2.175155429152755 val_loss1.9611135721206665\n",
            "epoch 5: train_loss:1.495107308701352 val_loss1.1538984775543213\n",
            "epoch 6: train_loss:0.9085830168144123 val_loss0.9860073328018188\n",
            "epoch 7: train_loss:0.7221157387033239 val_loss0.7807842493057251\n",
            "epoch 8: train_loss:0.5211424273145091 val_loss0.627478301525116\n",
            "epoch 9: train_loss:0.39641281076379725 val_loss0.5648301839828491\n",
            "epoch 10: train_loss:0.32093540253537195 val_loss0.5099711418151855\n",
            "epoch 11: train_loss:0.2551417872309685 val_loss0.48005813360214233\n",
            "epoch 12: train_loss:0.20547030654710693 val_loss0.4620193839073181\n",
            "epoch 13: train_loss:0.16823062916529608 val_loss0.5184423923492432\n",
            "epoch 14: train_loss:0.13630531761828843 val_loss0.5679428577423096\n",
            "epoch 15: train_loss:0.10484005012423606 val_loss0.5340061783790588\n",
            "epoch 16: train_loss:0.08412831887049046 val_loss0.5436007976531982\n",
            "epoch 17: train_loss:0.07376472597841073 val_loss0.5996929407119751\n",
            "epoch 18: train_loss:0.07372564523740932 val_loss0.5643675923347473\n",
            "epoch 19: train_loss:0.08468259040366959 val_loss0.5487853288650513\n",
            "epoch 20: train_loss:0.10817155050248042 val_loss0.6492505669593811\n",
            "epoch 21: train_loss:0.12311799534562942 val_loss0.7209601402282715\n",
            "epoch 22: train_loss:0.10135173954505909 val_loss0.5597946047782898\n",
            "Reducing learning rate to 0.0005\n",
            "epoch 23: train_loss:0.07821759611561156 val_loss0.5943243503570557\n",
            "epoch 24: train_loss:0.0527650289917113 val_loss0.5302688479423523\n",
            "epoch 25: train_loss:0.029560628284171626 val_loss0.5459544658660889\n",
            "epoch 26: train_loss:0.013748310468587521 val_loss0.5226753950119019\n",
            "epoch 27: train_loss:0.006654412823368434 val_loss0.5161256790161133\n",
            "epoch 28: train_loss:0.005043229078748138 val_loss0.5188610553741455\n",
            "epoch 29: train_loss:0.004019645099876331 val_loss0.524515688419342\n",
            "epoch 30: train_loss:0.003462527659386061 val_loss0.5279488563537598\n",
            "epoch 31: train_loss:0.003095045975233252 val_loss0.5327751040458679\n",
            "epoch 32: train_loss:0.0028157188296989277 val_loss0.5372889041900635\n",
            "Reducing learning rate to 0.00025\n",
            "epoch 33: train_loss:0.002595995567520862 val_loss0.5405365228652954\n",
            "epoch 34: train_loss:0.0024772919095666693 val_loss0.54263836145401\n",
            "epoch 35: train_loss:0.0023867326866214475 val_loss0.5442963242530823\n",
            "epoch 36: train_loss:0.0023094278700997936 val_loss0.545804500579834\n",
            "epoch 37: train_loss:0.0022401739373857673 val_loss0.5472591519355774\n",
            "epoch 38: train_loss:0.0021763775054038175 val_loss0.5486664175987244\n",
            "epoch 39: train_loss:0.0021170403788428383 val_loss0.5500244498252869\n",
            "epoch 40: train_loss:0.002061077632714768 val_loss0.5513836741447449\n",
            "epoch 41: train_loss:0.0020079920844485364 val_loss0.5527282357215881\n",
            "epoch 42: train_loss:0.0019576068315235477 val_loss0.5540456175804138\n",
            "Reducing learning rate to 0.000125\n",
            "epoch 43: train_loss:0.0019077152673989903 val_loss0.5551460981369019\n",
            "epoch 44: train_loss:0.0018780195961291021 val_loss0.5559270977973938\n",
            "epoch 45: train_loss:0.0018537387357781398 val_loss0.5565950274467468\n",
            "epoch 46: train_loss:0.0018315346644633358 val_loss0.5572174787521362\n",
            "epoch 47: train_loss:0.0018104388313535768 val_loss0.5578163266181946\n",
            "epoch 48: train_loss:0.0017900769981589201 val_loss0.5584134459495544\n",
            "epoch 49: train_loss:0.001770278285278494 val_loss0.5590054392814636\n",
            "epoch 50: train_loss:0.0017510113237648027 val_loss0.5595929026603699\n",
            "epoch 51: train_loss:0.001732192917303224 val_loss0.5601734519004822\n",
            "epoch 52: train_loss:0.0017138013046188699 val_loss0.5607426762580872\n",
            "Reducing learning rate to 6.25e-05\n",
            "epoch 53: train_loss:0.001694877314442131 val_loss0.5612201690673828\n",
            "epoch 54: train_loss:0.0016834434931510473 val_loss0.5615559220314026\n",
            "epoch 55: train_loss:0.0016738777486754262 val_loss0.5618452429771423\n",
            "epoch 56: train_loss:0.001665021677769989 val_loss0.5621216893196106\n",
            "epoch 57: train_loss:0.0016564387185205411 val_loss0.5623955130577087\n",
            "epoch 58: train_loss:0.0016480239954051303 val_loss0.5626683831214905\n",
            "epoch 59: train_loss:0.0016396961060587553 val_loss0.562939465045929\n",
            "epoch 60: train_loss:0.0016314915847033262 val_loss0.563214123249054\n",
            "epoch 61: train_loss:0.0016233593640798652 val_loss0.5634890794754028\n",
            "epoch 62: train_loss:0.0016153131586474341 val_loss0.5637628436088562\n",
            "epoch 63: train_loss:0.00160735638587744 val_loss0.5640297532081604\n",
            "epoch 64: train_loss:0.0015994845123894447 val_loss0.5642977356910706\n",
            "epoch 65: train_loss:0.0015916655462566629 val_loss0.5645711421966553\n",
            "epoch 66: train_loss:0.0015839412897565143 val_loss0.5648437738418579\n",
            "epoch 67: train_loss:0.0015763280156009772 val_loss0.5651076436042786\n",
            "epoch 68: train_loss:0.001568724794852388 val_loss0.5653738379478455\n",
            "epoch 69: train_loss:0.0015612461002174397 val_loss0.5656358003616333\n",
            "epoch 70: train_loss:0.001553825972190647 val_loss0.5658965110778809\n",
            "epoch 71: train_loss:0.0015464894002123089 val_loss0.5661577582359314\n",
            "epoch 72: train_loss:0.0015392236040301017 val_loss0.5664187669754028\n",
            "epoch 73: train_loss:0.001532048506133661 val_loss0.5666755437850952\n",
            "epoch 74: train_loss:0.0015249181943127533 val_loss0.5669302940368652\n",
            "epoch 75: train_loss:0.0015178907341176964 val_loss0.5671796202659607\n",
            "epoch 76: train_loss:0.0015109044469280488 val_loss0.5674304962158203\n",
            "epoch 77: train_loss:0.0015039914468542447 val_loss0.567685604095459\n",
            "epoch 78: train_loss:0.001497152661810302 val_loss0.5679396390914917\n",
            "epoch 79: train_loss:0.0014903810172056495 val_loss0.5681856870651245\n",
            "epoch 80: train_loss:0.0014836433842287374 val_loss0.5684323906898499\n",
            "epoch 81: train_loss:0.00147696891023467 val_loss0.5686743855476379\n",
            "epoch 82: train_loss:0.0014703748540571043 val_loss0.5689143538475037\n",
            "epoch 83: train_loss:0.0014637969526030156 val_loss0.5691573023796082\n",
            "epoch 84: train_loss:0.0014573156488597863 val_loss0.5693995356559753\n",
            "epoch 85: train_loss:0.001450893329666206 val_loss0.5696383714675903\n",
            "epoch 86: train_loss:0.0014445131730082704 val_loss0.5698761343955994\n",
            "epoch 87: train_loss:0.0014382077038585972 val_loss0.5701127648353577\n",
            "epoch 88: train_loss:0.0014319709763832827 val_loss0.5703496932983398\n",
            "epoch 89: train_loss:0.0014257652242982608 val_loss0.5705888271331787\n",
            "epoch 90: train_loss:0.0014196227211295417 val_loss0.5708200931549072\n",
            "epoch 91: train_loss:0.0014135308570526373 val_loss0.5710577368736267\n",
            "epoch 92: train_loss:0.001407498973610534 val_loss0.5712923407554626\n",
            "epoch 93: train_loss:0.0014015381176733655 val_loss0.5715180039405823\n",
            "epoch 94: train_loss:0.0013956103803220768 val_loss0.5717485547065735\n",
            "epoch 95: train_loss:0.0013897397525520442 val_loss0.5719783306121826\n",
            "epoch 96: train_loss:0.0013839246428294762 val_loss0.5722028613090515\n",
            "epoch 97: train_loss:0.0013781597983062704 val_loss0.5724309682846069\n",
            "epoch 98: train_loss:0.0013724418266819 val_loss0.5726606845855713\n",
            "epoch 99: train_loss:0.00136680288611584 val_loss0.5728796124458313\n",
            "Best val_loss : tf.Tensor(0.46201938, shape=(), dtype=float32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:\n",
            "W1:\n",
            "[[-0.02085437  0.01866391  0.01303958 ...  0.01135506 -0.00068393\n",
            "  -0.02710604]\n",
            " [-0.00638591  0.01777487 -0.00625102 ... -0.0135013  -0.02190342\n",
            "  -0.01678702]\n",
            " [-0.00358494  0.00258934  0.00966893 ...  0.00429086 -0.01014254\n",
            "   0.00594161]\n",
            " ...\n",
            " [ 0.00207208 -0.00173876  0.01744212 ... -0.00305538 -0.0014465\n",
            "  -0.00210311]\n",
            " [-0.00820849  0.00232205  0.00764629 ... -0.01300319 -0.00310775\n",
            "   0.00225077]\n",
            " [ 0.0249713   0.01420243 -0.01537449 ...  0.00368684 -0.00175021\n",
            "   0.01113094]]\n",
            "b1:\n",
            "[[ 1.0706624e-01]\n",
            " [ 1.8618616e-01]\n",
            " [ 1.5982701e-02]\n",
            " [-6.2890276e-02]\n",
            " [ 5.8442038e-02]\n",
            " [ 3.2657463e-02]\n",
            " [-1.3553600e-02]\n",
            " [-1.0060068e-03]\n",
            " [ 9.6418418e-02]\n",
            " [ 2.6995407e-02]\n",
            " [ 2.5515171e-02]\n",
            " [-2.6752637e-03]\n",
            " [ 3.1163154e-02]\n",
            " [-1.0515926e-02]\n",
            " [ 1.0291452e-02]\n",
            " [ 2.7705472e-02]\n",
            " [ 9.7377533e-03]\n",
            " [ 1.3197353e-01]\n",
            " [ 7.4122600e-02]\n",
            " [-7.2899275e-02]\n",
            " [ 3.0579541e-02]\n",
            " [ 1.9527411e-02]\n",
            " [ 9.9782869e-02]\n",
            " [ 2.2011360e-02]\n",
            " [ 2.4308138e-02]\n",
            " [-6.4992500e-03]\n",
            " [-1.6663142e-02]\n",
            " [ 5.9526537e-02]\n",
            " [-4.9794793e-02]\n",
            " [ 1.7171921e-02]\n",
            " [ 2.9812345e-02]\n",
            " [ 3.3069991e-02]\n",
            " [ 8.7506190e-02]\n",
            " [ 4.5021907e-03]\n",
            " [ 1.3232780e-01]\n",
            " [ 2.6167454e-02]\n",
            " [-2.0729665e-02]\n",
            " [ 6.6195935e-02]\n",
            " [-1.0570063e-02]\n",
            " [ 1.1266119e-01]\n",
            " [ 3.6368135e-02]\n",
            " [-4.9090579e-02]\n",
            " [ 4.2466108e-02]\n",
            " [-3.7389848e-02]\n",
            " [ 1.0362823e-01]\n",
            " [ 8.9689279e-03]\n",
            " [ 1.0383247e-03]\n",
            " [ 4.7393337e-02]\n",
            " [ 2.6895199e-02]\n",
            " [ 3.8301561e-02]\n",
            " [-7.3888667e-02]\n",
            " [ 7.9534268e-03]\n",
            " [ 3.6637105e-02]\n",
            " [ 4.8924144e-02]\n",
            " [ 3.1877100e-02]\n",
            " [ 1.3909703e-01]\n",
            " [-1.3275287e-02]\n",
            " [-1.2577451e-02]\n",
            " [ 4.0572606e-02]\n",
            " [-4.4647228e-02]\n",
            " [ 3.9859217e-02]\n",
            " [ 1.1439394e-01]\n",
            " [ 7.5772507e-03]\n",
            " [-5.0415877e-02]\n",
            " [-4.0454857e-02]\n",
            " [-3.0554466e-02]\n",
            " [ 1.9777562e-02]\n",
            " [ 4.8867662e-02]\n",
            " [-2.8612798e-02]\n",
            " [ 1.1923654e-02]\n",
            " [-2.5193466e-02]\n",
            " [ 1.7068677e-02]\n",
            " [ 1.1972951e-02]\n",
            " [-1.2577324e-05]\n",
            " [ 3.6715478e-02]\n",
            " [ 1.9663654e-02]\n",
            " [ 4.8034519e-02]\n",
            " [-4.4435235e-03]\n",
            " [ 2.1824578e-02]\n",
            " [ 4.4034936e-02]\n",
            " [ 4.6976369e-02]\n",
            " [ 1.2396409e-01]\n",
            " [ 5.2350968e-02]\n",
            " [ 6.4280473e-02]\n",
            " [-1.5475867e-02]\n",
            " [ 2.4593696e-02]\n",
            " [-3.0749463e-02]\n",
            " [-2.9083584e-03]\n",
            " [ 1.3139714e-02]\n",
            " [ 3.4294873e-02]\n",
            " [ 1.5605202e-02]\n",
            " [ 1.4643194e-01]\n",
            " [-1.7338006e-02]\n",
            " [-4.7153071e-02]\n",
            " [ 6.2558867e-02]\n",
            " [ 1.4726569e-01]\n",
            " [ 1.2377358e-02]\n",
            " [-8.8753849e-03]\n",
            " [ 7.0531704e-03]\n",
            " [-8.7239118e-03]\n",
            " [ 8.2619358e-03]\n",
            " [ 4.1912571e-03]\n",
            " [ 9.0920972e-03]\n",
            " [ 3.4579039e-02]\n",
            " [ 8.6783906e-03]\n",
            " [ 1.3342520e-02]\n",
            " [ 3.6228247e-02]\n",
            " [ 1.5355998e-02]\n",
            " [ 2.0830207e-02]\n",
            " [ 3.5710495e-03]\n",
            " [ 4.7991943e-02]\n",
            " [ 4.6609633e-02]\n",
            " [ 1.7011264e-02]\n",
            " [-1.9300232e-03]\n",
            " [-5.5871967e-02]\n",
            " [ 9.9989451e-02]\n",
            " [ 4.8339624e-02]\n",
            " [ 4.8292939e-02]\n",
            " [ 4.0271290e-02]\n",
            " [ 3.9612837e-02]\n",
            " [-5.1804978e-02]\n",
            " [-1.4898396e-02]\n",
            " [ 1.6488323e-02]\n",
            " [ 4.1455731e-02]\n",
            " [ 1.2951680e-03]\n",
            " [ 9.7743779e-02]\n",
            " [-9.2231482e-04]\n",
            " [ 5.8983068e-04]]\n",
            "W2:\n",
            "[[ 0.02863262  0.1554643  -0.09919124 ...  0.1336967   0.07825319\n",
            "   0.07850438]\n",
            " [-0.20861399  0.19044855 -0.20162995 ...  0.07876439  0.01402856\n",
            "   0.05192468]\n",
            " [ 0.02507996 -0.05528829 -0.07816624 ...  0.01474786  0.02095673\n",
            "  -0.03218842]\n",
            " ...\n",
            " [-0.03002051 -0.02190115  0.1697507  ... -0.05449949  0.03738058\n",
            "   0.14922328]\n",
            " [ 0.04714737  0.01370055 -0.02596337 ... -0.0168692   0.01059734\n",
            "  -0.0082904 ]\n",
            " [ 0.00688484  0.00266474  0.00087567 ... -0.01086882  0.00089024\n",
            "   0.00266648]]\n",
            "b2:\n",
            "[[ 0.1449514 ]\n",
            " [ 0.09607564]\n",
            " [-0.04601704]\n",
            " [ 0.31074962]\n",
            " [ 0.08978909]\n",
            " [-0.04258018]\n",
            " [ 0.03463938]\n",
            " [ 0.00809974]\n",
            " [-0.00566098]\n",
            " [-0.00745634]\n",
            " [-0.09729723]\n",
            " [-0.02675166]\n",
            " [-0.08141676]\n",
            " [ 0.02891315]\n",
            " [-0.02360477]\n",
            " [ 0.11822716]\n",
            " [ 0.07455818]\n",
            " [ 0.01770189]\n",
            " [-0.02894592]\n",
            " [ 0.0488764 ]\n",
            " [ 0.09177342]\n",
            " [ 0.07155651]\n",
            " [-0.00667025]\n",
            " [-0.0456517 ]\n",
            " [ 0.20346062]\n",
            " [ 0.03545021]\n",
            " [ 0.00847711]\n",
            " [-0.0166621 ]\n",
            " [-0.02900132]\n",
            " [ 0.24131392]\n",
            " [ 0.0827618 ]\n",
            " [-0.06746531]\n",
            " [ 0.18387385]\n",
            " [ 0.00098031]\n",
            " [-0.00153128]\n",
            " [ 0.01746692]\n",
            " [ 0.15157732]\n",
            " [ 0.22902575]\n",
            " [ 0.06592174]\n",
            " [ 0.04486626]\n",
            " [-0.06235862]\n",
            " [ 0.1404331 ]\n",
            " [ 0.09250364]\n",
            " [-0.12310087]\n",
            " [ 0.02801884]\n",
            " [ 0.00617384]\n",
            " [ 0.04893959]\n",
            " [ 0.03084974]\n",
            " [-0.03173613]\n",
            " [ 0.18008143]\n",
            " [ 0.02184616]\n",
            " [ 0.05534903]\n",
            " [ 0.0190462 ]\n",
            " [-0.00882241]\n",
            " [ 0.00349057]\n",
            " [ 0.08184557]\n",
            " [ 0.04648496]\n",
            " [ 0.01757082]\n",
            " [ 0.07605085]\n",
            " [ 0.07407786]\n",
            " [ 0.00956602]\n",
            " [ 0.12375404]\n",
            " [ 0.009457  ]\n",
            " [ 0.00404139]]\n",
            "W3:\n",
            "[[-5.95953584e-01  5.16258031e-02 -2.97682430e-03 -2.24274382e-01\n",
            "  -3.63173217e-01 -6.06532805e-02  2.61147052e-01  2.93181568e-01\n",
            "  -6.75937580e-03  3.31801206e-01  2.43443936e-01 -6.57778904e-02\n",
            "  -1.55373514e-02 -9.25401971e-03 -3.68696183e-01 -2.53238320e-01\n",
            "   4.06345688e-02 -6.74197152e-02  2.37403721e-01  7.89270103e-01\n",
            "   7.78599560e-01 -9.63120721e-03  2.23821461e-01  1.83993042e-01\n",
            "  -9.94091094e-01  1.29942894e-01 -6.26107864e-03 -5.85096657e-01\n",
            "  -4.12102968e-01 -5.15599489e-01  1.03016943e-02 -1.22125357e-01\n",
            "   2.61314064e-01  3.41317873e-03  3.89806122e-01 -1.27123728e-01\n",
            "  -8.44663978e-02  5.11645675e-02 -2.47601718e-01 -1.29249200e-01\n",
            "   6.43514618e-02 -6.02476485e-02  2.99104273e-01 -8.91747400e-02\n",
            "  -2.49404442e-02 -2.26186424e-01  5.94654270e-02  4.42397803e-01\n",
            "  -1.55808359e-01 -1.14963457e-01  8.07166845e-03  3.63570899e-01\n",
            "  -1.02816457e-02  1.32612903e-02  4.51822691e-02 -1.22581512e-01\n",
            "  -6.69754148e-02 -1.58455133e-01 -8.86461958e-02 -3.35179955e-01\n",
            "   5.02114415e-01 -1.24581322e-01 -3.24153937e-02 -1.41835958e-02]\n",
            " [ 1.95864931e-01  2.46464834e-01 -5.57884797e-02  9.49568391e-01\n",
            "   1.41880721e-01 -5.59869826e-01  7.07222056e-03 -2.06289783e-01\n",
            "   5.07377740e-03  5.08926697e-02 -5.29870749e-01 -1.72356397e-01\n",
            "  -2.02415392e-01 -4.18217070e-02 -3.20195377e-01  1.20314658e-01\n",
            "  -1.31753594e-01 -1.11790158e-01 -2.56879628e-01  1.29548460e-01\n",
            "  -2.17433795e-02 -2.63617545e-01 -1.80356950e-01 -4.38760728e-01\n",
            "   3.25795650e-01  4.26837206e-01  1.54628502e-02 -3.65913622e-02\n",
            "  -2.19185710e-01 -2.95931064e-02  1.90198556e-01 -5.39743528e-02\n",
            "  -4.00482744e-01 -7.45290599e-04 -5.36608636e-01  3.51019830e-01\n",
            "   5.63692749e-01  2.96625614e-01 -3.76465023e-01 -3.08602124e-01\n",
            "  -3.76539320e-01  3.42954993e-01  4.43438262e-01 -2.92909771e-01\n",
            "   1.86327833e-03 -3.51317883e-01  6.67061657e-02  3.18720311e-01\n",
            "  -1.74933717e-01  3.21554065e-01  8.08371753e-02  1.70965165e-01\n",
            "   1.12489536e-01 -1.55155605e-03 -4.30335552e-01  3.41893733e-01\n",
            "   5.16144812e-01 -1.04328245e-01  1.24299087e-01  2.60479659e-01\n",
            "  -1.11669928e-01  2.31842980e-01 -1.15616284e-01 -7.20690703e-03]\n",
            " [-8.31863940e-01 -4.09934551e-01 -2.55287498e-01 -1.95878938e-01\n",
            "  -3.55681300e-01  1.49278045e-01  4.73321415e-02  2.96544939e-01\n",
            "  -6.62909122e-03 -1.61410421e-01  4.28425401e-01  4.59691435e-01\n",
            "  -1.19031919e-02 -5.36426017e-03  3.77588362e-01 -3.36651266e-01\n",
            "  -3.94849151e-01  3.80200148e-02  2.98672915e-01  1.24551013e-01\n",
            "   3.94606620e-01 -2.29677692e-01 -2.42689818e-01 -3.24238479e-01\n",
            "   9.71821174e-02 -1.83721796e-01  1.00526571e-01  7.81403542e-01\n",
            "   2.19660550e-01 -5.30864179e-01  2.73167491e-01 -3.02310079e-01\n",
            "   3.69129665e-02 -1.84239615e-02  4.78533655e-01  1.57486379e-01\n",
            "  -5.84414840e-01 -1.16036497e-01 -2.37258196e-01 -1.75773263e-01\n",
            "   2.52471983e-01  4.33841139e-01 -4.24257010e-01 -4.30582557e-03\n",
            "   2.14869827e-01 -3.02902788e-01 -2.64072597e-01 -2.57313251e-01\n",
            "   3.87918279e-02 -1.59200013e-01  2.76439893e-03 -3.78504931e-03\n",
            "   2.35178974e-03  1.08728558e-02  1.36106059e-01  5.15204668e-02\n",
            "   1.35431826e-01 -1.25011683e-01 -1.01445325e-01  5.68223834e-01\n",
            "   2.59133607e-01  2.99491972e-01 -7.23020062e-02  1.95701066e-02]\n",
            " [ 4.82655853e-01  4.22900766e-01 -4.90612090e-02 -6.07906461e-01\n",
            "   4.32342738e-01  1.01034716e-01 -1.30495951e-01  2.16905087e-01\n",
            "   4.05868050e-04 -2.09583461e-01 -8.05696249e-02 -4.93271928e-03\n",
            "  -1.64534703e-01 -1.46741401e-02  4.80561823e-01 -5.00295401e-01\n",
            "  -1.21870480e-01  5.46677373e-02  2.64631957e-01 -3.87416810e-01\n",
            "  -1.74629927e-01 -1.79677516e-01 -2.62226433e-01 -9.38817561e-02\n",
            "   5.10930002e-01  4.26001996e-02 -3.68911289e-02  4.56780434e-01\n",
            "   6.51061952e-01 -3.45631272e-01 -3.66962962e-02 -1.25970483e-01\n",
            "  -2.76721269e-01  2.50425097e-03 -3.64024878e-01 -3.50689515e-02\n",
            "   1.31401315e-01 -3.16464007e-01 -1.88772231e-01 -4.97391552e-01\n",
            "   5.66438660e-02  3.20649147e-01  2.02268764e-01 -1.22906037e-01\n",
            "   5.20153567e-02  2.16330409e-01 -4.49562252e-01 -5.01659572e-01\n",
            "   4.05841440e-01 -2.17331201e-01 -9.72390026e-02 -2.32579559e-01\n",
            "  -9.22830403e-03  1.31790526e-03  4.22952801e-01 -1.06673501e-01\n",
            "  -8.64747167e-02  3.58354510e-03 -1.81709811e-01  2.64257282e-01\n",
            "  -1.18595734e-01  1.71646446e-01 -8.67231283e-03 -2.74630496e-03]\n",
            " [ 1.20505452e-01 -1.60901025e-01 -6.55840663e-03  5.38130343e-01\n",
            "   4.06902134e-02 -1.40153602e-01  2.19523162e-02 -1.35338098e-01\n",
            "  -4.01250843e-04 -2.88872629e-01 -3.93045336e-01 -2.12320343e-01\n",
            "  -2.11581811e-01  1.00193381e-01 -3.83035243e-01  4.72654045e-01\n",
            "   4.07870859e-01  1.46207167e-02 -1.59034893e-01  8.11720565e-02\n",
            "  -2.20006965e-02  2.04896018e-01  1.17178895e-01 -1.99820787e-01\n",
            "  -1.09805621e-01 -3.33131164e-01  7.23644625e-03 -6.09779418e-01\n",
            "  -4.40553546e-01  1.02813840e+00 -1.14076899e-03 -1.20080911e-01\n",
            "   3.84294719e-01  1.93645377e-02 -2.14995787e-01 -4.00282055e-01\n",
            "  -1.38575703e-01  4.18259174e-01  3.90240371e-01  4.39757973e-01\n",
            "  -3.69600028e-01 -2.93392509e-01 -2.17710868e-01 -4.18696880e-01\n",
            "  -2.87273284e-02 -3.60310599e-02  1.31418645e-01 -5.56560494e-02\n",
            "  -1.25397637e-01  4.91689861e-01  1.51324142e-02  4.36147228e-02\n",
            "   5.08894539e-03  1.03371749e-02 -2.47009978e-01  7.22025940e-03\n",
            "  -2.84003884e-01  2.43828043e-01  2.23070443e-01 -3.46626908e-01\n",
            "  -2.42257819e-01 -7.43239671e-02  1.16778053e-01  2.89565381e-02]\n",
            " [ 1.73739672e-01  1.24462415e-02  1.09085932e-01 -6.65627897e-01\n",
            "  -1.74121231e-01  2.56876379e-01 -4.76763956e-02  9.04150978e-02\n",
            "   3.57086118e-03  1.54189810e-01  2.32799381e-01 -6.56365678e-02\n",
            "   3.32113832e-01  3.06128711e-02  2.70798236e-01  3.81714106e-02\n",
            "  -2.15111021e-02  8.60297084e-02 -2.45043244e-02 -2.87547410e-01\n",
            "  -1.97622448e-01  3.65003616e-01  2.22994402e-01  1.10478258e+00\n",
            "  -4.59704190e-01 -1.87117040e-01 -1.24561263e-03  2.56888181e-01\n",
            "   1.64204538e-01  2.86730677e-01 -3.55445355e-01  3.05998147e-01\n",
            "   2.31059283e-01 -2.14994256e-03  3.52826566e-01 -2.24257633e-01\n",
            "   1.15101896e-01 -2.92957544e-01  3.00213158e-01  7.05691278e-02\n",
            "   3.07790071e-01 -5.99247932e-01 -7.44647682e-02  5.36698043e-01\n",
            "  -3.22998539e-02  6.40067637e-01  1.06613986e-01 -1.42541870e-01\n",
            "   4.09560613e-02 -6.55971244e-02 -7.07969666e-02 -5.22668846e-02\n",
            "   1.52620673e-02  8.80965311e-03  5.78250408e-01 -2.50983655e-01\n",
            "  -4.06407952e-01  2.78231073e-02 -1.80253565e-01 -9.18392688e-02\n",
            "  -7.25777149e-02 -1.19178496e-01  2.01971997e-02 -1.10724047e-02]\n",
            " [-3.89967918e-01 -4.85768080e-01  1.14893973e-01 -2.26174239e-02\n",
            "  -2.81626791e-01  2.38286220e-02 -2.62294617e-02 -4.16565627e-01\n",
            "  -1.12588573e-02  2.04722077e-01  5.29550672e-01  4.88108784e-01\n",
            "   9.05570239e-02 -2.12161889e-04 -1.88006580e-01  1.21120714e-01\n",
            "  -2.85652101e-01  2.75974870e-01 -2.81607717e-01  1.71141461e-01\n",
            "  -5.81443422e-02 -1.90985009e-01 -1.77644640e-01 -2.27902353e-01\n",
            "  -8.49303722e-01  1.31383538e-01 -4.20974754e-03 -3.28590512e-01\n",
            "  -3.60666215e-01  6.47410095e-01  8.43254998e-02  1.96305677e-01\n",
            "   8.50607991e-01  7.56649440e-03  6.38191581e-01 -3.01493287e-01\n",
            "  -3.76309812e-01 -3.46805573e-01 -8.89910012e-03 -1.53093055e-01\n",
            "  -1.65469870e-01 -8.31705481e-02 -2.64177024e-01  4.73900624e-02\n",
            "   3.52801159e-02 -2.78248757e-01  5.20806730e-01 -9.67169777e-02\n",
            "  -2.29996055e-01  2.21838236e-01 -6.32079765e-02 -1.23508938e-01\n",
            "   1.47252372e-02 -2.00728580e-04 -7.74427950e-02  1.63045719e-01\n",
            "   1.48252741e-01  3.45027983e-01 -1.08294211e-01 -1.50531799e-01\n",
            "  -5.03352284e-01 -1.34724408e-01  1.18756834e-02  2.25160737e-02]\n",
            " [ 4.40652728e-01 -1.32336169e-01 -2.01340556e-01  6.55479282e-02\n",
            "   3.85641187e-01  1.03692003e-01 -6.80221096e-02 -3.15441430e-01\n",
            "  -1.71435047e-02 -3.35393190e-01 -5.11000037e-01 -2.30817422e-01\n",
            "  -6.27435669e-02  1.27435056e-02  2.05964968e-01  3.91149968e-02\n",
            "   1.05156727e-01 -5.23980409e-02 -3.24037224e-01 -3.57253939e-01\n",
            "  -3.65619421e-01  3.77251446e-01 -6.52182773e-02 -2.54837126e-01\n",
            "   1.01362121e+00 -4.72070903e-01 -5.49461506e-02  3.83183241e-01\n",
            "   2.50897378e-01 -2.14572221e-01 -8.72495323e-02 -2.76052028e-01\n",
            "  -3.91306251e-01 -1.78576615e-02 -5.57451427e-01 -2.10940354e-02\n",
            "  -4.62820567e-02  3.42158020e-01  2.68711716e-01  3.37198287e-01\n",
            "  -2.37804756e-01 -5.83499782e-02 -3.84712785e-01 -1.90443084e-01\n",
            "  -5.71250953e-02  2.78647989e-01 -4.53557312e-01 -3.81084830e-01\n",
            "  -4.46797013e-02 -1.84029505e-01 -2.05389056e-02 -9.12286192e-02\n",
            "  -2.05752216e-02 -2.46529169e-02 -4.09945756e-01 -3.04158404e-02\n",
            "  -2.95361161e-01 -1.32597789e-01  1.61673769e-01  2.62175292e-01\n",
            "  -1.90200284e-01  3.70366454e-01  3.00668869e-02  1.35093415e-02]\n",
            " [ 3.01578883e-02 -2.37828434e-01  2.38109887e-01  3.64208907e-01\n",
            "  -7.59171620e-02  1.31943867e-01 -1.79228112e-01 -2.86742181e-01\n",
            "  -2.30190880e-03  2.83739388e-01  3.17599386e-01  1.12740196e-01\n",
            "   3.08063298e-01 -4.87270094e-02  9.67348740e-02  3.78965050e-01\n",
            "  -3.94954681e-02 -1.47757247e-01 -1.49260715e-01 -3.57768983e-01\n",
            "  -5.22418797e-01 -1.49737716e-01  9.70156789e-02 -4.41049449e-02\n",
            "   4.18407619e-01  3.46211821e-01 -1.90556049e-03  2.80708492e-01\n",
            "   1.59760863e-01 -1.32634491e-02  5.53516373e-02  4.03986573e-01\n",
            "  -2.38058642e-01 -7.41188321e-03  1.39974177e-01  3.85881662e-01\n",
            "  -1.14870325e-01 -2.25720808e-01  2.99475193e-02  2.46829346e-01\n",
            "   2.04453915e-01 -4.01173308e-02 -8.13244358e-02  4.69992548e-01\n",
            "  -1.50758535e-01 -6.27778098e-02  3.94575983e-01  4.35725987e-01\n",
            "   2.93052662e-02 -1.13734894e-01  9.17367116e-02 -1.68223009e-01\n",
            "  -5.57625964e-02 -1.23702334e-02 -2.44928479e-01  1.95987716e-01\n",
            "   4.09181654e-01 -3.77076156e-02  1.11116081e-01  6.23088926e-02\n",
            "   2.99870893e-02 -2.42200479e-01  3.35579254e-02 -2.22262405e-02]\n",
            " [ 4.07474756e-01  6.91997588e-01  1.45615608e-01 -1.69917211e-01\n",
            "   2.30569303e-01 -1.67578105e-02  9.03735086e-02  4.67902213e-01\n",
            "  -4.96956566e-03 -2.46161725e-02 -2.46596888e-01 -3.18387359e-01\n",
            "  -9.97896716e-02  2.48381426e-03 -1.88093305e-01 -6.26759082e-02\n",
            "   4.18207645e-01 -1.37247458e-01  3.43592614e-01  1.45985171e-01\n",
            "   1.81026980e-01  9.90212858e-02  2.58181781e-01  2.72645295e-01\n",
            "   4.82035838e-02  1.30530149e-01 -2.29688901e-02 -5.90073705e-01\n",
            "   9.57345497e-03 -3.13905776e-01 -9.96240452e-02  1.08003952e-01\n",
            "  -4.60055918e-01 -2.22151708e-02 -3.18604112e-01  1.32899538e-01\n",
            "   4.90043700e-01  1.83948517e-01  7.81857073e-02  1.31432742e-01\n",
            "   2.09931701e-01  2.88185235e-02  5.80089331e-01  2.98020411e-02\n",
            "  -3.81370746e-02  1.51841208e-01 -1.32744238e-01  2.23276690e-01\n",
            "   2.75799245e-01 -1.78415358e-01  2.80693937e-02  1.30496085e-01\n",
            "  -3.98209803e-02  5.14112134e-03  2.49313936e-01 -1.94849834e-01\n",
            "  -1.01840176e-01 -7.06309751e-02  7.61384470e-03 -5.02044737e-01\n",
            "   4.63695675e-01 -3.76594722e-01  4.61156294e-02 -1.28651354e-02]]\n",
            "b3:\n",
            "[[-0.48480558]\n",
            " [ 0.5816229 ]\n",
            " [ 0.02825078]\n",
            " [-0.26894796]\n",
            " [ 0.7281275 ]\n",
            " [ 0.04277851]\n",
            " [-0.2632677 ]\n",
            " [ 0.39438877]\n",
            " [-0.5862575 ]\n",
            " [-0.17186952]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning curve represent a decrease in loss over the initial epochs. However, around epochs 20 to 22, we observe fluctuations in the curve. Notably, the validation loss curve exhibits more variation during this period. The provided the result for showcases the best parameters and the best validation loss achieved by the model.  \n",
        "\n",
        "**Get help of Generative AI to get the best parameters : Command given below:**\n",
        "\n",
        "\"logic to find best valuation loss of a model in python and best parameter\"."
      ],
      "metadata": {
        "id": "pMAb21z1i8Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add Dropout Regularization (5pt)**\n",
        "\n",
        "To add dropout regularization, you should modify the forward pass when applied to the training data and multiply the output of neurons in each layer (Z) by 1/* a random matrix of zeros and ones sampled from a Bernoulli distribution with probability . Where (.=1- dropout_rate) is the probability of keeping a neuron. To get a random matrix of zeros and ones with the same shape as Z, you can first create a Bernoulli distribution with probability  using tfp.distributions.Bernoulli then sample from this distribution.\n",
        "\n",
        "Note: The dropout should only be applied during training that is, it should only be applied to the training data. The dropout should NOT be applied at the time of inference that is, it should NOT be applied to the validation and test sets.\n",
        "\n",
        "Also note that the dropout should NOT be applied to the final layer because we do not want to drop any neuron in the final layer ( the final layer in a multi-class classification problem should have as many neurons as the number of output classes)\n",
        "Re-train the model with dropout regularization ( use a dropout rate between 0.2-0.5 ( that is a keep probability  between 0.8-0.5). Answer the following question:\n",
        "\n",
        "*   Does dropout regularization help reduce the gap between train and validation losses?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ-cfvYrQGaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Parameter Initialization\n",
        "def initialize_parameters(nx, nh1, nh2, ny):\n",
        "    tf.random.set_seed(195)\n",
        "    W1 = tf.Variable(tf.random.normal(shape=(nh1, nx), stddev=0.01), name=\"W1\")\n",
        "    b1 = tf.Variable(tf.zeros(shape=(nh1, 1), name=\"b1\"))\n",
        "    W2 = tf.Variable(tf.random.normal(shape=(nh2, nh1), stddev=0.01), name=\"W2\")\n",
        "    b2 = tf.Variable(tf.zeros(shape=(nh2, 1), name=\"b2\"))\n",
        "    W3 = tf.Variable(tf.random.normal(shape=(ny, nh2), stddev=0.01), name=\"W3\")\n",
        "    b3 = tf.Variable(tf.zeros(shape=(ny, 1), name=\"b3\"))\n",
        "\n",
        "    # Initialize velocity for each parameter\n",
        "    v_W1 = tf.Variable(tf.zeros_like(W1), trainable=False)\n",
        "    v_b1 = tf.Variable(tf.zeros_like(b1), trainable=False)\n",
        "    v_W2 = tf.Variable(tf.zeros_like(W2), trainable=False)\n",
        "    v_b2 = tf.Variable(tf.zeros_like(b2), trainable=False)\n",
        "    v_W3 = tf.Variable(tf.zeros_like(W3), trainable=False)\n",
        "    v_b3 = tf.Variable(tf.zeros_like(b3), trainable=False)\n",
        "\n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
        "    velocities = {\"v_W1\": v_W1, \"v_b1\": v_b1, \"v_W2\": v_W2, \"v_b2\": v_b2, \"v_W3\": v_W3, \"v_b3\": v_b3}\n",
        "\n",
        "    return parameters, velocities\n",
        "\n",
        "def dropout_samplemat(shape, dropout_rate, dtype=tf.float32):\n",
        "\n",
        "    bernoulli_dist = tfp.distributions.Bernoulli(probs =1 - dropout_rate) # Generated AI code\n",
        "    dropout_sample = bernoulli_dist.sample(shape) # Generated AI code\n",
        "    #print(dropout_mask)\n",
        "\n",
        "    return tf.cast(dropout_sample, dtype)\n",
        "\n",
        "# Forward Pass with Dropout Regularization during training\n",
        "def forward_pass(parameters, X, dropout_rate):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "\n",
        "    # Apply dropout only during training\n",
        "    Z1 = tf.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    dropout_1 = dropout_samplemat(tf.shape(A1), dropout_rate)\n",
        "    A1 *= dropout_1 / (1 - dropout_rate)\n",
        "\n",
        "    Z2 = tf.matmul(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
        "    A2 = tf.nn.relu(Z2)\n",
        "    dropout_2 = dropout_samplemat(tf.shape(A2), dropout_rate)\n",
        "    A2 *= dropout_2 / (1 - dropout_rate)\n",
        "\n",
        "    # Z1 = tf.matmul(parameters[\"W1\"], X) + parameters[\"b1\"]\n",
        "    # A1 = tf.nn.relu(Z1)\n",
        "    # Z2 = tf.matmul(parameters[\"W2\"], A1) + parameters[\"b2\"]\n",
        "    # A2 = tf.nn.relu(Z2)\n",
        "    # Z3 = tf.matmul(parameters[\"W3\"], A2) + parameters[\"b3\"]\n",
        "\n",
        "    Z3 = tf.matmul(parameters[\"W3\"], A2) + parameters[\"b3\"]\n",
        "\n",
        "\n",
        "    Yhat = tf.nn.softmax(Z3, axis=0)\n",
        "\n",
        "    return Yhat\n",
        "\n",
        "# Loss calculations\n",
        "def compute_loss(Y, Yhat):\n",
        "    loss = -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(Yhat + 1e-10), axis=0))\n",
        "    return loss\n",
        "\n",
        "# Backward Pass\n",
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients = tape.gradient(loss, parameters.values())\n",
        "    return gradients\n",
        "\n",
        "# Modify update_parameters to use Nesterov Momentum\n",
        "def update_parameters(parameters, velocities, gradients, learning_rate, mu):\n",
        "    for param, grad, vel in zip(parameters.values(), gradients, velocities.values()):\n",
        "        # Update velocity\n",
        "        vel.assign(mu * vel - learning_rate * grad)\n",
        "        # Update parameter using velocity\n",
        "        param.assign_add(mu * vel - learning_rate * grad)\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Create a new create_nn_model function to utilize the modified update_parameters\n",
        "def create_nn_model_Dropout(train_X, train_Y, val_X, val_Y, num_iterations, learning_rate, nh1, nh2, batch_size, mu, dropout_rate):\n",
        "    nx, m = train_X.shape\n",
        "    ny = train_Y.shape[0]\n",
        "\n",
        "    parameters, velocities = initialize_parameters(nx, nh1, nh2, ny)\n",
        "\n",
        "    best_val_loss = True\n",
        "    best_parameters = None\n",
        "    no_improvement_count = 0\n",
        "    min_learning_rate = 0.0001\n",
        "\n",
        "    val_losses = []\n",
        "    train_losses = []\n",
        "\n",
        "    num_batches = (m + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        epoch_train_loss = 0\n",
        "        train_loss = []\n",
        "\n",
        "        for batch in range(num_batches):\n",
        "            start = batch * batch_size\n",
        "            end = min(start + batch_size, m)\n",
        "            X_batch = train_X[:, start:end]\n",
        "            Y_batch = train_Y[:, start:end]\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                train_Yhat = forward_pass(parameters, tf.convert_to_tensor(X_batch, dtype=tf.float32), dropout_rate)\n",
        "                train_loss = compute_loss(tf.convert_to_tensor(Y_batch, dtype=tf.float32), train_Yhat)\n",
        "\n",
        "            gradients = backward_pass(parameters, train_loss, tape)\n",
        "            parameters = update_parameters(parameters, velocities, gradients, learning_rate, mu)\n",
        "\n",
        "            epoch_train_loss = epoch_train_loss + train_loss.numpy()  # Accumulate loss for the current batch\n",
        "\n",
        "\n",
        "        epoch_train_loss = epoch_train_loss/num_batches\n",
        "        train_losses.append(epoch_train_loss)  # Append epoch loss to train_losses\n",
        "\n",
        "\n",
        "        # Calculate validation loss after each epoch\n",
        "        val_Yhat = forward_pass(parameters, tf.convert_to_tensor(val_X, dtype=tf.float32), dropout_rate=0.0)\n",
        "        val_loss = compute_loss(tf.convert_to_tensor(val_Y, dtype=tf.float32), val_Yhat)\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        print(\"epoch {}: train_loss:{} val_loss{}\".format(i, epoch_train_loss, val_loss.numpy()))\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_parameters = parameters\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "\n",
        "\n",
        "        # If no improvement for 10 epochs, reduce learning rate by half\n",
        "        if no_improvement_count >= 10:\n",
        "            if learning_rate > min_learning_rate:\n",
        "                learning_rate = learning_rate * 0.5\n",
        "                no_improvement_count = 0\n",
        "                print(f\"Reducing learning rate to {learning_rate}\")\n",
        "\n",
        "    #print(\"Best val_loss :\" , best_val_loss)\n",
        "\n",
        "    history = {\"val_loss\": val_losses, \"train_loss\": train_losses}\n",
        "    return best_parameters, history\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"German_digits.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract features (X) and labels (Y)\n",
        "X = df.iloc[:, :-1].values / 255.0\n",
        "Y = df.iloc[:, -1].values\n",
        "\n",
        "# Split the dataset into train and test\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=12)\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 10\n",
        "train_Y = pd.get_dummies(train_Y).values\n",
        "test_Y = pd.get_dummies(test_Y).values\n",
        "\n",
        "# Transpose the datasets\n",
        "train_X = train_X.T\n",
        "train_Y = train_Y.T\n",
        "test_X = test_X.T\n",
        "test_Y = test_Y.T\n",
        "\n",
        "# print(\"train_X\", train_X.shape)\n",
        "# print(\"train_Y\", train_Y.shape)\n",
        "# print(\"test_X\", test_X.shape)\n",
        "# print(\"test_Y\", test_Y.shape)\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_iterations = 55\n",
        "nh1 = 128\n",
        "nh2 = 64\n",
        "batch_size = 32\n",
        "mu = 0.99\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Train the model\n",
        "best_parameters_dropout, history = create_nn_model_Dropout(train_X, train_Y, test_X, test_Y, num_iterations, learning_rate, nh1, nh2, batch_size, mu, dropout_rate)\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.plot(history['train_loss'], 'b', label='Training Loss')\n",
        "plt.plot(history['val_loss'], 'r', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f823785-e588-41c5-871b-1fcfe1ec2716",
        "id": "D3hYGXaRq8Nh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: train_loss:2.3029086246146813 val_loss2.3025577068328857\n",
            "epoch 1: train_loss:2.3019051895485267 val_loss2.3017027378082275\n",
            "epoch 2: train_loss:2.3005121441574783 val_loss2.2992660999298096\n",
            "epoch 3: train_loss:2.2948066608325854 val_loss2.2854056358337402\n",
            "epoch 4: train_loss:2.2360575328002104 val_loss2.108203411102295\n",
            "epoch 5: train_loss:1.8416563465788558 val_loss1.5040284395217896\n",
            "epoch 6: train_loss:1.324563377612346 val_loss1.0806617736816406\n",
            "epoch 7: train_loss:0.9604579962051667 val_loss0.8230413794517517\n",
            "epoch 8: train_loss:0.7130228860958202 val_loss0.6760463118553162\n",
            "epoch 9: train_loss:0.5596933343389012 val_loss0.5584534406661987\n",
            "epoch 10: train_loss:0.4639652549952 val_loss0.4871675372123718\n",
            "epoch 11: train_loss:0.36355414288537996 val_loss0.4713653028011322\n",
            "epoch 12: train_loss:0.3076675602713147 val_loss0.46048682928085327\n",
            "epoch 13: train_loss:0.25378194176130464 val_loss0.41088005900382996\n",
            "epoch 14: train_loss:0.21327075714597832 val_loss0.39485639333724976\n",
            "epoch 15: train_loss:0.20043106608696887 val_loss0.4027941823005676\n",
            "epoch 16: train_loss:0.1610053873008436 val_loss0.3790959417819977\n",
            "epoch 17: train_loss:0.1282030529602691 val_loss0.3795207738876343\n",
            "epoch 18: train_loss:0.1196818398563443 val_loss0.37164023518562317\n",
            "epoch 19: train_loss:0.10203486295030997 val_loss0.4009881913661957\n",
            "epoch 20: train_loss:0.09621700007669828 val_loss0.3971281051635742\n",
            "epoch 21: train_loss:0.08024727061458 val_loss0.3845996856689453\n",
            "epoch 22: train_loss:0.07357507897121413 val_loss0.40246954560279846\n",
            "epoch 23: train_loss:0.06230678484850639 val_loss0.41495731472969055\n",
            "epoch 24: train_loss:0.05328984504884428 val_loss0.4115537405014038\n",
            "epoch 25: train_loss:0.04654926180982174 val_loss0.40869516134262085\n",
            "epoch 26: train_loss:0.04563356284891163 val_loss0.4123742878437042\n",
            "epoch 27: train_loss:0.04517637277315664 val_loss0.41980594396591187\n",
            "epoch 28: train_loss:0.04110511355647431 val_loss0.4187305271625519\n",
            "Reducing learning rate to 0.0005\n",
            "epoch 29: train_loss:0.03668589794563616 val_loss0.4404808580875397\n",
            "epoch 30: train_loss:0.03866721670193648 val_loss0.39836594462394714\n",
            "epoch 31: train_loss:0.03276086018026412 val_loss0.43425244092941284\n",
            "epoch 32: train_loss:0.025013108490253084 val_loss0.42336785793304443\n",
            "epoch 33: train_loss:0.02575824692685929 val_loss0.44676119089126587\n",
            "epoch 34: train_loss:0.02396645812686902 val_loss0.4326682388782501\n",
            "epoch 35: train_loss:0.020678723556081857 val_loss0.43177559971809387\n",
            "epoch 36: train_loss:0.020069295501646895 val_loss0.4171867072582245\n",
            "epoch 37: train_loss:0.021532482146138645 val_loss0.43592897057533264\n",
            "epoch 38: train_loss:0.018653615466990247 val_loss0.4549802243709564\n",
            "Reducing learning rate to 0.00025\n",
            "epoch 39: train_loss:0.016248182634044823 val_loss0.4569535553455353\n",
            "epoch 40: train_loss:0.020757082407153002 val_loss0.46616271138191223\n",
            "epoch 41: train_loss:0.01748222375114507 val_loss0.4547865688800812\n",
            "epoch 42: train_loss:0.016462699907122867 val_loss0.4501517713069916\n",
            "epoch 43: train_loss:0.014333265413517586 val_loss0.45651814341545105\n",
            "epoch 44: train_loss:0.01693833310750674 val_loss0.451474130153656\n",
            "epoch 45: train_loss:0.015219452729037552 val_loss0.4516504108905792\n",
            "epoch 46: train_loss:0.013391129163865772 val_loss0.44758233428001404\n",
            "epoch 47: train_loss:0.012244747838620012 val_loss0.4446979761123657\n",
            "epoch 48: train_loss:0.015019813460205589 val_loss0.448322594165802\n",
            "Reducing learning rate to 0.000125\n",
            "epoch 49: train_loss:0.017889049268371704 val_loss0.4626189172267914\n",
            "epoch 50: train_loss:0.014255445631984453 val_loss0.45830237865448\n",
            "epoch 51: train_loss:0.014478951231901627 val_loss0.4556431174278259\n",
            "epoch 52: train_loss:0.013379983832065418 val_loss0.455106258392334\n",
            "epoch 53: train_loss:0.011726057472605224 val_loss0.4563775956630707\n",
            "epoch 54: train_loss:0.01227096578332822 val_loss0.4548719525337219\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dropout regularization helps in reduce the gap between train and validation losses and increase the accuracy on test data. We can see the same in the above graph."
      ],
      "metadata": {
        "id": "D7xWBRZGvHYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting and evaluating the NN model\n",
        "\n",
        "def predict(parameters, X):\n",
        "    Yhat = forward_pass(parameters, X, dropout_rate=0.0)\n",
        "    predictions = np.argmax(Yhat, axis=0)\n",
        "    return predictions\n",
        "\n",
        "# Use the trained parameters to make predictions on the test set\n",
        "test_predictions_do = predict(best_parameters_dropout, tf.convert_to_tensor(test_X, dtype=tf.float32))\n",
        "\n",
        "# Compare predicted labels with true labels to calculate test accuracy\n",
        "correct_predictions_do = np.sum(test_predictions_do == np.argmax(test_Y, axis=0))\n",
        "total_samples = test_Y.shape[1]\n",
        "test_accuracy_do = correct_predictions_do / total_samples * 100\n",
        "print(\"Test Accuracy: {:.1f}%\".format(test_accuracy_do))\n",
        "\n",
        "# Plot a few images in the test set along with their predicted labels\n",
        "num_images_to_plot = 5\n",
        "random_indices = np.random.choice(test_X.shape[1], num_images_to_plot, replace=False)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "for i, idx in enumerate(random_indices, 1):\n",
        "    plt.subplot(1, num_images_to_plot, i)\n",
        "    plt.imshow(test_X[:, idx].reshape(40, 40), cmap='gray')\n",
        "    plt.title(\"True Label: {}\\nPredicted Label: {}\".format(test_Y[:, idx].argmax(), test_predictions_do[idx]))\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "pVIEHhCQzZpg",
        "outputId": "8b9039c6-86e7-4506-ebdb-ff0f73d51432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 90.4%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get help of Generative AI to get the best parameters : Command given below:\n",
        "\n",
        "**\"random matrix same shape of z\".**"
      ],
      "metadata": {
        "id": "U1Sk8bZpAcU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete this assignment  I have referred few articles the links are given below:\n",
        "\n",
        "https://medium.com/@jaleeladejumo/gradient-descent-from-scratch-batch-gradient-descent-stochastic-gradient-descent-and-mini-batch-def681187473\n",
        "\n",
        "https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/"
      ],
      "metadata": {
        "id": "-6m104kWAncm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZm0i3BrAv9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}