{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a simple neural network for regression**\n"
        
      ],
      "metadata": {
        "id": "YGh79xwm0iyL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezdD9bLG0dln"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nx is the number of neurons in the input layer (i.e., the number of features in the dataset)\n",
        "nh is the number of neurons in the hidden layer\n",
        "ny is the number of neurons in the output layer (For this example we are using one nueron in the output layer so ny=1)\n",
        "\"\"\"\n",
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set tensorflow global random seed\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer. Note that weights and biases are defined as tensorflow variables instead of numpy arrays\n",
        "    W1=tf.Variable(tf.random.uniform(shape=(nh,nx), minval=-0.01, maxval=0.01), name=\"W1\")\n",
        "    b1=tf.Variable(tf.zeros(shape=(nh,1),name=\"b1\" ))\n",
        "    W2=tf.Variable(tf.random.uniform(shape=(ny,nh), minval=-0.01, maxval=0.01), name=\"W2\")\n",
        "    b2=tf.Variable(tf.zeros(shape=(ny,1), name=\"b2\"))\n",
        "\n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "cgg45UtI0kJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In forward pass we do the computations in the computational graph. We cache the intermediate nodes we will later need in the backward pass\n",
        "\"\"\"\n",
        "def forward_pass(parameters,X):\n",
        "    #the input image is read as an integer, use tf.cast to cast it to float before using it in fowrard pass computation.\n",
        "    X= tf.cast(X, tf.float32)\n",
        "    Z1= tf.matmul(parameters[\"W1\"],X)+ parameters[\"b1\"] # b1 is broadcasted n times before it is added to\n",
        "    A1=tf.nn.relu(Z1)\n",
        "    Z2=tf.matmul(parameters[\"W2\"],A1)+parameters[\"b2\"] #b2 is broadcasted n times before it is added to np.dpt(W2,A1)\n",
        "\n",
        "    Yhat=Z2\n",
        "\n",
        "    return Yhat\n"
      ],
      "metadata": {
        "id": "7PbG5EkRTn7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_loss(Y, Yhat):\n",
        "\n",
        "    \"\"\"Calculates Mean Squared Error loss for regression.\n",
        "\n",
        "    Args:\n",
        "        Y: Actual/observed output values\n",
        "        Yhat: Predicted output values\n",
        "\n",
        "    Returns:\n",
        "        MSE loss tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    squared_errors = tf.square(Y - Yhat)\n",
        "    loss = tf.reduce_mean(squared_errors)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LYd31O2wU26G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients= tape.gradient(loss,parameters)\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "rBduKTaVU358"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"].assign_sub(learning_rate*gradients[\"W1\"])\n",
        "    parameters[\"W2\"].assign_sub(learning_rate*gradients[\"W2\"])\n",
        "    parameters[\"b1\"].assign_sub(learning_rate*gradients[\"b1\"])\n",
        "    parameters[\"b2\"].assign_sub(learning_rate*gradients[\"b2\"])\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "bCtR7y8nU8I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNTD9QB95een"
      },
      "source": [
        "\"\"\"\n",
        "Arguments: train_X: is the training dataset (features)\n",
        "           train_Y: is the vector of labels for training_X\n",
        "           val_X: is the vector of validation dataset (features)\n",
        "           val_y: is the vector of labels for val_X\n",
        "           nh: is the number of neurons in the hidden layer\n",
        "           num_iterations: The number of iterations of gradient descent\n",
        "\"\"\"\n",
        "def create_nn_model(train_X,train_Y,nh, val_X, val_Y, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Do some safety check on the data before proceeding.\n",
        "    train_X and val_X must have the same number of features (i.e., same number of rows)\n",
        "    train_X must have the same number of examples as train_Y (i.e., same number of columns )\n",
        "    val_X must have the same number of examples as Val_Y\n",
        "    \"\"\"\n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "\n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "\n",
        "    # We want to use this network for binary classification, so we have only one neuron in the output layer with a sigmoid activation\n",
        "    ny=1\n",
        "\n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "\n",
        "\n",
        "    #initialize lists to store the training and valideation losses.\n",
        "    val_losses=[]\n",
        "    train_losses=[]\n",
        "\n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "\n",
        "      \"\"\"\n",
        "        run forward pass and compute the loss function on training and validation data.\n",
        "        Note that the forward pass and loss computations on the training data are enclosed inside the gradient tape context in order to build the computational graph.\n",
        "        The gradients are only computed on the training data and used to update the parameter. Validation data is not used for training and updating the parameters.\n",
        "        \"\"\"\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        #run the forward pass on train_X\n",
        "        train_Yhat=forward_pass(parameters,train_X)\n",
        "        #compute the train_loss\n",
        "        train_loss=compute_loss(train_Y,train_Yhat)\n",
        "\n",
        "\n",
        "       #compute validation loss\n",
        "      Yhat_val= forward_pass(parameters,val_X)\n",
        "      val_loss=compute_loss(val_Y,Yhat_val)\n",
        "\n",
        "      #print the trianing loss and validation loss for each iteration.\n",
        "      print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss,val_loss))\n",
        "\n",
        "       # append the train and validation loss for the current iteration to the train_losses and val_losses\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "\n",
        "      \"\"\"\n",
        "      Compute the gradients and update the parameters\n",
        "      \"\"\"\n",
        "      #compute the gradients on the training data\n",
        "      gradients=backward_pass(parameters,train_loss,tape)\n",
        "\n",
        "      # update the parameters\n",
        "      parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_losses,\n",
        "             \"train_loss\": train_losses}\n",
        "\n",
        "\n",
        "    #return the parameters and the history\n",
        "    return parameters, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Preparing California Housing Data (4pts)**\n",
        "\n",
        "Once you modified your network apply it to California Housing Dataset to predict the median house value.\n",
        "\n",
        "California housing dataset is already included as a sample dataset in Google Colab. Once in Colab, click on “files” on the left hand side and you will see California_housing_train.csv and california_housing_test.csv files.\n",
        "\n",
        "Import patndas and use pd.read_csv to read the train and test files. For example:\n",
        "train=pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
        "Before training your neural network with this data, you will have to do some preprocessing:\n",
        "\n",
        "\n",
        "*   Split the training data into 80% training and 20% validation. There are several ways to do this; for instance, you can use dataframe sample method\n",
        "\n",
        "\n",
        "> We will use the train data to build our model. Validation data to select and tune hyper- parameters and the test data for the final evaluation of the model after hyperparameters are selected.\n",
        "\n",
        "\n",
        "\n",
        "*   Convert the train/validation/and test data into numpy arrays using to_numpy method.\n",
        "*   Separate the features from the target variable (median_house_value) in train, validation, and test datasets.\n",
        "*   It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization:\n",
        "\n",
        "\n",
        "\n",
        "> for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in numpy using the mean and std methods to get the mean and standard deviation of each column in a numpy array. Make sure that you use the mean and std of the training data to normalize the validation and test data. You should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization.\n",
        "\n",
        "\n",
        "*   The target variable “median_house_value” has a large range from 17K-500K. Large target values can make the loss gradient large which results in large updates to the parameters possibly making the gradient descent algorithm unstable and fluctuating. Let’s divide the median_house_values by 100K to scale them down.\n",
        "*   The neural network we implemented in the lab uses the transpose of the features matrix where rows represent features and columns represent data points. Transpose the feature matrices for train/validation/and test data and reshape their target vectors to 2D arrays similar to what we did in the lab.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3uYuN86XQlbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
        "\n",
        "\n",
        "# Split train data into train and validation sets (80%/20%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Separate features and target variable\n",
        "train_features = train_data.drop(\"median_house_value\", axis=1)\n",
        "train_target = train_data[\"median_house_value\"]\n",
        "val_features = val_data.drop(\"median_house_value\", axis=1)\n",
        "val_target = val_data[\"median_house_value\"]\n",
        "\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "train_features_np = train_features.to_numpy()\n",
        "train_target_np = train_target.to_numpy().reshape(-1, 1)  # Reshape to 2D\n",
        "val_features_np = val_features.to_numpy()\n",
        "val_target_np = val_target.to_numpy().reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Feature normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features_np)\n",
        "val_features_scaled = scaler.transform(val_features_np)\n",
        "\n",
        "\n",
        "# Target variable scaling\n",
        "train_target_scaled = train_target_np / 100000\n",
        "val_target_scaled = val_target_np / 100000\n",
        "\n",
        "\n",
        "# Transpose features and reshape targets\n",
        "train_features_scaled_t = train_features_scaled.T\n",
        "train_target_scaled_t = train_target_scaled.T\n",
        "val_features_scaled_t = val_features_scaled.T\n",
        "val_target_scaled_t = val_target_scaled.T\n",
        "\n",
        "\n",
        "# Now you have the preprocessed data in NumPy arrays ready for your neural network!\n",
        "print(\"Train features shape:\", train_features_scaled_t.shape)\n",
        "print(\"Train target shape:\", train_target_scaled_t.shape)\n",
        "print(\"Validation features shape:\", val_features_scaled_t.shape)\n",
        "print(\"Validation target shape:\", val_target_scaled_t.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3xePtXyZG7E",
        "outputId": "2892c42a-a414-4ee0-ab0e-b90353ece997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (8, 13600)\n",
            "Train target shape: (1, 13600)\n",
            "Validation features shape: (8, 3400)\n",
            "Validation target shape: (1, 3400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similar conversion for test dataset.**"
      ],
      "metadata": {
        "id": "4P2WOhVHSy2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "test_data = pd.read_csv(\"sample_data/california_housing_test.csv\")\n",
        "\n",
        "# Separate features and target variable\n",
        "test_features = test_data.drop(\"median_house_value\", axis=1)\n",
        "test_target = test_data[\"median_house_value\"]\n",
        "\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "test_features_np = test_features.to_numpy()\n",
        "test_target_np = test_target.to_numpy().reshape(-1, 1)  # Reshape to 2D\n",
        "\n",
        "\n",
        "# Feature normalization\n",
        "test_features_scaled = scaler.fit_transform(test_features_np)\n",
        "\n",
        "# Target variable scaling\n",
        "test_target_scaled = test_target_np / 100000\n",
        "\n",
        "\n",
        "# Transpose features and reshape targets\n",
        "test_features_scaled_t = test_features_scaled.T\n",
        "test_target_scaled_t = test_target_scaled.T\n",
        "\n",
        "\n",
        "print(\"Train features shape:\", train_features_scaled_t.shape)\n",
        "print(\"Train target shape:\", train_target_scaled_t.shape)\n",
        "print(\"Test features shape:\", test_features_scaled_t.shape)\n",
        "print(\"Test target shape:\", test_target_scaled_t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge2HvmDESxxb",
        "outputId": "2918a64d-69a8-4439-c43d-b4c6d6adfdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (8, 13600)\n",
            "Train target shape: (1, 13600)\n",
            "Test features shape: (8, 3000)\n",
            "Test target shape: (1, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Training and hyper-parameter tuning (3 pts)**\n",
        "\n",
        "Now you are ready to train your regression neural network model on training data. Plot the train and validation losses for each iteration of gradient descent. Interpret the learning curves and answer the following questions:\n",
        "\n",
        "\n",
        "*   Is your model overfitting?\n",
        "*   Does your validation loss continue to decrease all the way to the last iteration or does it flatten after a certain number of iterations? If your validation loss continues to decrease all the way to the last iteration, then try training your model for a greater\n",
        "number of iterations and continue doing so until the validation loss starts to flatten or increase.\n",
        "*   Does your training loss fluctuate or is it monotonically decreasing? If your training loss fluctuates, consider using a smaller learning rate.\n",
        "\n",
        "Try to play with different values for the hyperparameters: learning_rate, number of iterations, and the number of neurons in the hidden layer. Try to increase the learning rate to speed up the training so long as the train loss does not start to fluctuate.\n",
        "\n",
        "*   Out of the hyperparameters you tried, what combination of learning rate, number of iterations, and the number of neurons in the hidden layer gave you the best validation loss?"
      ],
      "metadata": {
        "id": "7c2-dIspRbXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "Lets start with\n",
        "* iterations=1000\n",
        "* nh=50\n",
        "* learning_rate = 0.01\n"
      ],
      "metadata": {
        "id": "YUNh4Y8LV_DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=200\n",
        "nh=50\n",
        "learning_rate = 0.01\n",
        "parameters, history=create_nn_model(train_features_scaled_t,train_target_scaled_t,nh, val_features_scaled_t, val_target_scaled_t, iterations, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqXbltaMTg9M",
        "outputId": "68cd02c8-4ca1-4854-904a-eb187f23bef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.608718395233154 val_loss5.7732834815979\n",
            "iteration 1 :train_loss:5.439090251922607 val_loss5.601189136505127\n",
            "iteration 2 :train_loss:5.2761921882629395 val_loss5.435875415802002\n",
            "iteration 3 :train_loss:5.11975622177124 val_loss5.277072429656982\n",
            "iteration 4 :train_loss:4.969523906707764 val_loss5.1245198249816895\n",
            "iteration 5 :train_loss:4.825247764587402 val_loss4.977971076965332\n",
            "iteration 6 :train_loss:4.686690807342529 val_loss4.837185382843018\n",
            "iteration 7 :train_loss:4.553624629974365 val_loss4.701935291290283\n",
            "iteration 8 :train_loss:4.425830841064453 val_loss4.5720014572143555\n",
            "iteration 9 :train_loss:4.303099155426025 val_loss4.447172164916992\n",
            "iteration 10 :train_loss:4.1852288246154785 val_loss4.327245712280273\n",
            "iteration 11 :train_loss:4.072025299072266 val_loss4.212027549743652\n",
            "iteration 12 :train_loss:3.9633047580718994 val_loss4.10133171081543\n",
            "iteration 13 :train_loss:3.858887195587158 val_loss3.994978189468384\n",
            "iteration 14 :train_loss:3.7586028575897217 val_loss3.892796516418457\n",
            "iteration 15 :train_loss:3.6622865200042725 val_loss3.7946202754974365\n",
            "iteration 16 :train_loss:3.56978178024292 val_loss3.700291872024536\n",
            "iteration 17 :train_loss:3.480936050415039 val_loss3.609658718109131\n",
            "iteration 18 :train_loss:3.3956048488616943 val_loss3.5225751399993896\n",
            "iteration 19 :train_loss:3.313647747039795 val_loss3.43890118598938\n",
            "iteration 20 :train_loss:3.234931707382202 val_loss3.3585007190704346\n",
            "iteration 21 :train_loss:3.1593267917633057 val_loss3.281245708465576\n",
            "iteration 22 :train_loss:3.0867106914520264 val_loss3.2070112228393555\n",
            "iteration 23 :train_loss:3.0169639587402344 val_loss3.1356778144836426\n",
            "iteration 24 :train_loss:2.9499733448028564 val_loss3.067131757736206\n",
            "iteration 25 :train_loss:2.8856289386749268 val_loss3.0012624263763428\n",
            "iteration 26 :train_loss:2.8238258361816406 val_loss2.9379642009735107\n",
            "iteration 27 :train_loss:2.764463186264038 val_loss2.877135992050171\n",
            "iteration 28 :train_loss:2.7074437141418457 val_loss2.8186798095703125\n",
            "iteration 29 :train_loss:2.652674913406372 val_loss2.7625017166137695\n",
            "iteration 30 :train_loss:2.600066661834717 val_loss2.7085120677948\n",
            "iteration 31 :train_loss:2.5495336055755615 val_loss2.6566245555877686\n",
            "iteration 32 :train_loss:2.500993013381958 val_loss2.6067559719085693\n",
            "iteration 33 :train_loss:2.4543657302856445 val_loss2.5588266849517822\n",
            "iteration 34 :train_loss:2.409575939178467 val_loss2.5127599239349365\n",
            "iteration 35 :train_loss:2.3665504455566406 val_loss2.4684829711914062\n",
            "iteration 36 :train_loss:2.325218915939331 val_loss2.4259238243103027\n",
            "iteration 37 :train_loss:2.2855138778686523 val_loss2.385014533996582\n",
            "iteration 38 :train_loss:2.247370719909668 val_loss2.3456907272338867\n",
            "iteration 39 :train_loss:2.210726737976074 val_loss2.307889223098755\n",
            "iteration 40 :train_loss:2.175523042678833 val_loss2.2715494632720947\n",
            "iteration 41 :train_loss:2.1417016983032227 val_loss2.2366139888763428\n",
            "iteration 42 :train_loss:2.1092076301574707 val_loss2.2030277252197266\n",
            "iteration 43 :train_loss:2.0779879093170166 val_loss2.170736074447632\n",
            "iteration 44 :train_loss:2.0479917526245117 val_loss2.139688730239868\n",
            "iteration 45 :train_loss:2.019169807434082 val_loss2.1098358631134033\n",
            "iteration 46 :train_loss:1.9914753437042236 val_loss2.0811305046081543\n",
            "iteration 47 :train_loss:1.9648635387420654 val_loss2.0535266399383545\n",
            "iteration 48 :train_loss:1.9392904043197632 val_loss2.0269808769226074\n",
            "iteration 49 :train_loss:1.9147144556045532 val_loss2.001451253890991\n",
            "iteration 50 :train_loss:1.8910959959030151 val_loss1.97689688205719\n",
            "iteration 51 :train_loss:1.8683964014053345 val_loss1.9532792568206787\n",
            "iteration 52 :train_loss:1.84657883644104 val_loss1.9305610656738281\n",
            "iteration 53 :train_loss:1.8256077766418457 val_loss1.9087063074111938\n",
            "iteration 54 :train_loss:1.8054486513137817 val_loss1.8876800537109375\n",
            "iteration 55 :train_loss:1.7860691547393799 val_loss1.8674497604370117\n",
            "iteration 56 :train_loss:1.7674375772476196 val_loss1.8479831218719482\n",
            "iteration 57 :train_loss:1.7495237588882446 val_loss1.829249620437622\n",
            "iteration 58 :train_loss:1.7322983741760254 val_loss1.8112196922302246\n",
            "iteration 59 :train_loss:1.7157334089279175 val_loss1.793865442276001\n",
            "iteration 60 :train_loss:1.6998023986816406 val_loss1.7771594524383545\n",
            "iteration 61 :train_loss:1.6844786405563354 val_loss1.761075496673584\n",
            "iteration 62 :train_loss:1.669737458229065 val_loss1.7455881834030151\n",
            "iteration 63 :train_loss:1.6555554866790771 val_loss1.7306737899780273\n",
            "iteration 64 :train_loss:1.6419099569320679 val_loss1.7163091897964478\n",
            "iteration 65 :train_loss:1.628778100013733 val_loss1.7024718523025513\n",
            "iteration 66 :train_loss:1.6161389350891113 val_loss1.68913996219635\n",
            "iteration 67 :train_loss:1.603972315788269 val_loss1.676293134689331\n",
            "iteration 68 :train_loss:1.5922582149505615 val_loss1.6639111042022705\n",
            "iteration 69 :train_loss:1.5809783935546875 val_loss1.6519755125045776\n",
            "iteration 70 :train_loss:1.5701148509979248 val_loss1.6404681205749512\n",
            "iteration 71 :train_loss:1.5596495866775513 val_loss1.6293706893920898\n",
            "iteration 72 :train_loss:1.5495656728744507 val_loss1.6186658143997192\n",
            "iteration 73 :train_loss:1.5398468971252441 val_loss1.6083375215530396\n",
            "iteration 74 :train_loss:1.5304781198501587 val_loss1.5983699560165405\n",
            "iteration 75 :train_loss:1.5214437246322632 val_loss1.5887477397918701\n",
            "iteration 76 :train_loss:1.5127301216125488 val_loss1.5794564485549927\n",
            "iteration 77 :train_loss:1.5043230056762695 val_loss1.5704821348190308\n",
            "iteration 78 :train_loss:1.4962093830108643 val_loss1.5618109703063965\n",
            "iteration 79 :train_loss:1.4883763790130615 val_loss1.5534300804138184\n",
            "iteration 80 :train_loss:1.4808117151260376 val_loss1.545326590538025\n",
            "iteration 81 :train_loss:1.473503828048706 val_loss1.5374888181686401\n",
            "iteration 82 :train_loss:1.46644127368927 val_loss1.5299046039581299\n",
            "iteration 83 :train_loss:1.459612488746643 val_loss1.522562861442566\n",
            "iteration 84 :train_loss:1.4530075788497925 val_loss1.5154529809951782\n",
            "iteration 85 :train_loss:1.4466158151626587 val_loss1.508565068244934\n",
            "iteration 86 :train_loss:1.4404281377792358 val_loss1.5018885135650635\n",
            "iteration 87 :train_loss:1.4344350099563599 val_loss1.4954144954681396\n",
            "iteration 88 :train_loss:1.4286274909973145 val_loss1.4891334772109985\n",
            "iteration 89 :train_loss:1.422996997833252 val_loss1.4830362796783447\n",
            "iteration 90 :train_loss:1.4175350666046143 val_loss1.4771145582199097\n",
            "iteration 91 :train_loss:1.4122328758239746 val_loss1.4713599681854248\n",
            "iteration 92 :train_loss:1.407083511352539 val_loss1.4657646417617798\n",
            "iteration 93 :train_loss:1.402078628540039 val_loss1.460321068763733\n",
            "iteration 94 :train_loss:1.3972113132476807 val_loss1.4550212621688843\n",
            "iteration 95 :train_loss:1.3924744129180908 val_loss1.4498594999313354\n",
            "iteration 96 :train_loss:1.3878620862960815 val_loss1.444827675819397\n",
            "iteration 97 :train_loss:1.3833673000335693 val_loss1.4399192333221436\n",
            "iteration 98 :train_loss:1.3789840936660767 val_loss1.43512761592865\n",
            "iteration 99 :train_loss:1.3747055530548096 val_loss1.4304471015930176\n",
            "iteration 100 :train_loss:1.370526671409607 val_loss1.4258712530136108\n",
            "iteration 101 :train_loss:1.366441249847412 val_loss1.421394944190979\n",
            "iteration 102 :train_loss:1.3624438047409058 val_loss1.4170117378234863\n",
            "iteration 103 :train_loss:1.3585290908813477 val_loss1.4127163887023926\n",
            "iteration 104 :train_loss:1.3546925783157349 val_loss1.4085038900375366\n",
            "iteration 105 :train_loss:1.3509291410446167 val_loss1.4043689966201782\n",
            "iteration 106 :train_loss:1.3472346067428589 val_loss1.4003076553344727\n",
            "iteration 107 :train_loss:1.3436038494110107 val_loss1.3963147401809692\n",
            "iteration 108 :train_loss:1.3400330543518066 val_loss1.3923864364624023\n",
            "iteration 109 :train_loss:1.3365179300308228 val_loss1.38851797580719\n",
            "iteration 110 :train_loss:1.3330543041229248 val_loss1.3847050666809082\n",
            "iteration 111 :train_loss:1.329638123512268 val_loss1.3809436559677124\n",
            "iteration 112 :train_loss:1.326265811920166 val_loss1.3772300481796265\n",
            "iteration 113 :train_loss:1.3229337930679321 val_loss1.3735606670379639\n",
            "iteration 114 :train_loss:1.3196384906768799 val_loss1.3699313402175903\n",
            "iteration 115 :train_loss:1.3163756132125854 val_loss1.3663378953933716\n",
            "iteration 116 :train_loss:1.3131426572799683 val_loss1.3627771139144897\n",
            "iteration 117 :train_loss:1.3099359273910522 val_loss1.3592466115951538\n",
            "iteration 118 :train_loss:1.3067526817321777 val_loss1.3557424545288086\n",
            "iteration 119 :train_loss:1.303590178489685 val_loss1.3522619009017944\n",
            "iteration 120 :train_loss:1.3004454374313354 val_loss1.3488019704818726\n",
            "iteration 121 :train_loss:1.297316074371338 val_loss1.3453598022460938\n",
            "iteration 122 :train_loss:1.294198989868164 val_loss1.341932773590088\n",
            "iteration 123 :train_loss:1.2910925149917603 val_loss1.338518500328064\n",
            "iteration 124 :train_loss:1.2879928350448608 val_loss1.3351136445999146\n",
            "iteration 125 :train_loss:1.2848972082138062 val_loss1.3317155838012695\n",
            "iteration 126 :train_loss:1.2818034887313843 val_loss1.3283212184906006\n",
            "iteration 127 :train_loss:1.278710126876831 val_loss1.3249295949935913\n",
            "iteration 128 :train_loss:1.275614619255066 val_loss1.3215382099151611\n",
            "iteration 129 :train_loss:1.272514820098877 val_loss1.3181442022323608\n",
            "iteration 130 :train_loss:1.2694091796875 val_loss1.3147461414337158\n",
            "iteration 131 :train_loss:1.266296148300171 val_loss1.3113411664962769\n",
            "iteration 132 :train_loss:1.2631733417510986 val_loss1.307928204536438\n",
            "iteration 133 :train_loss:1.260039210319519 val_loss1.3045051097869873\n",
            "iteration 134 :train_loss:1.256892204284668 val_loss1.3010708093643188\n",
            "iteration 135 :train_loss:1.253730297088623 val_loss1.2976229190826416\n",
            "iteration 136 :train_loss:1.2505526542663574 val_loss1.2941598892211914\n",
            "iteration 137 :train_loss:1.2473574876785278 val_loss1.2906804084777832\n",
            "iteration 138 :train_loss:1.2441426515579224 val_loss1.2871829271316528\n",
            "iteration 139 :train_loss:1.2409073114395142 val_loss1.2836658954620361\n",
            "iteration 140 :train_loss:1.2376503944396973 val_loss1.280128002166748\n",
            "iteration 141 :train_loss:1.234370231628418 val_loss1.2765679359436035\n",
            "iteration 142 :train_loss:1.2310665845870972 val_loss1.272984266281128\n",
            "iteration 143 :train_loss:1.2277371883392334 val_loss1.2693761587142944\n",
            "iteration 144 :train_loss:1.2243818044662476 val_loss1.2657421827316284\n",
            "iteration 145 :train_loss:1.2209992408752441 val_loss1.2620809078216553\n",
            "iteration 146 :train_loss:1.2175887823104858 val_loss1.2583918571472168\n",
            "iteration 147 :train_loss:1.214150071144104 val_loss1.254673957824707\n",
            "iteration 148 :train_loss:1.2106821537017822 val_loss1.2509269714355469\n",
            "iteration 149 :train_loss:1.2071834802627563 val_loss1.2471495866775513\n",
            "iteration 150 :train_loss:1.2036534547805786 val_loss1.2433393001556396\n",
            "iteration 151 :train_loss:1.200092077255249 val_loss1.239498257637024\n",
            "iteration 152 :train_loss:1.1964986324310303 val_loss1.2356250286102295\n",
            "iteration 153 :train_loss:1.1928725242614746 val_loss1.2317193746566772\n",
            "iteration 154 :train_loss:1.1892129182815552 val_loss1.227779507637024\n",
            "iteration 155 :train_loss:1.1855202913284302 val_loss1.2238062620162964\n",
            "iteration 156 :train_loss:1.1817940473556519 val_loss1.2197980880737305\n",
            "iteration 157 :train_loss:1.1780340671539307 val_loss1.2157559394836426\n",
            "iteration 158 :train_loss:1.1742395162582397 val_loss1.2116791009902954\n",
            "iteration 159 :train_loss:1.1704102754592896 val_loss1.2075673341751099\n",
            "iteration 160 :train_loss:1.1665468215942383 val_loss1.2034204006195068\n",
            "iteration 161 :train_loss:1.1626487970352173 val_loss1.1992385387420654\n",
            "iteration 162 :train_loss:1.1587172746658325 val_loss1.1950222253799438\n",
            "iteration 163 :train_loss:1.1547520160675049 val_loss1.190771222114563\n",
            "iteration 164 :train_loss:1.1507536172866821 val_loss1.1864855289459229\n",
            "iteration 165 :train_loss:1.1467223167419434 val_loss1.1821659803390503\n",
            "iteration 166 :train_loss:1.1426583528518677 val_loss1.1778128147125244\n",
            "iteration 167 :train_loss:1.1385616064071655 val_loss1.1734254360198975\n",
            "iteration 168 :train_loss:1.1344331502914429 val_loss1.1690043210983276\n",
            "iteration 169 :train_loss:1.1302728652954102 val_loss1.16455078125\n",
            "iteration 170 :train_loss:1.1260817050933838 val_loss1.1600648164749146\n",
            "iteration 171 :train_loss:1.1218609809875488 val_loss1.1555472612380981\n",
            "iteration 172 :train_loss:1.1176111698150635 val_loss1.1509994268417358\n",
            "iteration 173 :train_loss:1.1133335828781128 val_loss1.146422266960144\n",
            "iteration 174 :train_loss:1.109027624130249 val_loss1.1418160200119019\n",
            "iteration 175 :train_loss:1.1046942472457886 val_loss1.1371811628341675\n",
            "iteration 176 :train_loss:1.100334644317627 val_loss1.1325187683105469\n",
            "iteration 177 :train_loss:1.0959504842758179 val_loss1.127830147743225\n",
            "iteration 178 :train_loss:1.0915420055389404 val_loss1.1231160163879395\n",
            "iteration 179 :train_loss:1.0871102809906006 val_loss1.1183772087097168\n",
            "iteration 180 :train_loss:1.0826566219329834 val_loss1.1136157512664795\n",
            "iteration 181 :train_loss:1.0781821012496948 val_loss1.1088321208953857\n",
            "iteration 182 :train_loss:1.0736875534057617 val_loss1.104027271270752\n",
            "iteration 183 :train_loss:1.0691739320755005 val_loss1.0992026329040527\n",
            "iteration 184 :train_loss:1.0646429061889648 val_loss1.0943593978881836\n",
            "iteration 185 :train_loss:1.0600957870483398 val_loss1.089497685432434\n",
            "iteration 186 :train_loss:1.0555332899093628 val_loss1.084619402885437\n",
            "iteration 187 :train_loss:1.0509575605392456 val_loss1.0797264575958252\n",
            "iteration 188 :train_loss:1.0463696718215942 val_loss1.0748201608657837\n",
            "iteration 189 :train_loss:1.041771411895752 val_loss1.0699023008346558\n",
            "iteration 190 :train_loss:1.0371642112731934 val_loss1.064974308013916\n",
            "iteration 191 :train_loss:1.0325493812561035 val_loss1.0600379705429077\n",
            "iteration 192 :train_loss:1.0279282331466675 val_loss1.055093765258789\n",
            "iteration 193 :train_loss:1.0233025550842285 val_loss1.0501443147659302\n",
            "iteration 194 :train_loss:1.0186736583709717 val_loss1.0451910495758057\n",
            "iteration 195 :train_loss:1.0140427350997925 val_loss1.040235161781311\n",
            "iteration 196 :train_loss:1.009411334991455 val_loss1.0352777242660522\n",
            "iteration 197 :train_loss:1.004780650138855 val_loss1.030320405960083\n",
            "iteration 198 :train_loss:1.0001529455184937 val_loss1.025364875793457\n",
            "iteration 199 :train_loss:0.9955300092697144 val_loss1.0204132795333862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(0, iterations), history[\"train_loss\"], 'b', label='Train Loss')\n",
        "plt.plot(range(0, iterations), history[\"val_loss\"], 'r', label='Validation Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "qY5TyRT4XMOI",
        "outputId": "825d8cda-debd-454f-833f-24d6e9ed69fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The model seems like overfitting because the there are not much different between training loss and validation loss. Training loss is not much fluctuating, its decreasing over iterations.\n",
        "\n",
        "The validation loss shows almost same values over 100 iterations. We will try with different parameter such as :\n",
        "\n",
        "> Will use below parameters to see the losses\n",
        "\n",
        "* Learning_rates = [0.001, 0.01, 0.1]\n",
        "* Iterations = [50,100, 150]\n",
        "* hidden_neurons_list = [32, 64, 128]\n"
      ],
      "metadata": {
        "id": "jXv0PBtaW7l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=50\n",
        "nh=32\n",
        "learning_rate = 0.001\n",
        "parameters, history=create_nn_model(train_features_scaled_t,train_target_scaled_t,nh, val_features_scaled_t, val_target_scaled_t, iterations, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GlQh_fxZgyP",
        "outputId": "d7f72f0d-d8ac-462f-aabb-76d071b10859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.608731746673584 val_loss5.773290634155273\n",
            "iteration 1 :train_loss:5.591633319854736 val_loss5.755946636199951\n",
            "iteration 2 :train_loss:5.57460355758667 val_loss5.738671779632568\n",
            "iteration 3 :train_loss:5.557641506195068 val_loss5.72146463394165\n",
            "iteration 4 :train_loss:5.540748119354248 val_loss5.704326629638672\n",
            "iteration 5 :train_loss:5.523922443389893 val_loss5.687256336212158\n",
            "iteration 6 :train_loss:5.5071635246276855 val_loss5.670253753662109\n",
            "iteration 7 :train_loss:5.490470886230469 val_loss5.653319358825684\n",
            "iteration 8 :train_loss:5.473845958709717 val_loss5.636451244354248\n",
            "iteration 9 :train_loss:5.457287311553955 val_loss5.6196513175964355\n",
            "iteration 10 :train_loss:5.440794944763184 val_loss5.602917194366455\n",
            "iteration 11 :train_loss:5.4243693351745605 val_loss5.586249351501465\n",
            "iteration 12 :train_loss:5.408008575439453 val_loss5.569648265838623\n",
            "iteration 13 :train_loss:5.391712665557861 val_loss5.553112983703613\n",
            "iteration 14 :train_loss:5.375483512878418 val_loss5.536643028259277\n",
            "iteration 15 :train_loss:5.359318256378174 val_loss5.520238876342773\n",
            "iteration 16 :train_loss:5.343216896057129 val_loss5.503899574279785\n",
            "iteration 17 :train_loss:5.327181339263916 val_loss5.4876251220703125\n",
            "iteration 18 :train_loss:5.311209201812744 val_loss5.4714155197143555\n",
            "iteration 19 :train_loss:5.29530143737793 val_loss5.455269813537598\n",
            "iteration 20 :train_loss:5.279456615447998 val_loss5.439188480377197\n",
            "iteration 21 :train_loss:5.263675689697266 val_loss5.423171043395996\n",
            "iteration 22 :train_loss:5.247957229614258 val_loss5.407217502593994\n",
            "iteration 23 :train_loss:5.232302188873291 val_loss5.391326904296875\n",
            "iteration 24 :train_loss:5.216710567474365 val_loss5.375499248504639\n",
            "iteration 25 :train_loss:5.2011799812316895 val_loss5.359734535217285\n",
            "iteration 26 :train_loss:5.1857123374938965 val_loss5.3440327644348145\n",
            "iteration 27 :train_loss:5.1703057289123535 val_loss5.328392505645752\n",
            "iteration 28 :train_loss:5.154961109161377 val_loss5.312815189361572\n",
            "iteration 29 :train_loss:5.139678478240967 val_loss5.297298908233643\n",
            "iteration 30 :train_loss:5.12445592880249 val_loss5.2818450927734375\n",
            "iteration 31 :train_loss:5.109294414520264 val_loss5.266452312469482\n",
            "iteration 32 :train_loss:5.094193458557129 val_loss5.251120090484619\n",
            "iteration 33 :train_loss:5.079153060913086 val_loss5.235849380493164\n",
            "iteration 34 :train_loss:5.064174175262451 val_loss5.220638751983643\n",
            "iteration 35 :train_loss:5.049252986907959 val_loss5.205489635467529\n",
            "iteration 36 :train_loss:5.034392356872559 val_loss5.190398693084717\n",
            "iteration 37 :train_loss:5.019591331481934 val_loss5.175368785858154\n",
            "iteration 38 :train_loss:5.004849433898926 val_loss5.160398483276367\n",
            "iteration 39 :train_loss:4.990166664123535 val_loss5.145487308502197\n",
            "iteration 40 :train_loss:4.975543022155762 val_loss5.130635738372803\n",
            "iteration 41 :train_loss:4.960976600646973 val_loss5.115842819213867\n",
            "iteration 42 :train_loss:4.946468830108643 val_loss5.101108551025391\n",
            "iteration 43 :train_loss:4.932019233703613 val_loss5.086433410644531\n",
            "iteration 44 :train_loss:4.917627334594727 val_loss5.071815490722656\n",
            "iteration 45 :train_loss:4.903293132781982 val_loss5.057255744934082\n",
            "iteration 46 :train_loss:4.889016151428223 val_loss5.04275369644165\n",
            "iteration 47 :train_loss:4.8747968673706055 val_loss5.028309345245361\n",
            "iteration 48 :train_loss:4.860633850097656 val_loss5.013922214508057\n",
            "iteration 49 :train_loss:4.846527099609375 val_loss4.999592304229736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=100\n",
        "nh=64\n",
        "learning_rate = 0.01\n",
        "parameters, history=create_nn_model(train_features_scaled_t,train_target_scaled_t,nh, val_features_scaled_t, val_target_scaled_t, iterations, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYmtR16yZhC4",
        "outputId": "29a33c37-b970-4443-843d-f5fc9e1ba2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.609335899353027 val_loss5.773921966552734\n",
            "iteration 1 :train_loss:5.439530849456787 val_loss5.601646423339844\n",
            "iteration 2 :train_loss:5.2764716148376465 val_loss5.436166286468506\n",
            "iteration 3 :train_loss:5.119887828826904 val_loss5.277209281921387\n",
            "iteration 4 :train_loss:4.9695210456848145 val_loss5.124517440795898\n",
            "iteration 5 :train_loss:4.825119495391846 val_loss4.97783899307251\n",
            "iteration 6 :train_loss:4.686446666717529 val_loss4.836933612823486\n",
            "iteration 7 :train_loss:4.553273677825928 val_loss4.701572418212891\n",
            "iteration 8 :train_loss:4.425379753112793 val_loss4.571534156799316\n",
            "iteration 9 :train_loss:4.302555084228516 val_loss4.446608066558838\n",
            "iteration 10 :train_loss:4.184597015380859 val_loss4.326590061187744\n",
            "iteration 11 :train_loss:4.0713114738464355 val_loss4.2112860679626465\n",
            "iteration 12 :train_loss:3.962512969970703 val_loss4.100509166717529\n",
            "iteration 13 :train_loss:3.8580222129821777 val_loss3.9940779209136963\n",
            "iteration 14 :train_loss:3.7576675415039062 val_loss3.891822099685669\n",
            "iteration 15 :train_loss:3.6612844467163086 val_loss3.7935760021209717\n",
            "iteration 16 :train_loss:3.5687155723571777 val_loss3.6991796493530273\n",
            "iteration 17 :train_loss:3.479808807373047 val_loss3.6084816455841064\n",
            "iteration 18 :train_loss:3.394418478012085 val_loss3.5213353633880615\n",
            "iteration 19 :train_loss:3.312403440475464 val_loss3.4375994205474854\n",
            "iteration 20 :train_loss:3.2336316108703613 val_loss3.3571395874023438\n",
            "iteration 21 :train_loss:3.157973051071167 val_loss3.279827117919922\n",
            "iteration 22 :train_loss:3.085303783416748 val_loss3.2055351734161377\n",
            "iteration 23 :train_loss:3.01550555229187 val_loss3.1341466903686523\n",
            "iteration 24 :train_loss:2.9484639167785645 val_loss3.0655457973480225\n",
            "iteration 25 :train_loss:2.8840694427490234 val_loss2.999621629714966\n",
            "iteration 26 :train_loss:2.822216272354126 val_loss2.936269521713257\n",
            "iteration 27 :train_loss:2.7628045082092285 val_loss2.875387668609619\n",
            "iteration 28 :train_loss:2.705735921859741 val_loss2.816878080368042\n",
            "iteration 29 :train_loss:2.6509180068969727 val_loss2.7606472969055176\n",
            "iteration 30 :train_loss:2.5982611179351807 val_loss2.7066047191619873\n",
            "iteration 31 :train_loss:2.5476791858673096 val_loss2.654663562774658\n",
            "iteration 32 :train_loss:2.4990899562835693 val_loss2.6047418117523193\n",
            "iteration 33 :train_loss:2.452413558959961 val_loss2.5567586421966553\n",
            "iteration 34 :train_loss:2.40757417678833 val_loss2.510637044906616\n",
            "iteration 35 :train_loss:2.3644986152648926 val_loss2.4663052558898926\n",
            "iteration 36 :train_loss:2.3231163024902344 val_loss2.4236905574798584\n",
            "iteration 37 :train_loss:2.283360004425049 val_loss2.382725477218628\n",
            "iteration 38 :train_loss:2.2451648712158203 val_loss2.3433444499969482\n",
            "iteration 39 :train_loss:2.208468437194824 val_loss2.3054847717285156\n",
            "iteration 40 :train_loss:2.173211097717285 val_loss2.2690861225128174\n",
            "iteration 41 :train_loss:2.1393351554870605 val_loss2.234090566635132\n",
            "iteration 42 :train_loss:2.106785774230957 val_loss2.2004427909851074\n",
            "iteration 43 :train_loss:2.0755090713500977 val_loss2.1680891513824463\n",
            "iteration 44 :train_loss:2.04545521736145 val_loss2.1369781494140625\n",
            "iteration 45 :train_loss:2.0165746212005615 val_loss2.107060194015503\n",
            "iteration 46 :train_loss:1.9888198375701904 val_loss2.0782883167266846\n",
            "iteration 47 :train_loss:1.9621456861495972 val_loss2.05061674118042\n",
            "iteration 48 :train_loss:1.936509132385254 val_loss2.024001359939575\n",
            "iteration 49 :train_loss:1.9118684530258179 val_loss1.9984004497528076\n",
            "iteration 50 :train_loss:1.888183355331421 val_loss1.9737730026245117\n",
            "iteration 51 :train_loss:1.865415334701538 val_loss1.950080156326294\n",
            "iteration 52 :train_loss:1.84352707862854 val_loss1.927284836769104\n",
            "iteration 53 :train_loss:1.8224836587905884 val_loss1.9053510427474976\n",
            "iteration 54 :train_loss:1.8022503852844238 val_loss1.8842434883117676\n",
            "iteration 55 :train_loss:1.7827942371368408 val_loss1.8639297485351562\n",
            "iteration 56 :train_loss:1.7640843391418457 val_loss1.8443772792816162\n",
            "iteration 57 :train_loss:1.7460896968841553 val_loss1.8255558013916016\n",
            "iteration 58 :train_loss:1.7287812232971191 val_loss1.8074356317520142\n",
            "iteration 59 :train_loss:1.7121309041976929 val_loss1.7899879217147827\n",
            "iteration 60 :train_loss:1.696111798286438 val_loss1.773186445236206\n",
            "iteration 61 :train_loss:1.6806976795196533 val_loss1.7570042610168457\n",
            "iteration 62 :train_loss:1.6658633947372437 val_loss1.7414157390594482\n",
            "iteration 63 :train_loss:1.6515854597091675 val_loss1.7263973951339722\n",
            "iteration 64 :train_loss:1.6378411054611206 val_loss1.711925983428955\n",
            "iteration 65 :train_loss:1.6246076822280884 val_loss1.6979788541793823\n",
            "iteration 66 :train_loss:1.6118640899658203 val_loss1.6845338344573975\n",
            "iteration 67 :train_loss:1.5995897054672241 val_loss1.6715707778930664\n",
            "iteration 68 :train_loss:1.587764859199524 val_loss1.6590694189071655\n",
            "iteration 69 :train_loss:1.5763709545135498 val_loss1.6470108032226562\n",
            "iteration 70 :train_loss:1.5653897523880005 val_loss1.6353766918182373\n",
            "iteration 71 :train_loss:1.5548033714294434 val_loss1.6241487264633179\n",
            "iteration 72 :train_loss:1.5445948839187622 val_loss1.6133102178573608\n",
            "iteration 73 :train_loss:1.5347484350204468 val_loss1.60284423828125\n",
            "iteration 74 :train_loss:1.525247573852539 val_loss1.5927354097366333\n",
            "iteration 75 :train_loss:1.516077995300293 val_loss1.5829678773880005\n",
            "iteration 76 :train_loss:1.5072247982025146 val_loss1.573527455329895\n",
            "iteration 77 :train_loss:1.4986746311187744 val_loss1.5643993616104126\n",
            "iteration 78 :train_loss:1.4904139041900635 val_loss1.5555708408355713\n",
            "iteration 79 :train_loss:1.4824296236038208 val_loss1.5470281839370728\n",
            "iteration 80 :train_loss:1.4747096300125122 val_loss1.5387587547302246\n",
            "iteration 81 :train_loss:1.4672423601150513 val_loss1.5307505130767822\n",
            "iteration 82 :train_loss:1.460016131401062 val_loss1.522991418838501\n",
            "iteration 83 :train_loss:1.4530192613601685 val_loss1.5154701471328735\n",
            "iteration 84 :train_loss:1.4462416172027588 val_loss1.5081764459609985\n",
            "iteration 85 :train_loss:1.439672827720642 val_loss1.5010994672775269\n",
            "iteration 86 :train_loss:1.4333033561706543 val_loss1.4942296743392944\n",
            "iteration 87 :train_loss:1.4271239042282104 val_loss1.4875571727752686\n",
            "iteration 88 :train_loss:1.4211255311965942 val_loss1.4810726642608643\n",
            "iteration 89 :train_loss:1.4152992963790894 val_loss1.4747673273086548\n",
            "iteration 90 :train_loss:1.409636378288269 val_loss1.4686322212219238\n",
            "iteration 91 :train_loss:1.4041285514831543 val_loss1.4626591205596924\n",
            "iteration 92 :train_loss:1.398768424987793 val_loss1.45684015750885\n",
            "iteration 93 :train_loss:1.393547773361206 val_loss1.4511672258377075\n",
            "iteration 94 :train_loss:1.388459324836731 val_loss1.4456332921981812\n",
            "iteration 95 :train_loss:1.3834965229034424 val_loss1.4402315616607666\n",
            "iteration 96 :train_loss:1.3786524534225464 val_loss1.434954285621643\n",
            "iteration 97 :train_loss:1.373921275138855 val_loss1.4297947883605957\n",
            "iteration 98 :train_loss:1.3692959547042847 val_loss1.4247466325759888\n",
            "iteration 99 :train_loss:1.364770531654358 val_loss1.4198040962219238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=150\n",
        "nh=128\n",
        "learning_rate = 0.1\n",
        "parameters, history=create_nn_model(train_features_scaled_t,train_target_scaled_t,nh, val_features_scaled_t, val_target_scaled_t, iterations, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xNncElmZhOE",
        "outputId": "af971491-1533-4cc4-cbd3-b7ffa68b02fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.609528541564941 val_loss5.774062156677246\n",
            "iteration 1 :train_loss:4.06143856048584 val_loss4.201159954071045\n",
            "iteration 2 :train_loss:3.072239398956299 val_loss3.1920852661132812\n",
            "iteration 3 :train_loss:2.4385812282562256 val_loss2.542428731918335\n",
            "iteration 4 :train_loss:2.0317232608795166 val_loss2.1226489543914795\n",
            "iteration 5 :train_loss:1.7693910598754883 val_loss1.849832534790039\n",
            "iteration 6 :train_loss:1.5987690687179565 val_loss1.6706583499908447\n",
            "iteration 7 :train_loss:1.48581862449646 val_loss1.5506752729415894\n",
            "iteration 8 :train_loss:1.4085277318954468 val_loss1.4675358533859253\n",
            "iteration 9 :train_loss:1.352593183517456 val_loss1.4066736698150635\n",
            "iteration 10 :train_loss:1.3086457252502441 val_loss1.3584821224212646\n",
            "iteration 11 :train_loss:1.2705615758895874 val_loss1.316641926765442\n",
            "iteration 12 :train_loss:1.2343389987945557 val_loss1.2769951820373535\n",
            "iteration 13 :train_loss:1.1974300146102905 val_loss1.2368563413619995\n",
            "iteration 14 :train_loss:1.1583701372146606 val_loss1.1946606636047363\n",
            "iteration 15 :train_loss:1.1165610551834106 val_loss1.149735450744629\n",
            "iteration 16 :train_loss:1.0721654891967773 val_loss1.10218346118927\n",
            "iteration 17 :train_loss:1.0259908437728882 val_loss1.0527706146240234\n",
            "iteration 18 :train_loss:0.9793033599853516 val_loss1.002759337425232\n",
            "iteration 19 :train_loss:0.9336128234863281 val_loss0.95368492603302\n",
            "iteration 20 :train_loss:0.8903815746307373 val_loss0.9070664644241333\n",
            "iteration 21 :train_loss:0.8507804870605469 val_loss0.8641218543052673\n",
            "iteration 22 :train_loss:0.8154996037483215 val_loss0.825632631778717\n",
            "iteration 23 :train_loss:0.7847294807434082 val_loss0.791858971118927\n",
            "iteration 24 :train_loss:0.7582990527153015 val_loss0.7626604437828064\n",
            "iteration 25 :train_loss:0.7357871532440186 val_loss0.7376282215118408\n",
            "iteration 26 :train_loss:0.7166689038276672 val_loss0.7162392735481262\n",
            "iteration 27 :train_loss:0.7004107236862183 val_loss0.6979466080665588\n",
            "iteration 28 :train_loss:0.686517596244812 val_loss0.6822317242622375\n",
            "iteration 29 :train_loss:0.6745601892471313 val_loss0.6686509847640991\n",
            "iteration 30 :train_loss:0.6641730666160583 val_loss0.6568140983581543\n",
            "iteration 31 :train_loss:0.655062198638916 val_loss0.6464089155197144\n",
            "iteration 32 :train_loss:0.6469824314117432 val_loss0.6371813416481018\n",
            "iteration 33 :train_loss:0.6397355198860168 val_loss0.628911554813385\n",
            "iteration 34 :train_loss:0.6331645846366882 val_loss0.6214284896850586\n",
            "iteration 35 :train_loss:0.6271445751190186 val_loss0.6145973205566406\n",
            "iteration 36 :train_loss:0.621579647064209 val_loss0.6083037853240967\n",
            "iteration 37 :train_loss:0.6163930892944336 val_loss0.6024582386016846\n",
            "iteration 38 :train_loss:0.6115204095840454 val_loss0.5969932675361633\n",
            "iteration 39 :train_loss:0.6069100499153137 val_loss0.5918455123901367\n",
            "iteration 40 :train_loss:0.6025205254554749 val_loss0.5869688391685486\n",
            "iteration 41 :train_loss:0.5983191728591919 val_loss0.582330048084259\n",
            "iteration 42 :train_loss:0.5942715406417847 val_loss0.5778865218162537\n",
            "iteration 43 :train_loss:0.5903555750846863 val_loss0.5736032724380493\n",
            "iteration 44 :train_loss:0.5865517854690552 val_loss0.5694593191146851\n",
            "iteration 45 :train_loss:0.5828526616096497 val_loss0.5654430389404297\n",
            "iteration 46 :train_loss:0.5792478919029236 val_loss0.5615347623825073\n",
            "iteration 47 :train_loss:0.5757303833961487 val_loss0.5577224493026733\n",
            "iteration 48 :train_loss:0.57229083776474 val_loss0.5540065765380859\n",
            "iteration 49 :train_loss:0.5689182281494141 val_loss0.5503807067871094\n",
            "iteration 50 :train_loss:0.5656061768531799 val_loss0.5468430519104004\n",
            "iteration 51 :train_loss:0.5623590350151062 val_loss0.5433753132820129\n",
            "iteration 52 :train_loss:0.5591727495193481 val_loss0.5399833917617798\n",
            "iteration 53 :train_loss:0.5560440421104431 val_loss0.53665691614151\n",
            "iteration 54 :train_loss:0.5529765486717224 val_loss0.5333932042121887\n",
            "iteration 55 :train_loss:0.5499667525291443 val_loss0.5301932096481323\n",
            "iteration 56 :train_loss:0.5470151901245117 val_loss0.5270603895187378\n",
            "iteration 57 :train_loss:0.5441206693649292 val_loss0.5239942669868469\n",
            "iteration 58 :train_loss:0.5412909388542175 val_loss0.520992636680603\n",
            "iteration 59 :train_loss:0.5385244488716125 val_loss0.5180546045303345\n",
            "iteration 60 :train_loss:0.5358183979988098 val_loss0.5151780843734741\n",
            "iteration 61 :train_loss:0.5331718325614929 val_loss0.5123673677444458\n",
            "iteration 62 :train_loss:0.5305822491645813 val_loss0.50962233543396\n",
            "iteration 63 :train_loss:0.5280535817146301 val_loss0.5069436430931091\n",
            "iteration 64 :train_loss:0.5255830883979797 val_loss0.5043293833732605\n",
            "iteration 65 :train_loss:0.5231675505638123 val_loss0.5017706155776978\n",
            "iteration 66 :train_loss:0.5208084583282471 val_loss0.49926885962486267\n",
            "iteration 67 :train_loss:0.5185059309005737 val_loss0.49682706594467163\n",
            "iteration 68 :train_loss:0.5162585377693176 val_loss0.4944453835487366\n",
            "iteration 69 :train_loss:0.5140663981437683 val_loss0.49212056398391724\n",
            "iteration 70 :train_loss:0.5119301080703735 val_loss0.4898562431335449\n",
            "iteration 71 :train_loss:0.5098429322242737 val_loss0.48764392733573914\n",
            "iteration 72 :train_loss:0.5078094601631165 val_loss0.485490083694458\n",
            "iteration 73 :train_loss:0.5058294534683228 val_loss0.4833946228027344\n",
            "iteration 74 :train_loss:0.5039026141166687 val_loss0.48135605454444885\n",
            "iteration 75 :train_loss:0.5020272135734558 val_loss0.4793742001056671\n",
            "iteration 76 :train_loss:0.5002031922340393 val_loss0.4774503707885742\n",
            "iteration 77 :train_loss:0.4984288215637207 val_loss0.47558140754699707\n",
            "iteration 78 :train_loss:0.4967024624347687 val_loss0.47376787662506104\n",
            "iteration 79 :train_loss:0.495024710893631 val_loss0.4720069468021393\n",
            "iteration 80 :train_loss:0.4933941960334778 val_loss0.4702976942062378\n",
            "iteration 81 :train_loss:0.49180570244789124 val_loss0.46864092350006104\n",
            "iteration 82 :train_loss:0.49025607109069824 val_loss0.4670298099517822\n",
            "iteration 83 :train_loss:0.488749235868454 val_loss0.46546781063079834\n",
            "iteration 84 :train_loss:0.48728492856025696 val_loss0.4639563262462616\n",
            "iteration 85 :train_loss:0.48586082458496094 val_loss0.4624907374382019\n",
            "iteration 86 :train_loss:0.4844748079776764 val_loss0.4610676169395447\n",
            "iteration 87 :train_loss:0.483127236366272 val_loss0.4596877098083496\n",
            "iteration 88 :train_loss:0.48181864619255066 val_loss0.4583517611026764\n",
            "iteration 89 :train_loss:0.4805476665496826 val_loss0.4570567309856415\n",
            "iteration 90 :train_loss:0.47931382060050964 val_loss0.4558044373989105\n",
            "iteration 91 :train_loss:0.47811517119407654 val_loss0.4545901417732239\n",
            "iteration 92 :train_loss:0.476949542760849 val_loss0.4534119963645935\n",
            "iteration 93 :train_loss:0.47581687569618225 val_loss0.452271968126297\n",
            "iteration 94 :train_loss:0.4747176468372345 val_loss0.4511685073375702\n",
            "iteration 95 :train_loss:0.47365084290504456 val_loss0.45009925961494446\n",
            "iteration 96 :train_loss:0.4726157784461975 val_loss0.44906213879585266\n",
            "iteration 97 :train_loss:0.4716103672981262 val_loss0.4480592906475067\n",
            "iteration 98 :train_loss:0.47063302993774414 val_loss0.44708696007728577\n",
            "iteration 99 :train_loss:0.46968355774879456 val_loss0.4461471140384674\n",
            "iteration 100 :train_loss:0.46875983476638794 val_loss0.4452350437641144\n",
            "iteration 101 :train_loss:0.46785932779312134 val_loss0.4443490207195282\n",
            "iteration 102 :train_loss:0.46698451042175293 val_loss0.44349104166030884\n",
            "iteration 103 :train_loss:0.46613389253616333 val_loss0.4426603317260742\n",
            "iteration 104 :train_loss:0.46530428528785706 val_loss0.4418526589870453\n",
            "iteration 105 :train_loss:0.4644959568977356 val_loss0.44106796383857727\n",
            "iteration 106 :train_loss:0.4637100100517273 val_loss0.44031020998954773\n",
            "iteration 107 :train_loss:0.462943971157074 val_loss0.43957147002220154\n",
            "iteration 108 :train_loss:0.4621979892253876 val_loss0.4388529062271118\n",
            "iteration 109 :train_loss:0.4614696502685547 val_loss0.4381537437438965\n",
            "iteration 110 :train_loss:0.46075788140296936 val_loss0.43747442960739136\n",
            "iteration 111 :train_loss:0.460064560174942 val_loss0.43681788444519043\n",
            "iteration 112 :train_loss:0.4593888819217682 val_loss0.43618056178092957\n",
            "iteration 113 :train_loss:0.4587291181087494 val_loss0.43555963039398193\n",
            "iteration 114 :train_loss:0.45808371901512146 val_loss0.43495482206344604\n",
            "iteration 115 :train_loss:0.4574534296989441 val_loss0.4343661367893219\n",
            "iteration 116 :train_loss:0.4568386971950531 val_loss0.43379682302474976\n",
            "iteration 117 :train_loss:0.45623865723609924 val_loss0.43324536085128784\n",
            "iteration 118 :train_loss:0.4556557238101959 val_loss0.4327097535133362\n",
            "iteration 119 :train_loss:0.45508694648742676 val_loss0.43218839168548584\n",
            "iteration 120 :train_loss:0.45452991127967834 val_loss0.431679904460907\n",
            "iteration 121 :train_loss:0.4539867341518402 val_loss0.4311816990375519\n",
            "iteration 122 :train_loss:0.45345425605773926 val_loss0.43069732189178467\n",
            "iteration 123 :train_loss:0.45293280482292175 val_loss0.4302251935005188\n",
            "iteration 124 :train_loss:0.45242369174957275 val_loss0.42976611852645874\n",
            "iteration 125 :train_loss:0.451928973197937 val_loss0.42931827902793884\n",
            "iteration 126 :train_loss:0.45144620537757874 val_loss0.4288821518421173\n",
            "iteration 127 :train_loss:0.4509739279747009 val_loss0.4284558892250061\n",
            "iteration 128 :train_loss:0.4505120515823364 val_loss0.4280376732349396\n",
            "iteration 129 :train_loss:0.4500599503517151 val_loss0.4276275038719177\n",
            "iteration 130 :train_loss:0.44961705803871155 val_loss0.4272282123565674\n",
            "iteration 131 :train_loss:0.44918179512023926 val_loss0.42683863639831543\n",
            "iteration 132 :train_loss:0.4487553834915161 val_loss0.4264582693576813\n",
            "iteration 133 :train_loss:0.44833797216415405 val_loss0.4260861277580261\n",
            "iteration 134 :train_loss:0.44792765378952026 val_loss0.42571669816970825\n",
            "iteration 135 :train_loss:0.4475250542163849 val_loss0.4253546893596649\n",
            "iteration 136 :train_loss:0.4471307098865509 val_loss0.4249992072582245\n",
            "iteration 137 :train_loss:0.44674262404441833 val_loss0.4246530830860138\n",
            "iteration 138 :train_loss:0.4463600814342499 val_loss0.4243127405643463\n",
            "iteration 139 :train_loss:0.4459828734397888 val_loss0.42397719621658325\n",
            "iteration 140 :train_loss:0.44561368227005005 val_loss0.4236490726470947\n",
            "iteration 141 :train_loss:0.44525253772735596 val_loss0.42332491278648376\n",
            "iteration 142 :train_loss:0.444898784160614 val_loss0.4230053126811981\n",
            "iteration 143 :train_loss:0.4445509910583496 val_loss0.42269065976142883\n",
            "iteration 144 :train_loss:0.4442082941532135 val_loss0.42238467931747437\n",
            "iteration 145 :train_loss:0.4438694715499878 val_loss0.4220838248729706\n",
            "iteration 146 :train_loss:0.4435359239578247 val_loss0.4217872619628906\n",
            "iteration 147 :train_loss:0.44320744276046753 val_loss0.42149633169174194\n",
            "iteration 148 :train_loss:0.4428846538066864 val_loss0.4212085008621216\n",
            "iteration 149 :train_loss:0.4425681531429291 val_loss0.42092299461364746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of all runs, below parameters gives the better result.\n",
        "* iterations=150\n",
        "* nh=128\n",
        "* learning_rate = 0.1"
      ],
      "metadata": {
        "id": "4KepYIQLaXeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Model Evaluation (4 pts)**\n",
        "\n",
        "Write a function to compute the Mean Absolute Percentage Error (MAPE) ( module 2 lecture, intro to feedforward neural networks, slide 21). Compute the MAPE on the test data and report the Mean Absolute Percentage Error.\n",
        "\n",
        "Let’s use a simple linear regression model as a baseline. Create a LinearRegression model using sklearn library and fit it on the training data. Here is a quick read if you are not sure how to create a LinearRegression model in python. When you want to fit the linear regression model to the data, you need to transpose back the rows and column in your training data. In addition, you need to flatten the target array back into a 1D array. That is, suppose that the training features and target arrays that you passed to your neural network model are trainx and trainy, respectively. You can fit the linear regression model to this data as follows:\n",
        "\n",
        "> Linear_model=LinearRegression().fit(trainx.T, trainy.flatten())\n",
        "\n",
        "\n",
        "Compute the MAPE of the linear regression model you created above on the test data and compare it with the MAPE of the neural networks. Which model performs better on this dataset?"
      ],
      "metadata": {
        "id": "72rwLyXATDH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(actual, predicted):\n",
        "  \"\"\"\n",
        "  Calculates the Mean Absolute Percentage Error (MAPE) between actual and predicted values.\n",
        "\n",
        "  Args:\n",
        "      actual: NumPy array of actual values.\n",
        "      predicted: NumPy array of predicted values.\n",
        "\n",
        "  Returns:\n",
        "      The MAPE value.\n",
        "  \"\"\"\n",
        "  if len(actual) != len(predicted):\n",
        "    raise ValueError(\"Actual and predicted values must have the same length.\")\n",
        "\n",
        "  non_zero_idx = actual != 0  # Avoid division by zero\n",
        "  mape = np.mean(np.abs((actual[non_zero_idx] - predicted[non_zero_idx]) / actual[non_zero_idx]) * 100)\n",
        "  return mape"
      ],
      "metadata": {
        "id": "JIgKvvbqn9fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression Predictions and MAPE**"
      ],
      "metadata": {
        "id": "xh3e5XNrgt08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Reshape train_target_scaled_t to match the number of samples in train_features_scaled_t\n",
        "train_target_scaled_t = train_target_scaled_t.reshape(-1, 1)\n",
        "\n",
        "# Ensure both train_features_scaled_t and train_target_scaled_t have the same number of samples\n",
        "assert train_features_scaled_t.shape[1] == train_target_scaled_t.shape[0], \"Mismatch in the number of samples\"\n",
        "\n",
        "# Transpose train_features_scaled_t to match dimensions\n",
        "linear_model = LinearRegression().fit(train_features_scaled_t.T, train_target_scaled_t.ravel())  # Use ravel() to flatten the target\n",
        "linear_predictions = linear_model.predict(test_features_scaled_t.T)\n",
        "\n",
        "linear_mape = calculate_mape(test_target_scaled_t.ravel(), linear_predictions)  # Use ravel() to flatten the target\n",
        "print(\"Linear Regression MAPE:\", linear_mape)"
      ],
      "metadata": {
        "id": "5M2k16-mpMiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5059effb-fef3-4719-81a7-8709ae2cdb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression MAPE: 30.089243516882696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Newtwork Predictions and MAPE**"
      ],
      "metadata": {
        "id": "NGq5fiFNg2m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_predictions(parameters, features):\n",
        "    \"\"\"\n",
        "    Generate predictions using a trained neural network model.\n",
        "\n",
        "    Args:\n",
        "        parameters: Dictionary containing the trained neural network parameters.\n",
        "        features: Input features for which predictions are needed.\n",
        "\n",
        "    Returns:\n",
        "        Predicted values.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Perform forward pass\n",
        "    Yhat = forward_pass(parameters, features)\n",
        "\n",
        "    return Yhat"
      ],
      "metadata": {
        "id": "TtevyF0XePUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = 0.1\n",
        "iterations = 150\n",
        "nh = 128\n",
        "\n",
        "# Neural Network model predictions\n",
        "parameters, history = create_nn_model(train_features_scaled_t,train_target_scaled_t,nh, test_features_scaled_t, test_target_scaled_t, iterations, learning_rates)\n",
        "\n",
        "neural_network_predictions_result=neural_network_predictions(parameters, test_features_scaled_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX8sZls6Lt3A",
        "outputId": "e0b32118-cf70-433a-934d-3c47db02f44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.609055042266846 val_loss5.516300201416016\n",
            "iteration 1 :train_loss:4.062067985534668 val_loss3.976081132888794\n",
            "iteration 2 :train_loss:3.074052572250366 val_loss2.993528366088867\n",
            "iteration 3 :train_loss:2.4424281120300293 val_loss2.366290330886841\n",
            "iteration 4 :train_loss:2.0388712882995605 val_loss1.9662551879882812\n",
            "iteration 5 :train_loss:1.7814375162124634 val_loss1.7116559743881226\n",
            "iteration 6 :train_loss:1.617606520652771 val_loss1.5501108169555664\n",
            "iteration 7 :train_loss:1.5136529207229614 val_loss1.4480029344558716\n",
            "iteration 8 :train_loss:1.4479098320007324 val_loss1.3837532997131348\n",
            "iteration 9 :train_loss:1.4064733982086182 val_loss1.343526840209961\n",
            "iteration 10 :train_loss:1.3804426193237305 val_loss1.3184778690338135\n",
            "iteration 11 :train_loss:1.3641386032104492 val_loss1.3029714822769165\n",
            "iteration 12 :train_loss:1.3539530038833618 val_loss1.2934355735778809\n",
            "iteration 13 :train_loss:1.3476030826568604 val_loss1.28761625289917\n",
            "iteration 14 :train_loss:1.343650221824646 val_loss1.2840981483459473\n",
            "iteration 15 :train_loss:1.3411916494369507 val_loss1.2819972038269043\n",
            "iteration 16 :train_loss:1.3396626710891724 val_loss1.280763864517212\n",
            "iteration 17 :train_loss:1.3387110233306885 val_loss1.2800581455230713\n",
            "iteration 18 :train_loss:1.338117241859436 val_loss1.2796707153320312\n",
            "iteration 19 :train_loss:1.3377455472946167 val_loss1.2794729471206665\n",
            "iteration 20 :train_loss:1.337511420249939 val_loss1.2793869972229004\n",
            "iteration 21 :train_loss:1.337362289428711 val_loss1.2793651819229126\n",
            "iteration 22 :train_loss:1.337266206741333 val_loss1.2793794870376587\n",
            "iteration 23 :train_loss:1.3372029066085815 val_loss1.279413104057312\n",
            "iteration 24 :train_loss:1.3371599912643433 val_loss1.2794557809829712\n",
            "iteration 25 :train_loss:1.337130069732666 val_loss1.2795023918151855\n",
            "iteration 26 :train_loss:1.3371081352233887 val_loss1.2795493602752686\n",
            "iteration 27 :train_loss:1.337091326713562 val_loss1.279595136642456\n",
            "iteration 28 :train_loss:1.3370779752731323 val_loss1.2796393632888794\n",
            "iteration 29 :train_loss:1.3370667695999146 val_loss1.2796812057495117\n",
            "iteration 30 :train_loss:1.337057113647461 val_loss1.2797207832336426\n",
            "iteration 31 :train_loss:1.3370484113693237 val_loss1.2797584533691406\n",
            "iteration 32 :train_loss:1.337040662765503 val_loss1.2797942161560059\n",
            "iteration 33 :train_loss:1.3370332717895508 val_loss1.2798280715942383\n",
            "iteration 34 :train_loss:1.3370264768600464 val_loss1.2798606157302856\n",
            "iteration 35 :train_loss:1.3370200395584106 val_loss1.2798916101455688\n",
            "iteration 36 :train_loss:1.3370139598846436 val_loss1.2799214124679565\n",
            "iteration 37 :train_loss:1.3370081186294556 val_loss1.2799499034881592\n",
            "iteration 38 :train_loss:1.3370026350021362 val_loss1.2799774408340454\n",
            "iteration 39 :train_loss:1.336997389793396 val_loss1.2800039052963257\n",
            "iteration 40 :train_loss:1.3369922637939453 val_loss1.2800294160842896\n",
            "iteration 41 :train_loss:1.3369874954223633 val_loss1.2800538539886475\n",
            "iteration 42 :train_loss:1.3369828462600708 val_loss1.2800776958465576\n",
            "iteration 43 :train_loss:1.3369783163070679 val_loss1.280100703239441\n",
            "iteration 44 :train_loss:1.3369741439819336 val_loss1.2801227569580078\n",
            "iteration 45 :train_loss:1.3369700908660889 val_loss1.2801443338394165\n",
            "iteration 46 :train_loss:1.3369660377502441 val_loss1.2801653146743774\n",
            "iteration 47 :train_loss:1.336962342262268 val_loss1.280185341835022\n",
            "iteration 48 :train_loss:1.3369587659835815 val_loss1.2802048921585083\n",
            "iteration 49 :train_loss:1.3369553089141846 val_loss1.2802238464355469\n",
            "iteration 50 :train_loss:1.3369519710540771 val_loss1.2802423238754272\n",
            "iteration 51 :train_loss:1.3369487524032593 val_loss1.2802599668502808\n",
            "iteration 52 :train_loss:1.336945652961731 val_loss1.2802772521972656\n",
            "iteration 53 :train_loss:1.3369427919387817 val_loss1.2802939414978027\n",
            "iteration 54 :train_loss:1.336939811706543 val_loss1.2803101539611816\n",
            "iteration 55 :train_loss:1.3369370698928833 val_loss1.2803256511688232\n",
            "iteration 56 :train_loss:1.3369344472885132 val_loss1.2803409099578857\n",
            "iteration 57 :train_loss:1.336931824684143 val_loss1.2803555727005005\n",
            "iteration 58 :train_loss:1.3369293212890625 val_loss1.2803702354431152\n",
            "iteration 59 :train_loss:1.3369269371032715 val_loss1.2803840637207031\n",
            "iteration 60 :train_loss:1.3369245529174805 val_loss1.280397653579712\n",
            "iteration 61 :train_loss:1.3369224071502686 val_loss1.2804107666015625\n",
            "iteration 62 :train_loss:1.3369202613830566 val_loss1.2804237604141235\n",
            "iteration 63 :train_loss:1.3369181156158447 val_loss1.2804361581802368\n",
            "iteration 64 :train_loss:1.3369160890579224 val_loss1.2804481983184814\n",
            "iteration 65 :train_loss:1.3369141817092896 val_loss1.280460000038147\n",
            "iteration 66 :train_loss:1.3369122743606567 val_loss1.2804713249206543\n",
            "iteration 67 :train_loss:1.336910367012024 val_loss1.2804824113845825\n",
            "iteration 68 :train_loss:1.3369086980819702 val_loss1.2804930210113525\n",
            "iteration 69 :train_loss:1.336906909942627 val_loss1.280503273010254\n",
            "iteration 70 :train_loss:1.3369052410125732 val_loss1.2805132865905762\n",
            "iteration 71 :train_loss:1.336903691291809 val_loss1.2805229425430298\n",
            "iteration 72 :train_loss:1.3369020223617554 val_loss1.2805323600769043\n",
            "iteration 73 :train_loss:1.3369005918502808 val_loss1.2805414199829102\n",
            "iteration 74 :train_loss:1.3368991613388062 val_loss1.280550241470337\n",
            "iteration 75 :train_loss:1.3368977308273315 val_loss1.280558705329895\n",
            "iteration 76 :train_loss:1.336896300315857 val_loss1.280566930770874\n",
            "iteration 77 :train_loss:1.3368949890136719 val_loss1.280574917793274\n",
            "iteration 78 :train_loss:1.3368935585021973 val_loss1.2805826663970947\n",
            "iteration 79 :train_loss:1.3368923664093018 val_loss1.2805901765823364\n",
            "iteration 80 :train_loss:1.3368910551071167 val_loss1.280597448348999\n",
            "iteration 81 :train_loss:1.3368899822235107 val_loss1.280604600906372\n",
            "iteration 82 :train_loss:1.3368887901306152 val_loss1.280611515045166\n",
            "iteration 83 :train_loss:1.3368875980377197 val_loss1.2806181907653809\n",
            "iteration 84 :train_loss:1.3368866443634033 val_loss1.2806246280670166\n",
            "iteration 85 :train_loss:1.3368854522705078 val_loss1.2806310653686523\n",
            "iteration 86 :train_loss:1.3368843793869019 val_loss1.280637264251709\n",
            "iteration 87 :train_loss:1.3368834257125854 val_loss1.280643105506897\n",
            "iteration 88 :train_loss:1.336882472038269 val_loss1.2806487083435059\n",
            "iteration 89 :train_loss:1.3368815183639526 val_loss1.2806543111801147\n",
            "iteration 90 :train_loss:1.3368804454803467 val_loss1.280659794807434\n",
            "iteration 91 :train_loss:1.3368796110153198 val_loss1.2806646823883057\n",
            "iteration 92 :train_loss:1.336878776550293 val_loss1.2806698083877563\n",
            "iteration 93 :train_loss:1.3368778228759766 val_loss1.280674695968628\n",
            "iteration 94 :train_loss:1.3368769884109497 val_loss1.2806791067123413\n",
            "iteration 95 :train_loss:1.3368761539459229 val_loss1.2806836366653442\n",
            "iteration 96 :train_loss:1.3368752002716064 val_loss1.280687928199768\n",
            "iteration 97 :train_loss:1.3368746042251587 val_loss1.2806921005249023\n",
            "iteration 98 :train_loss:1.3368736505508423 val_loss1.280695915222168\n",
            "iteration 99 :train_loss:1.3368730545043945 val_loss1.2806998491287231\n",
            "iteration 100 :train_loss:1.3368722200393677 val_loss1.2807035446166992\n",
            "iteration 101 :train_loss:1.3368715047836304 val_loss1.2807071208953857\n",
            "iteration 102 :train_loss:1.336870789527893 val_loss1.2807104587554932\n",
            "iteration 103 :train_loss:1.3368700742721558 val_loss1.280713677406311\n",
            "iteration 104 :train_loss:1.336869478225708 val_loss1.2807167768478394\n",
            "iteration 105 :train_loss:1.3368687629699707 val_loss1.2807197570800781\n",
            "iteration 106 :train_loss:1.3368680477142334 val_loss1.2807226181030273\n",
            "iteration 107 :train_loss:1.3368675708770752 val_loss1.2807252407073975\n",
            "iteration 108 :train_loss:1.336866855621338 val_loss1.2807278633117676\n",
            "iteration 109 :train_loss:1.3368662595748901 val_loss1.2807302474975586\n",
            "iteration 110 :train_loss:1.3368656635284424 val_loss1.2807326316833496\n",
            "iteration 111 :train_loss:1.3368650674819946 val_loss1.280734896659851\n",
            "iteration 112 :train_loss:1.3368645906448364 val_loss1.2807369232177734\n",
            "iteration 113 :train_loss:1.3368638753890991 val_loss1.2807390689849854\n",
            "iteration 114 :train_loss:1.336863398551941 val_loss1.2807409763336182\n",
            "iteration 115 :train_loss:1.3368629217147827 val_loss1.280742883682251\n",
            "iteration 116 :train_loss:1.336862325668335 val_loss1.2807444334030151\n",
            "iteration 117 :train_loss:1.3368618488311768 val_loss1.2807461023330688\n",
            "iteration 118 :train_loss:1.336861252784729 val_loss1.2807475328445435\n",
            "iteration 119 :train_loss:1.3368607759475708 val_loss1.280748963356018\n",
            "iteration 120 :train_loss:1.3368604183197021 val_loss1.2807503938674927\n",
            "iteration 121 :train_loss:1.3368598222732544 val_loss1.2807515859603882\n",
            "iteration 122 :train_loss:1.3368593454360962 val_loss1.2807527780532837\n",
            "iteration 123 :train_loss:1.336858868598938 val_loss1.2807537317276\n",
            "iteration 124 :train_loss:1.3368585109710693 val_loss1.2807546854019165\n",
            "iteration 125 :train_loss:1.3368579149246216 val_loss1.280755639076233\n",
            "iteration 126 :train_loss:1.336857557296753 val_loss1.2807564735412598\n",
            "iteration 127 :train_loss:1.3368570804595947 val_loss1.280757188796997\n",
            "iteration 128 :train_loss:1.336856722831726 val_loss1.280758023262024\n",
            "iteration 129 :train_loss:1.3368562459945679 val_loss1.2807585000991821\n",
            "iteration 130 :train_loss:1.3368558883666992 val_loss1.2807589769363403\n",
            "iteration 131 :train_loss:1.336855411529541 val_loss1.2807594537734985\n",
            "iteration 132 :train_loss:1.3368550539016724 val_loss1.2807599306106567\n",
            "iteration 133 :train_loss:1.3368546962738037 val_loss1.2807600498199463\n",
            "iteration 134 :train_loss:1.3368542194366455 val_loss1.280760407447815\n",
            "iteration 135 :train_loss:1.3368538618087769 val_loss1.280760645866394\n",
            "iteration 136 :train_loss:1.3368535041809082 val_loss1.2807607650756836\n",
            "iteration 137 :train_loss:1.3368531465530396 val_loss1.2807607650756836\n",
            "iteration 138 :train_loss:1.336852788925171 val_loss1.2807607650756836\n",
            "iteration 139 :train_loss:1.3368524312973022 val_loss1.2807607650756836\n",
            "iteration 140 :train_loss:1.3368520736694336 val_loss1.2807607650756836\n",
            "iteration 141 :train_loss:1.336851716041565 val_loss1.2807605266571045\n",
            "iteration 142 :train_loss:1.3368513584136963 val_loss1.2807602882385254\n",
            "iteration 143 :train_loss:1.3368510007858276 val_loss1.2807600498199463\n",
            "iteration 144 :train_loss:1.336850643157959 val_loss1.2807598114013672\n",
            "iteration 145 :train_loss:1.3368504047393799 val_loss1.2807594537734985\n",
            "iteration 146 :train_loss:1.3368500471115112 val_loss1.2807592153549194\n",
            "iteration 147 :train_loss:1.3368496894836426 val_loss1.2807586193084717\n",
            "iteration 148 :train_loss:1.3368494510650635 val_loss1.280758261680603\n",
            "iteration 149 :train_loss:1.3368490934371948 val_loss1.2807579040527344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MAPE for Neural Networks\n",
        "neural_network_mape = calculate_mape(test_target_scaled_t, neural_network_predictions_result)\n",
        "\n",
        "print(\"Neural Network MAPE:\", neural_network_mape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpgvFI4JhPG9",
        "outputId": "488ae692-87af-4c22-8aef-1cabaaf87f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network MAPE: 60.477604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare which model performs better\n",
        "if linear_mape < neural_network_mape:\n",
        "    print(\"Linear Regression performs better.\")\n",
        "else:\n",
        "    print(\"Neural Network performs better.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90nTdwHgphQE",
        "outputId": "2c47e1dc-1bc4-4715-830d-8af052188828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression performs better.\n"
          ]
        }
      ]
    }
  ]
}
